\documentclass[12pt]{article}

% packages
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{graphicx}


\newcommand{\Q}[1]{\subsection*{Ex #1}}
\newenvironment{question}[1]
{\Q{#1}}{}


\newcommand{\union}[1]{\underset{#1}{\cup} }
\newcommand{\bigunion}[1]{\underset{#1}{\bigcup} \, }
\newcommand{\inter}[1]{\underset{#1}{\cap} }
\newcommand{\biginter}[1]{\underset{#1}{\bigcap} }
\newcommand{\minimize}[3]{\optimize{#1}{#2}{#3}{min}}
\newcommand{\maximize}[3]{\optimize{#1}{#2}{#3}{max}}
\newcommand{\esp}{{\mathbb E}}
\newcommand{\pr}{{\mathbb P}}

\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\err}{err}


% parameters
\geometry{hmargin=1.5cm,vmargin=1.5cm}
\title{COS511 - Problem Set 1}
\author{Bachir EL KHADIR }

\begin{document}
\maketitle

\begin{question}{1}

Let $h_{r^*}$ the true classifier.
Denote $r_{\min} = \max \{ ||x_i|| : y_i = 1 \}$, let's consider the algorithm that choses $h_{r_{\min}}$.
The algorithm labels corretly points $x$ outside the region $r_{\min} \le ||x|| \le r^*$.
Let's call $A$ the r.v. that measures this region with respect to the distribution of $x$.

For a sample of size $m$: $\pr(A > \varepsilon) \le (1 - \varepsilon)^m$, in order to make this quantity smaller than $\delta > 0$, it sufficies to choose $m \ge \frac{\log \frac1\delta}{\varepsilon}
$
c/c: $m_{\mathcal H}(\delta, \varepsilon) \le \lceil \frac{\log \frac1\delta}{\varepsilon} \rceil$
\end{question}
\begin{question}{2}
For a concept $\hat f$ fixed, call $\mathcal D'$ the distribution of $X, Y$ when sampled according to this new concept. Let's note $\alpha$ the proportion of element $x$ for which $\hat f$ has changed the label, eg $\alpha(\hat f) := \err_{\mathcal D'}(f)$. On average, $\varepsilon_0$ would be assigned a new value, and of those, only half would be assigned a label different from their original label. so $E_{\hat f}[\alpha(\hat f)] = \frac{\varepsilon_0}2$

$\mathcal A$ being an agnostic learner, for a sample of size $m$ bigger than $P(\frac1\varepsilon, \log(\delta), \log|H|) := \frac{2 \log(2 |H| / \delta)}{\varepsilon^2}$, with probability at least $1 - \delta$ over the possible samples $S \sim \mathcal {D'}^m$, $\err_{\mathcal D'}(h_{\mathcal A}) < \alpha + \epsilon$.
Taking the expectation with respect to $\hat f$ we get the desired result.
\end{question}
\begin{question}{3}
\begin{itemize}
\item
  \begin{align*}
    \pr( X \ge t) &= \pr( e^{\lambda X} \ge e^{\lambda t}) &\text{(because $\exp$ is increasing)}
    \\& =  \pr( e^{\lambda (X - t)} \ge 1)
    \\&\le \esp[e^{\lambda (X - t)} ] &\text{(Markov inequality)}
    \\&= e^{-\lambda t} \prod \esp e^{\lambda x_i} &\text{(By independence)}
    \\& = e^{-\lambda t} (\esp e^{\lambda x_i})^k &x_i \sim x_1
    \\& = e^{-\lambda t} (\frac12 (e^{\lambda} + e^{-\lambda}))^k 
  \end{align*}
\item
  $e^{\lambda} + e^{-\lambda} = \sum^{\infty} \frac{\lambda^k}{k!} + (-1)^k \frac{\lambda^k}{k!} = 2 \sum^{\infty}  \frac{\lambda^{2k}}{(2k!)}$
  $e^{\frac12 \lambda^2} = \sum \frac{\lambda^{2k}}{2^k k!}$
  Since $\frac{(2k)!}{2^kk!} = \prod_{j = 1 \ldots 2k} \frac{k+j}{2} \ge 1$, $\frac12 e^{\lambda} + \frac12 e^{-\lambda} \le e^{\frac12 \lambda^2}$

\item $\pr(X \ge t) \le e^{-\lambda t} (\frac12 e^{\lambda} + \frac12 e^{-\lambda})^k \le e^{-\lambda t} e^{\frac k2 \lambda^2}$
$\lambda \rightarrow \frac k2 \lambda^2 - \lambda t$ is quadratic function that attain its minimum for $\lambda = \frac tk$, and the minimum is $-\frac{t^2}k$, therefore $ \pr(X \ge t) \le e^{-\frac{t^2}k}$.
\end{itemize}
\end{question}
\begin{question}{4}
\begin{itemize}
\item
  Let's prove this fact by contraposition. Let's suppose $f \not \in \mathcal H$, and prove that it can't be learnable by $\mathcal H$.
  
  $X$ and $Y$ are finite, so is $\mathcal H$.
  
  $f \not \in \mathcal H$, then $\forall h \in \mathcal H, \err(h) > 0$, and $\alpha = \min_{h \in \mathcal H} \err(h) > 0$.

  For any algorithm producing a hyppotesis $h$ depending on a sample of an arbitrary size, $\err(h) > \alpha$ with probability 1.
  As a result $f$ is not learnable by $\mathcal H$.
  
\item
  Counter example:
  
  Choose $X = \mathbb N, Y = \{0, 1\}, f = 0, \mathcal H = \{ h_n(x) := 1_{x = n} : n \in \mathbb N \}$.
  Let's call the joint distribution $\mathcal D$, and let $p_k = \pr_{\mathcal D}(X = k)$
  
  It is clear that $f \not \in h$

  For a sample $S = \{ (x_i, y_i) \}$ of finite size $m$, let's consider the algorithm $A(S)$ that returns $h_{\hat n}$ where $\hat n = \max_{i=1\ldots m} x_i + 1$ .
  
  $\err_{\mathcal D}(\mathcal A(S)) = \pr_{X}(X = \hat n) \le \prod_{i=1\ldots m} \pr_{X}(X < x_i) \le \frac1{2^m}$

  For any $\varepsilon$, $\pr_{S \sim {\cal D}^m}( \err_{\mathcal D}(\mathcal A(S)) > \varepsilon ) \le 1 - 1_{\{ m > -\log_2(\varepsilon)\}}$, if $m > -\log_2 \varepsilon$ then this probability drops to 0.
  Which means thant $f$ is learnable and still $f \not \in {\cal H}$.
\end{itemize}
\end{question}
\begin{question}{5}

$X = \mathbb N$.

$\varepsilon > 0, m, A$ fixed.

Fix $f$, and let $\mathcal D$ be the uniform distribution over $\{(x, f(x)), x \le \frac m \varepsilon\}$. This leads to $\pr(X \not \in S) = 1 - \varepsilon$.

\begin{align*}
  Q &= \esp_f  \esp_S \err A(S)
  \\&= \esp_S \esp_f  \esp_x  [A(S)(x) \ne f(x)]
  \\&= \esp_{S,x} \esp_f    [A(S)(x) \ne f(x) | x \in S] \mathbb{P}(x \in S)
  \\&+ \esp_{S,x} \underbrace{\esp_f  [A(S)(x) \ne f(x) | x \not \in S]}_{= \frac12} \underbrace{\mathbb{P}(x \not \in S)}_{\ge 1 - \varepsilon}
\end{align*}
Therefore $Q \ge 0 + \frac12 (1 - \varepsilon) \ge \frac12 - \varepsilon$, which means that there exist a concept $f$ such that
$\err_{\mathcal D}(f) = 0$ and $\esp_S \err A(S) \ge \frac12 - \varepsilon$.

\end{question}


\end{document}




















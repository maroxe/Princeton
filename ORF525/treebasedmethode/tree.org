#+HTML_HEAD:    <link rel="stylesheet" type="text/css" href="./org-style.css" />
#+HTML_HEAD:    <link rel="stylesheet" type="text/css" href="./special-block.css" />
#+latex_header: \usepackage{amsthm}
#+latex_header: \newtheorem{theorem}{Theorem}
#+latex_header: \newtheorem{definition}{Definition}

#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t broken-links:nil c:nil creator:nil
#+OPTIONS: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+OPTIONS: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:t title:t toc:t todo:t |:t
#+TITLE: Tree-based Methode (regression)
#+DATE: <2016-02-23 Tue>
#+AUTHOR: Bachir El Khadir
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Bachir El Khadir



* Tree

#+name: Tree regression   
#+begin_definition
$\mathcal{F} := \{ f(x) = \sum_{j=1}^M \beta_j 1_{x \in R_j} \}$  Where $R_1, \ldots, R_M$ for a /tree partition/
#+end_definition

#+name: Tree Partition   
#+begin_definition
A parititon of the input space $X$ that can be formed by recursively applying the following 2 rules
- Choose a cell of the current partition
- Split the chosen cell into two daughters by binary splitting along one dimension (one variable)
#+end_definition

#+name: SampleVersion
#+begin_definition
$\hat R(f) = \frac1n \sum_{i=1}^M (Y_i - f(X_i))^2$
minimizing this quantity leads to overfitting, so we need to regularize.
For example we can restrict the search space to $\mathcal{F}_{K_{min}} =\{f(x) = \sum_{j=1}^M \beta_j 1_{x \in R_j} \}$ and $R_j$ contains at least $K_min$ data points (e.g $K_{\min} = 5$)

Computation: Conbinatoric! (NP-Hard)
In practice we use a Greedy Algorithm.
#+end_definition

#+name: GreedyAlgorithm
#+begin_definition
Grow a tree recursively by repeating the following steps:
for each terminal node of the tree, until the minimal node size $K_{\min}$ is achieved
1. Pick a variable / split point which decreases $\hat R(f)$ the most
2. Split the node into two daughters
#+end_definition

Still overfits.

** Prune The tree
    1) Givena full grown tree $T_0$, find an internal node which after collapsing the subtree into iteself, will increase $\hat R(f)$ the least.
    2) Collapse the subtree into this internal node. We get a new tree $T$, reoeat tihs process we get a sequence of new trees $T_0, T_1, \ldots$.
    3) Pick one tree by minimizing $\hat R(\hat f_T) + \lambda |T|$, where $\lambda$ is obtained by CV tuning, $|T|$ the number of nodes in $T$.

** Pros and Cons of Tree
    - Pro: Simple a interpretable
    - Cond: Fitten functions are non smooth: theoritically extremely challenging (no persistensy result)

      
* Bagging (Bootstrap Aggregation)
  
#+name: Bagging
#+begin_algorithm
For $b = 1, \ldots, B$
a. Draw a boostrap ${Z_{1:n}^*}^{(b)}$ of size $n$ from $Z_{1:n}$
b. Fit a regression tree on the bootstrapped data (with minimum node size $K_{\min}$, no pruning)
Output: $\hat f^{\text{bagging}} = \frac1B \sum_{b=1}^B \hat f^b(x)$
#+end_algorithm
** Bagging vs Tree
   1. $\hat f^{\text{bagging}}$ has the same bias as $\hat f^b(x)$, but potentially smaller variance.
   2. The larger $B$ is, the better (but diminishing return)
   3. Works well only if $\hat f^1(X), \ldots, \hat f^B(x)$ are decorrelated.
  
* Random Forest   
  







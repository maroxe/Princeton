\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage[a4paper, total={7in, 10in}]{geometry}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{remark}{Remark}
\newcommand{\esp}{{\mathbb E}}
\newcommand{\pr}{{\mathbb P}}
\newcommand{\norm}[1]{\Vert #1 \Vert}
\newcommand{\fnorm}[1]{\Vert #1 \Vert_F}
\newcommand{\nucnorm}[1]{\Vert #1 \Vert_*}
\newcommand{\opnorm}[1]{\Vert #1 \Vert_{op}}
\newcommand{\inner}[2]{\langle #1 \, , \, #2 \rangle}


\title{ORF525 - Class Notes}
\author{Bachir EL KHADIR }

\newenvironment{class}[1]
{\section*{Class #1}}
{ ----------------------}

\begin{document}
\maketitle

\begin{class}{1}
\begin{definition}[Oridnary Lease Squares Regression]
$f_i = \{ f(x) = \beta^T X\}$
$\hat \beta^{OLS} = \arg \min_{\beta} ||Y - X \beta||_2^2$
$F(\beta) = Y^TY + \beta^TX^TX\beta - 2\beta^TX^TY$
$\frac{\partial F(\beta)}{\partial \beta} = 2X^TX\beta - 2X^TY = 0 \implies \hat \beta = (X^TX)^{-1}X^TY$
\end{definition}
\begin{definition}[Model-based Interpretation of OLS]
  Statistical Model $Y = \beta^TX + \varepsilon, \varepsilon \sim \mathcal N(0, 1)$
  Joint-Loglikelihood
  $$l_n(\beta, \sigma^2) =f \sum_{i=1}^n \log p_{\beta, \sigma^2}(Y_i, X_i) =  \sum_{i=1}^n \log p_{\beta, \sigma^2}(Y_i| X_i) + \underbrace{ \sum_{i=1}^n \log p(X_i) }_{\text{does not depend on $\beta$ or $\sigma^2$}}  $$
  $\implies$
  \begin{align*}
    \arg\max_{\beta, \sigma^2} l_n(\beta, \sigma^2)
    &= \arg\max_{\beta, \sigma^2}  \underbrace{\sum_{i=1}^n \log p_{\beta, \sigma^2}(Y_i| X_i)}_{\text{Conditional log-likelihood}}
    \\& = \arg\max_{\beta, \sigma^2} \frac1{2\sigma^2} \sum (Y_i - \beta^TX_i)^2 + n \log(\frac1{\sqrt{2\pi\sigma^2}})
  \end{align*}
  $\implies$
  $\hat \beta^{MLE} = \arg\min \sum (Y_i - \beta^T X_i)^2 = \hat \beta^{OLS}$
\end{definition}

\section{Linear Regression with Basis Expansion}
From linear to non linear
\begin{itemize}
\item Input vairables can be transofrmation of original feautres: Handraft features, Box-Cox tranformation (find the best transmformation)
\item Input can have interactions, eg $X_1X_2 \ldots$
\item Inputs can have basis expansions. Instead of $f(x) = \beta^Tx$ we can have $f(x) = \sum_j \beta_j \underbrace{h_j}_{\text{Adaptative learning}}(x)$.
\end{itemize}

\begin{definition} [Categorical Variable]
  A variable that can take on only one of a limited values.
  \textbf{Dummy coding}
\end{definition}

\section{High Dimensional Regression}
\begin{definition}[High Dimensional Regression]
  Data when dimension $d$ is bigger than the sample size $n$.
  \[
    Y = \left(
      \begin{array}{c}
        Y_1\\\cdots\\Y_n
      \end{array}
    \right)
  \]
  \[
    X = \left(
      \begin{array}{ccc}
        X_{11}&\ldots&X_{1n}\\&\cdots\\X_{n1}&\ldots&X_{nn}
      \end{array}
    \right)
  \]
\end{definition}
Question: $\hat \beta^{OLS} = (\underbrace{X^TX}_{\text{not invertible}})^{-1}X^TY$, what should we do?
\begin{itemize}
\item Ridge Estimation $\hat \beta^{\lambda} =  (\underbrace{X^TX + \lambda I}_{\text{Tuning Parameters}})^{-1}X^TY$
  $$\iff \hat \beta^{\lambda} = \arg\min_{\beta \in \mathbb R^d} ||Y - X\beta||_2^2 + \lambda ||\beta||_2^2$$
  $$\iff \hat \beta^{t} = \arg\min_{||\beta||_2^2 < t} ||Y - X\beta||_2^2$$
  
\item Computation of Ridge:
  \begin{itemize}
  \item Convex Optimzation (QP)
  \item Never naively use a \textit{general-purpose} solver. (CVX, AMPL)
  \end{itemize}
\item Question: How to choose the tuning parameter $\lambda$?
  Model selection: $\Lambda = \{ \lambda_1, \ldots, \lambda_n \}$
  Basic Method: $D = D_1 \cup D_2$, let $\hat \beta^{\lambda_1}, \ldots, \hat \beta^{\lambda_k}$ be ridge estimators on $D_1$.
  We define the data split score $DS(k) = \frac1{n^2} \sum_{D_2} (Y_i - X_i^T \hat \beta^{\lambda_k})^2$
  We then pick the model with the smallest DS score.
  Intuition: Conditioning on $D_1$, $DS(k)$ is an unbiased estimator of $R(\hat \beta^{\lambda_1})$.
  Pro:Theoritically and conceptually simple.
  Con: Waste of the training sample. $\implies$ Cross validation.
\end{itemize}
\end{class}
\begin{class}{2}[Data spliting]

\subsubsection{Pros and cons o f data splitting}
\textbf{Pro:} Theoritically and computationaly simple.
\textbf{Con:} Waste if training data $\implies$ cross validation.

\begin{itemize}
\item training / test split: conditional (on the training) prediction error.
  $$\esp_{X, Y}[ |Y - \hat f_{D_{train}}(X)|^2 | D_{train}]$$
\item cross validation: converges to expected training data.
  $$\esp_D[ \esp[ |Y - \hat f_{D_{train}}(X)|^2 | D_{train}] ]$$
\end{itemize}
\begin{definition}[$J$-Fold Cross validation]
  We split the data $\mathcal D$ into $J$-equally sized parts $\mathcal D_1, \ldots, \mathcal D_J$. This forms:
  \begin{align*}
    (DS1):& \mathcal D_1 \text{ vs } \mathcal D \setminus \mathcal D_1\\
    \cdots\\
    (DS1):& \mathcal D_1 \text{ vs } \mathcal D \setminus \mathcal D_n\\
  \end{align*}
  For $\lambda_k \in \Lambda$ we calculate the data splitting scores Using $DS1, \ldots DS2$. Denote the result as $DS_1(k), \ldots, DS_J(k)$. The cross validation is
  $$CV(k) := \frac1J \sum_j^J DS_j(k)$$
  We then pick $\arg\min CV(k)$.
  In pratice, picke the most parsimonious model whose error is no more than one standard deviation above the smallest CV score.
  
\end{definition}
Question: After CVm we pick $\hat \lambda_k$. Then what shall we do?
\begin{itemize}
\item Use $\lambda_k$ to fit the entire data, then deliver
\item Take the average of the estimators.
\end{itemize}

\subsubsection{Model assessment vs selection}
\begin{definition}[Lasso]
  Bridge estimator with $\beta =1 $
  Least absolute shrinkage and selection operator
  Sparsity: Intersection of ellipsoid ($||Y - X\beta||_2^2 = cte$) and a polytope $||\beta||_1 = cte$)
\end{definition}

Sparsity: many ekements of $\beta$ are 0 $\implies$ model selection. (select variable with coefficient $\ne$ 0)


\end{class}
\begin{class}{3}[Persistency]
  
  \begin{tabular}{| l | l |}
    Ridge & Lasso\\ \hline
    Not Sparse &  Sparse \\ \hline
    Handles collinearity &  Doesn't handle collinearity
  \end{tabular}
  \begin{definition}[Collinearity]
    A phenomenon in which two or more predictor variables are highly correlated.
  \end{definition}
  
  Question : Combine Ridge and Lasso?
  Answer: \textbf{Elastic-Net}
  $$\hat \beta^{\text{Elastic}} = \arg \min ||Y - X\beta||_2^2 + \lambda ( \alpha ||\beta|||_1 + (1-\alpha)||\beta||_2^2)$$
  \begin{itemize}
  \item $\alpha = 1 \implies$ Lasso.
  \item $\alpha = 0 \implies$ Ridge.
  \end{itemize}

  Question: two tuning parameters, how to choose then?
  Answer: Use a two stage approach:
  \begin{itemize}
  \item Use $\alpha = 1$, fit a full Lasso path, visualize the regularization path.
  \item Use $\alpha = 0.6$, fit the regularization path pagain.
    Then we examine whether there si significant change of the final path:
    \begin{itemize}
    \item If not $\implies \alpha = 1$ (Lasso)
    \item o/w $\implies \alpha = 0.6$ (Elastic)
    \end{itemize}
  \end{itemize}

  \subsubsection*{Insight of the Lasso Estimator}
  \begin{definition}[SQRT-Lasso]
    An equivalent representation of the lasso is called SQRT-Lasso:
    \begin{equation}
    \hat \beta^{\text{Elastic}} = \arg \min ||Y - X\beta||_2^2 + \lambda ||\beta||_1\label{eq:*}
  \end{equation}

    Symptoticcaly $\lambda^{\text{optimaly}} \sim 2.1 \sqrt{\frac t{\log d}{n}}$, $n > 10k$ + The model has to be linear 
  \end{definition}
  \begin{theorem}[Robust Optimization Representation of Lasso]
    The SQRT-Lasso problem in (\ref{eq:*}) is equivalent to the following robust linear regression problem:
    $$\min_{\beta} \max_{U \in \Omega_{\lambda}} ||Y - (X + U)\beta||_2$$
    Where $\Omega_{\lambda} := \{ U = (U_1, \ldots, U_d)\in \mathbb R^{n \times d}, \max_j ||U_j||_2 \le \lambda \}$
  \end{theorem}
  \begin{proof}
    We only need to prove
    $\max_{U \in \Omega_{\Lambda}} ||Y - (X+U)\beta||_2 = ||Y - X\beta||_2 + \lambda ||\beta||_1$
    \begin{itemize}
    \item     $\max_{U \in \Omega_{\Lambda}} ||Y - (X+U)\beta||_2 \le ||Y - X\beta||_2 + \lambda ||\beta||_1$
      $||Y - (X+U)\beta||_2 \le ||Y - X\beta||_2 + \sum_j  |\beta_j| \; ||U_j||_2 \le ||Y - X\beta||_2 + \lambda ||\beta||_1$
    \item     $||Y - X\beta||_2 + \lambda ||\beta||_1 \le \max_{U \in \Omega_{\Lambda}} ||Y - (X+U)\beta||_2$
      \[
        u = \left\{
          \begin{array}{cc}
            \frac{Y - X\beta}{||Y-X\beta||_2} &\text{if } Y \ne X\beta\\
            \text{arbitrary unit vector}&\text{o.w}
          \end{array}
        \right.
      \]
      And define:
      
      $$U^*_j = -\lambda sign(\beta_j) u$$
      ($sign(0) = 1$)
      
      We can verify that $|U_j|_2 \le \lambda$
      \begin{align*}
        |(Y - (X+U^*)\beta|_2 \ge |(Y - X\beta - \sum_j \beta_jU^*_j|_2\\
        &\ge |(Y - X\beta - \sum_j |\beta_j| \frac{Y - X\beta}{||Y-X\beta||_2}\\
        &= | (|Y - X\beta)|_2 + \lambda |\beta|_1) \frac{Y - X\beta}{||Y-X\beta||_2} |_2\\
        &= |Y - X\beta|_2 + \lambda |\beta|_1
      \end{align*}


    \end{itemize}
    
  \end{proof}

  \begin{definition}[Theory of Lasso (Greenshtein and Ritov '2006)]
    We define
    $$R(\beta) = E_{Y, X} (Y - \beta^TX)^2, \hat R(\beta) = \frac1n \sum_j (Y_j - \beta^TX_i)^2$$
    $\hat \beta = \arg\min_{|\beta|_1 \le L} \hat R(\beta)$: Lasso estimator
    $\beta^* = \arg\min_{|\beta|_1 \le L} R(\beta)$: Lasso estimator
  \end{definition}
  \begin{definition}[Persistence]
    An estimator $\hat \beta$ is persistent within a class ${\cal B}_n$ if $R(\hat \beta) - \inf_{\beta \in {\cal B}_n} R(\beta) \rightarrow_{\mathbb P} 0$ as $n \rightarrow \infty$
  \end{definition}
  \begin{theorem}[Lasso]
    Assume $|Y_i| \le B$ and $|X|_{\infty} \le B$. Then
    $$P\left(R(\hat \beta) - R(\beta^*) \le 2 (1 + L^2) \sqrt{\frac{2B^4\log(\frac{2d^2}{\delta})}n} \right) \ge 1 - \delta$$
  \end{theorem}

    \begin{proof}
      \begin{align*}
        R(\hat \beta) - R(\beta^*)
        &=  R(\hat \beta) - \hat R(\hat \beta) + \hat R(\hat \beta) - R(\beta^*)\\
        &\le R(\hat \beta) - \hat R(\hat \beta) + \hat R(\beta^*) - R(\beta^*)\\
        &\le 2 \sup_{\norm{\beta}_{L_1} \le L} |R(\beta) - R(\hat \beta)|
      \end{align*}

      Let $Z = (Y, X^T)^T$, $r = (-1, \beta^T)^T$

      $$R(\beta) = \esp(Y - \beta^TX)^2 = \esp(r^TZZ^Tr) = r^T \esp(ZZ^T)r = r^T \Sigma r$$
      $$\hat R(\beta) = \frac1n \sum (Y_i - \beta^TX_i)^2 =  r^T \frac1n \sum Z_iZ_i^T r = r^T \hat \Sigma r$$

      Therefore
      
      \begin{align*}
        \sup_{\norm{\beta}_{L_1} \le L} |R(\beta) - R(\hat \beta)|
        &= \sup |r^T(\hat \Sigma - \Sigma)r|
        \\&\le \norm{r(\beta)}_1^2 \norm{\hat \Sigma - \Sigma}_{\infty}
      \end{align*}
      By Hoeding
      $$\pr(\norm{\hat \Sigma - \Sigma}_{\infty} > t) \le \sum \pr(\hat \Sigma_{ij} - \Sigma_{ij} > t) \le 2 d^2 \exp(-\frac{nt^2}{2B^4})$$
      
    \end{proof}
    
  \begin{theorem}[Persistency of the Lasso]
    $\forall k > 0, d = O(n^k)$, ${\cal B}_n = \{\beta, |\beta|_1 \le L_n, L_ = o(\frac n{\log n})^{\frac14}\}$
    Then:
    $R(\hat \beta) - \inf_{\beta \in {\cal B}_n} R(\beta) \rightarrow_{\mathbb P} 0$ as $n \rightarrow \infty$
  \end{theorem}

\end{class}
\end{document}





















\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}

\newcommand{\hbeta}{\hat \beta }
\newcommand{\union}[1]{\underset{#1}{\cup} }
\newcommand{\bigunion}[1]{\underset{#1}{\bigcup} \, }
\newcommand{\inter}[1]{\underset{#1}{\cap} }
\newcommand{\biginter}[1]{\underset{#1}{\bigcap} }
\newcommand{\minimize}[3]{\optimize{#1}{#2}{#3}{min}}
\newcommand{\maximize}[3]{\optimize{#1}{#2}{#3}{max}}
\newcommand{\esp}{{\mathbb E}}
\newcommand{\pr}{{\mathbb P}}
\newcommand{\norm}[1]{\Vert #1 \Vert}
\newcommand{\fnorm}[1]{\Vert #1 \Vert_F}
\newcommand{\nucnorm}[1]{\Vert #1 \Vert_*}
\newcommand{\opnorm}[1]{\Vert #1 \Vert_{op}}
\newcommand{\inner}[2]{\langle #1 \, , \, #2 \rangle}
\newcommand{\row}[2]{\begin{pmatrix}#1 & #2\end{pmatrix}}


\begin{document}

\section{P2}
\begin{itemize}
\item
  Let's assume  $X\beta_1 \ne X\beta_2$.
  
  Let $f^*$ be the optimal value, $\alpha = \frac12$, $\beta_{\alpha} = \alpha \beta_1 + (1-\alpha) \beta_2$.
  Then, by the convexity of $\norm{.}_2^2, \norm{.}_1$:
  \begin{align*}
    f^* &\le ||Y - X \beta_{\alpha}||_2^2 + \lambda ||\beta_{\alpha}||_1
    \\&= \norm{ \alpha (Y - X \beta_1) + (1-\alpha)(Y - X\beta_2)}_2^2
        + \lambda \norm{\alpha \beta_1 + (1-\alpha)\beta_2}_1
    \\&<
        \alpha \left(||Y - X \beta_1||_2^2 + \lambda ||\beta_1||_1\right)
        + (1-\alpha)\left(||Y - X \beta_2||_2^2 + \lambda ||\beta_2||_1\right)p
        &\text{(By strict convexity of $\norm{.}_2^2$)}
    \\&\le f^*
  \end{align*}
  Contradicition.
  
\item
  ${\cal L}(\beta^*, \lambda) = \frac12 \norm{Y-X\beta}_2^2 + \lambda \norm{\beta}_1$

  $\partial \norm{\beta}_1 = \{ \alpha \in [-1, 1]^n, \alpha_j = sign(\hat \beta_j) \text{ when } \hat \beta_j \ne 0\}$
  
  Let $(\beta^*, \lambda^*)$ be an optimal solution, then $0 \in \partial_{\lambda} L (\beta^*, \lambda^*)$

  $\partial_{\lambda^*} L (\beta, \lambda^*) =  -X^T(Y-X\beta) +  \lambda^* \partial \norm{\beta}_1$

  Coordinate wise, this gives for all $j$:
  
  $ X_j^T (Y - X\beta) = \lambda sign(\beta_j)$ if $\beta_j \ne 0$
  
  $ - X (Y - X\beta) = \lambda \alpha_i$ if $\beta_j = 0$
  
  e.g
  
  $ \lambda^* = -sign(\beta_j^*) X_j^T(Y - X\beta^*)$ if $\beta_j^* \ne 0$
  
  $ \lambda^* \ge |2  X_j^T(Y - X\beta^*)|$ if $\beta_j^* = 0$
  

  
\item
  Let $\hat \beta$ be an optimal solution. Let $\chi = \{ j, \hat \beta_j \ne 0 \}$, and let's suppose it is non empty.
  Let $j$ such that $\hat \beta_j > 0$ (If such $j$ exists)
  
  By 2.2,  $\lambda = X_j^T (Y - X \hat \beta)$, but since $\lambda > \norm{X^TY}_{\infty} \ge X_j^TY$, then $X_j^TX\hat \beta > 0$.
  
  Similarly, if there for $j$ such that $\hat \beta < 0$, $X_j^TX\hat \beta < 0$.
  
  c/c $\beta_j \ne 0 \implies \beta_j X_j^TX\hat \beta > 0$

  
  \begin{align*}
    \frac12 \norm{Y - X\beta}_2^2 + \lambda \norm{\beta}_1
    &= \frac12 \norm{Y}_2^2 - \hat \beta^TX^TY + \frac12 \beta^T X^TX \hat \beta +  \lambda \sum_{i \in \chi} |\hat \beta_i|
    \\&\ge \frac12 \norm{Y}_2^2 + \sum_{i \in \chi} |\hat \beta_i| (\lambda - |X_i^TY|)
        + \frac12  \underbrace{\sum_{i \in \chi} \hat\beta_i X_i^TX\hat \beta}_{> 0}
    \\&> \frac12 \norm{Y}_2^2
    \\&= \frac12 \norm{Y - X0}_2^2 + \lambda\norm{0}_1
  \end{align*}
  Contradiction, so $\hat \beta = 0$
\item
  $$\lambda \in [\lambda_0, \lambda_1]$$
  Let $\chi(\lambda) = \{ j, \hat \beta_j(\lambda) \ne 0 \} := \chi$, $r = |\chi|$ (doesn't depend on $\lambda$ by assumption)

  We have proved in 2.2 that there exist $\alpha(\lambda)$
  $$X^T(Y - X\hat \beta(\lambda)) = \lambda \alpha(\lambda) $$
  where $\alpha(\lambda) \in \partial \norm{\hat \beta(\lambda)}_1$.

  It is easy to see that this KKT conditions is actualy necessary and sufficient (because we are minimizing a convexe function), since we are assuming uniqueness, $\hat \beta(\lambda)$ is the unique solution to :
  $$(\exists \alpha(\lambda) \in \partial \norm{\hat \beta(\lambda)}_1) \; X^T(Y - X\hat \beta(\lambda)) = \lambda \alpha(\lambda) $$
  
  Note that by uniqueness of $X\beta$ and $\hat \beta(\lambda)$, $\alpha(\lambda)$ is unique when $\lambda > 0$.
  
  Note also, that since we assumed that the signs and support are unchanged, $\partial \norm{\hat \beta(\lambda)}_1 = \partial \norm{\hat \beta(\lambda_0)}_1$.
  
  The last condition becomes:
  $$X^T(Y - X\hat \beta(\lambda)) \in \lambda \partial \norm{\hat \beta(\lambda_0)}_1$$


  \textbf{Notation: } $ \alpha(\lambda_0) =  X^T \underbrace{\frac{(Y - X\hat \beta(\lambda_0))}{\lambda_0}}_v = X^Tv$,  $\gamma_0 = X^{\dagger} v$, $\delta = \hat \beta(\lambda_0) - (\lambda - \lambda_0) \gamma_0$. 

  Note that:
  $$X^TX\gamma_0 = X^T XX^{\dagger} v = (V \Lambda U^T) (U\Lambda V^T)  (V \Lambda^{-1} U^T) v = V \Lambda U^T v = X^T v = \alpha(\lambda_0)$$
  
  
  \begin{align*}
    X^T(Y - X\delta)
    &= \underbrace{X^T(Y - X\hat \beta(\lambda_0))}_{\lambda_0 \alpha(\lambda_0)} + (\lambda - \lambda_0) \underbrace{X^T X \alpha_0}_{\alpha(\lambda_0)}
    \\&= \lambda \alpha(\lambda_0) \in \lambda \partial \norm{\hat \beta(\lambda_0)}_1
  \end{align*}
  Which proves that $\hat \beta(\lambda) = \delta = \hat \beta(\lambda_0) - (\lambda - \lambda_0) \alpha(\lambda_0)$

  
\item
  \textbf{Notation}: For a vector $v$, let $v^+ = \max(v, 0), v^- = -\min(-v, 0)$, $sign(v), supp(v)$ the sign and support of $v$, $\phi(v) = (supp(v^+), supp(v^-))$
  
  The number of values $\phi(v)$ can take is finite and at most $n^2$ because $\phi(v) \in \mathcal P(\{1 \ldots n\})^2$.

  Notice that in the last part, we have proven a stronger result:
  if for $\lambda_1, \lambda_2$, $\phi(\beta(\lambda_1)) = \phi(\beta(\lambda_2))$, then $\beta(\lambda_2) = \beta(\lambda_1) - (\lambda_2 - \lambda_1) \gamma_0$, where $\gamma_0$ depend only on $\lambda_1$. This proves a segment of the path $C$ is fully caracterized by the $\phi(v)$ where $v(C)$ is one of the element of $C$ chosen arbitrarly.
  

  Let $\mathcal A$ denote the set of segments that form the lasso path, and consider the following application:
  
  ${\cal A} \rightarrow {\cal B}; C \rightarrow \phi(v(C)) $  Where $v$ is an arbitrary element in $C$.

  We have proven that this application is injective, so $|{\cal A}| \le n^2 < \infty $.

\end{itemize}

\section{P3}
\begin{itemize}
\item

  Let's consider the unconstrained optimization problem:
  $$\min ||Y - X \beta||^2$$
  $\beta$ is optimal iff $X^TY = X^TX\beta$.
  
  We check easily that $(X^TX)^{\dagger}X^TY$ is a solution to the last equation, therefore it minimizes the $L_2$ risk.
  
  If $t > \norm{(X^TX)^{\dagger}X^TY}_{L_1}$, then it is also solution to the following problem:   $\min_{\norm{\beta}_{L_1} \le t} ||Y - X \beta||^2$.
  
\item
  1.)
  $X_i, Y_i, i \in V_k$ and $\hbeta_{\hat t}^{V_k}$ are independent.
  $(Y - X^T \hbeta_{\hat t}^{V_k})^2 \le |Y|^2 + \norm{X}_{\infty}^2  \norm{\hbeta_{\hat t}^{V_k}}_1^2 \le b^2 (1 + \hat t^2) \le b^2(1 + t_n^2)$
  
  $$\underset{X_i, Y_i, i \in V_k}{\pr} \left( \left|\frac{1}{|V_k|} \sum_{i \in V_k} (Y_i - X_i^T\hbeta_{\hat t}^{V_k})^2 - \esp_{X, Y}[(Y - X^T\hbeta_{\hat t}^{V_k})^2]\right| > \varepsilon\right) \le 2 \exp( - \frac{|V_k| \varepsilon^2}{2b^4(1 + t_n^2)}) $$

  $${\pr} \left( \frac1K \sum_k R(\hbeta^{V_k}_{\hat t}) - \hat R_{CV}(\hat t) > \varepsilon\right) \le 2K \exp( - \frac{n \varepsilon^2}{2Kb^4(1 + t_n^2)}) $$

  Since $R$ is convexe, $\frac1K \sum_k R(\hbeta^{V_k}_{\hat t}) \ge R(\frac1K \sum_k \hbeta^{V_k}_{\hat t}) \ge R(\hbeta_{\hat t})$, so:

  $${\pr} \left( R(\hbeta_{\hat t}) - \frac1K \sum_k R(\hbeta^{V_k}_{\hat t}) - \hat R_{CV}(\hat t) > \varepsilon\right) \le 2K \exp( - \frac{n \varepsilon^2}{2Kb^4(1 + t_n^2)}) $$
  2.)
  $$\hat R_{CV}(\hat t) - \hat R_{CV}(t_{\max}) \le 0$$

  
  4.)

  $$\hat R(\hbeta_{t_{\max}}) = \hat R(\hbeta_{t_n})$$
  
  5.)  Hoefdding:
  $$ \pr(\hat R(\hbeta_{t_n}) - R(\hbeta_{t_n}) > \varepsilon) \le 2 \exp(-\frac{n \varepsilon^2}{2b^4(1+t_n^2)})$$
  6.) We did this one in class:
  $$\pr\left(R(\hat \beta) - R(\beta^*) > 2 (1 + t_n^2) \sqrt{\frac{2b^4\log(\frac{2d^2}{\delta})}n} \right) \le \delta$$
\end{itemize}


\section{Proof of Hoeffding}
  
\begin{itemize}
\item
  \begin{align*}
    \pr( X \ge t) &= \pr( e^{\lambda X} \ge e^{\lambda t}) &\text{(because $\exp$ is increasing)}
    \\& =  \pr( e^{\lambda (X - t)} \ge 1)
    \\&\le \esp[e^{\lambda (X - t)} ] &\text{(Markov inequality)}
    \\&= e^{-\lambda t} \prod \esp e^{\lambda x_i} &\text{(By independence)}
    \\& = e^{-\lambda t} (\esp e^{\lambda x_i})^k &x_i \sim x_1
    \\& = e^{-\lambda t} (\frac12 (e^{\lambda} + e^{-\lambda}))^k 
  \end{align*}
\item
  $e^{\lambda} + e^{-\lambda} = \sum^{\infty} \frac{\lambda^k}{k!} + (-1)^k \frac{\lambda^k}{k!} = 2 \sum^{\infty}  \frac{\lambda^{2k}}{(2k!)}$
  $e^{\frac12 \lambda^2} = \sum \frac{\lambda^{2k}}{2^k k!}$
  Since $\frac{(2k)!}{2^kk!} = \prod_{j = 1 \ldots 2k} \frac{k+j}{2} \ge 1$, $\frac12 e^{\lambda} + \frac12 e^{-\lambda} \le e^{\frac12 \lambda^2}$

\item $\pr(X \ge t) \le e^{-\lambda t} (\frac12 e^{\lambda} + \frac12 e^{-\lambda})^k \le e^{-\lambda t} e^{\frac k2 \lambda^2}$
  $\lambda \rightarrow \frac k2 \lambda^2 - \lambda t$ is quadratic function that attain its minimum for $\lambda = \frac tk$, and the minimum is $-\frac{t^2}k$, therefore $\pr(X \ge t) \le e^{-\frac{t^2}k}$.
\end{itemize}


\end{document}



















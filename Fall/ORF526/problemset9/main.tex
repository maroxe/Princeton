\documentclass[12pt]{article}

% packages
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{amsthm}





% custom commands
\newcommand{\Q}[1]{\subsubsection*{Question #1}}
\newcommand{\OB}[1]{\overbrace{#1}^{\in F_n}}
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}

% parameters
\geometry{hmargin=1cm,vmargin=1cm}
\title{ORF526 - Problem Set 9}
\author{Bachir EL KHADIR }


\begin{document}

\maketitle

\Q{1}
Because it is predictable we have that $E[M_{n+1}|F_n] = M_{n+1}$, and because it is a martingale $E[M_{n+1}|F_n] = M_n$. Therefore $M_n$ is constant equal to $M_0$.

\Q{2}
Let $M_n = X_0 + \sum_{i=0}^{n-1} X_{i+1} - E[X_{i+1}|F_i]$, $A_n = X_n - M_n = \sum_{i=0}^{n-1}E[X_{i+1}|F_i] - X_i$ and as a convention $A_0 = 0$.

Then

\begin{itemize}
\item $(M_n)$ is an $(F_n)$ martingale because
  \begin{itemize}
  \item It is $(F_n)$-adapted: For all $n$, $M_n$ for $i = 0... n-1$, $X_{i+1}$ and $E[X_{i+1}|F_i]$ are $F_n$ measurable.
  \item $M_{n+1} - M_n = X_{n+1} - E[X_{n+1} | F_n]$, so $$E[M_{n+1}|F_n] - M_n = E[M_{n+1} - M_n | F_n] = E[X_{n+1}|F_n] - E[E[X_{n+1} | F_n]|F_n] = 0$$
  \end{itemize}
\item $(A_n)$ is a non-decreasing predictable process because:
  \begin{itemize}
  \item $(A_n)$ is predictable because for $i < n$, $X_i$ and $E[X_{i+1}|F_i]$ are $F_i$ (and $F_{n-1}$) measurable
  \item $(A_n)$ is non-decreasing: $A_{n+1} - A_n = E[X_{n+1}|F_n] - X_n \ge 0$ because $X_n$ is a submartingale.
  \end{itemize}
\end{itemize}
The decomposition is unique, because if there exist an other decomposition $X_n = M'_n + A'_n$ with the same properties then:
$M_n - M'_n = A_n - A'_n$, which is a marintgale (as the difference of two martingales), and predictable (as the difference of two predictable processes). By question 1, this sequence is constant equal to $A_0 - A'_0 = 0$

\Q{3}
Let $n, p \in \mathbb{N}$,

By the itereated expectation: $$E[M_{n+i+1}M_{n+i}] = E[E[M_{n+i+1}|F_n]M_{n+i}] = E[M_n^2]$$
So:
$$||M_{n+p} - M_{n}||_2^2 = E[|M_{n+p} - M_n|^2] = E[M_{n+p}^2] + E[M_n^2] - 2 E[M_{n+p}M_n] = E[M_{n+p}^2] - E[M_{n}^2]$$

$M_n^2$ is a submartingale, so $E[M_n^2]$ is non-decreasing, and since it is bounded, it converges and therefore $E[M_{n+p}^2] - E[M_{n}^2] \rightarrow_{n,p} 0$
c/c: $||M_{n+p} - M_{n}||_2 \rightarrow_{n,p} 0$, and $(M_n)$ is a cauchy sequence.

\Q{4}
\begin{enumerate}
\item $M_n$ is $L^p$ bounded and $p > 1$, so $(M_n^p)$ is submartingale that is $L_1$ bounded, and therefore: $|M_n|^p$ converges to $S \in L_1$ a.s.
  Moreover, $M_n$ is $L_1$ bounded so it has an as limit
  $M_{\infty}$.
  Therefore $M_{\infty}^p = S \in L_1$,
  so $M_{\infty} \in L_p$

\item
  Let $\Omega$ the set of measure 1 where $S_n = \max_{k \le n} |M_k|$ holds. Then for every $\omega \in \Omega$,
  $S_n(\omega)$ is pointwise non-decreasing as., so it has a limit $S(\omega)$, and by monotonuous convergence theorem $E[S_n^p] \rightarrow E[S^p]$
  By Doobs inequality
  $$||S_n||_p = || \max_{k \le n} |M_k| ||_p \le \frac{p}{p-1}  ||M_n||_p \le \frac{p}{p-1} \sup_k ||M_k||_p$$
  By taking the limit, and taking the $p$-th power:
  $$E[S^p] = || \max_{k \le n} |M_k| ||_p^p \le \left(\frac{p}{p-1} \right)^p \sup||M_k||_p^p$$

\item
  Since $|M_{\infty} - M_n| \le  |M_{\infty}| + |M_n|$, $|M_n| \le S_n \le S$ and $|M_{\infty}| = \lim_n |M_n| \le \lim_n S_n \le S$:
  $$|M_{\infty} - M_n| \le  2 S$$
  Therefore $|M_{\infty} - M_n|^p \le  2^p S^p \in L_1$,and by Dominated Convergence theorem $E[|M_{\infty} - M_n|^p] \rightarrow 0$

\end{enumerate}

\Q{5}
Let $F_n$ be the filtration generated by $B_n$
\begin{enumerate}
\item $B_n$ is $F_n$ adapted, so is $B_n^2-n$
\item $B_n^2$ is $L_1$, and
  \begin{align*}
    E[B_{n+1}^2-(n+1) | B_n]
    &= E[(B_{n+1} - B_n + B_n)^2 - (n+1)]
    \\&= E[(B_{n+1} - B_n)^2 | B_n] + E[B_n^2|B_n] + 2 E[ (B_{n+1} - B_{n})B_n|F_n] - (n+1)
    \\& = E[ \mathcal N(0,1)^2 ] + B_n^2 - (n+1)
    \\&\text{(because  $B_{n+1}-B_n \sim \mathcal N(0,1)$ and is independent from $B_n$)}
    \\& = B_n^2 - n
  \end{align*}

so $B_n^2 - n$ is a martingale.

\item
  $\exp(\sigma B_{n+1} - \frac12 \sigma (n+1)^2)$ is $L_1$ (using the moment generating function of the normal distribtion)
  
  $Y_i = B_{i+1} - B_i \sim \mathcal N(0,1)$ and independent from $F_i$.

  $E[\exp(\sigma B_{n+1} - \frac12 \sigma (n+1)^2) | F_n]
  = \exp(\sigma B_n - \frac12 \sigma (n+1)^2) E[\exp(\sigma Y_n) | F_n]
  = \exp(\sigma B_n - \frac12 \sigma (n+1)^2) \exp(\frac12 \sigma^2) 
  = \exp(\sigma B_n - \frac12 \sigma n^2) $

\item
  By defintion of the borwnian motion $(B_t)$, $B^{(m)}$ is a normal vector that has expectation $0$ and covariance matrix:
  $$cov(B^{(m)}_{i/m}, B^{(m}_{j/m})
  = \frac{1}{m} cov(B_i, B_j)
  = \frac1 m  (i \wedge j) = (i/m \wedge j/m)$$

  So $B^{(m)}$ has the same distribution as $(B_n)_{n=1..m}$
  \begin{itemize}
  \item It's a martingale
  \item The increments are independent
  \item It is an discretisation of the brownian motion
  \end{itemize}
  
\end{enumerate}

\Q{6}
\begin{enumerate}
\item
  \begin{align*}
    a\log(b) \le a log(a) + \frac b e
    &\iff \log(b/a) \le \frac b a e^{-1}
    \\&\iff  \log(x) \le \frac x e 
    & (x = \frac b a)
    \\ & \iff \log(x) \le \log'(e) (x-e) + \log(e)\\ &\text{(true because $\log$ is concave)}
  \end{align*}

\item
  \begin{itemize}
  \item if $b \le 1$, then the inequality in equivalent to $0 \le a log^+(a) + b/e$ which is true because all the terms are non negative.
  \item if $b > 1$ and $a > 1$, the inequality in equivalent to the one of question 1.
  \item if $b > 1$ and $a \le 1$, by question 1, $a\log(b) \le 1 \log(b) \le 1 \log(1) + \frac{b}e$, or $a \log^+(n) \le \frac{b}{e}$ which is what we want to prove.
  \end{itemize}
  
\end{enumerate}

\Q{7}

\begin{enumerate}
\item It is clear that
  \begin{itemize}
  \item $E[X|G]$ is $\sigma(G, H)$ measurable
  \item $E[X|G]$ is $L_1$
  \end{itemize}
  In addition, by linearity of the conditional probability, we only need to show that it holds for non negative rv. Let's assume $X \ge 0$ as. 

  Let's consider the two measures on $\sigma(G, H)$: 
$\mu_1: C \rightarrow E[ 1_C E[X|G]]$, and $\mu_2: C  \rightarrow E[ 1_C X]$, and consider $D$ the set on which they agree.

  Also, the set $D$ on which they agree is a Dynkin system:
  \begin{itemize}
  \item $\Omega \in D$ because $\mu_1(\Omega) = \mu_1(\Omega) = E[X]$.
  \item if $A \subset B$, so is $B\setminus A$ because$\mu_1(B \setminus A) = \mu_1(B) - \mu_1(A) = \mu_2(B) - \mu_2(A) = \mu_2(B \setminus A)$.
  \item  if $A_i$ is an non-dreasing sequence of subsets in $D$, then $\cup A_i \in D$, because
    $\mu_1( \cup_i A_i) = \lim_i \mu_1(A_i) = \lim_i \mu_1(A_i) = \mu_2( \cup_i A_i)$
  \end{itemize}

  
    Let's define $S := \{ A \cap B | A \in G, B \in H \}$,
  $H \cup G \subset S \subset \sigma(H,G)$, so $S$ is a generator of $\sigma(H,G)$. Furthermore, it is stable by intersection. It is then a $\pi$-system.

    Let $A \in G, B \in H$, and let's show that $E[ 1_{A \cap B} E[X|G]] = E[1_{A \cap B} X]$.

  Indeed by indepdence of $E[1_AX|G]$ and $1_B$:
  $E[ 1_A 1_B E[X|G]] = E[1_B E[1_A X | G]] = E[1_B] E[ E[1_AX|G] ] = E[1_B] E[1_AX] = E[1_B 1_AX]$


  So $\mu_1, \mu_2$  agree on $S$, e.g. $S \subset D$, 
  By $\pi-\lambda$ theorem, $\sigma(S) = \sigma(H, G) \subset D$, eg $\mu_1, \mu_2$ agree everywhere.
  
  As a result $E[X|G] = E[X | \sigma(G,H)]$
  
\item Let $X, Y$ two independent bernouilli variable (taking values 1 and 0 with probability $\frac12$). And let $Z = X + Y \operatorname{mod} 2$. Then it is easy to verify (see problemset 1) that:
  \begin{itemize}
  \item $\sigma(Y)$ and $\sigma(Z)$ are independent, because $P(Y = a, Z = b) = P(Y = a, X = b+a \, mod(2) ) = P(Y = a) P(X = b+a\, mod(2) ) = \frac1 4 = P(Y = a)P(Z = b)$, for $a, b \in \{0, 1\}$
  \item $\sigma(X)$ and $\sigma(Z)$ are independent (same argument).
  \end{itemize}
  But
  \begin{itemize}
  \item $E[X | \sigma(Y,Z)] = Z - Y \operatorname{mod}(2) = X$
  \item By independence $E[X | \sigma(Z)] = E[X] = \frac12 \ne X$ 
  \end{itemize}
  
\end{enumerate}
\Q{8}
By indepedence of the $X_i$, $E[X_1 | S_n, S_{n+1}...] = E[X_1 | \sigma(S_n, \sigma(X_{n+1}, ...))] = E[X_1 | S_n]$ because $\sigma(X_1, S_n)$ and $\sigma(X_{n+1}, ...)$ are independent.

By symmetry, when $i \le n$, $E[X_i|S_n] = E[X_1 | S_n]$, and therefore:

$S_n = E[S_n | S_n] = \sum_{i=1}^n E[X_i | S_n] = n E[X_1 | S_n]$,

ie $E[X_1 | S_n, S_{n+1}...] = \frac{S_n}{n}$

\end{document}


\documentclass[12pt]{article}

% packages
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}


% custom commands
\newcommand{\Q}[1]{\subsubsection*{Question #1}}

% parameters
\geometry{hmargin=1cm,vmargin=1cm}
\title{ORF526 - Problem Set 7}
\author{Bachir EL KHADIR }

\begin{document}

\maketitle

\Q{1} 
\begin{enumerate}
\item
  It is easy to check that $C$ is symmetric and that:
\[
C(s,t) = \left\{ \begin{array}{cc}\min(|s|, |t|) & \text{if } ts \geq 0 \\ 0 & \text{otherwise}\end{array} \right.
\]
  Let $(t_i)_1^n \in \mathbb{R}$, and $f_i = 1_{(0, t_i)}$ where 
$(0, t) = (t, 0)$ if $t < 0$, then:  $C(t_i, t_j) = \int_R f_i f_j$ which is a scalar product in $L_2$.
  $C$ is definite positive semi-definite as a conclusion.

\item $C(t, s) = \min(t, s)$ when $t, s \ge 0$.

\item \begin{itemize} 
\item $Var(B_0) = 0$ so $B_0 = 0$ as. 
\item $B_t - B_s$ is normal because $B_t$ is a guaussian process.
  $E[B_t - B_s] = 0$, and $Var(B_t - B_s) = Var(B_t) + Var(B_s) + 2 Cov(B_t, B_s) = |t| + |s| + 2 C(t,s) = |t-s|$
\item $Cov(B_t - B_s, B_u - B_v) = C(t,u) + C(s, v) - C(s, u) - C(t, v) = \frac 1 2 (|t| + |u| - |t-u| + |s| + |v| - |s-v| - |s| - |u| + |s-u| - |t| - |v| + |t-v| = \frac12 ( u -t + v - s + u - s + v - t) = 0$, and since  the 2d process $B_t - B_s, B_u - B_v$ is guassian, its compenonets are independent.
\end{itemize}
\end{enumerate}

\Q{2}
Let's call $C_1$ the function $C$ definited on quesiton 1.
\begin{enumerate}
\item $C(u, v) = C_1(u_1, v_1) C_1(u_2, v_2) = \int_{R^{2}} 1_{(0, u_1)}(x)1_{(0, v_1)}(x) 1_{(0, u_2)}(y)1_{(0, v_2)}(y)dx dy = \int_{R^2} 1_{(0, u_1) \times (0, u_2)}1_{(0, v_1) \times (0, v_2)} = \, <1_{(0, u_1) \times (0, u_2)},1_{(0, v_1) \times (0, v_2)}>$

So $C$ is positive semi-definite.

\item  $C(u, v) = \min(u_1, v_1) \min(u_2, v_2)$ when $u, v \ge 0$

\item if one compononet of $u$ is $0$, then $Var(X_u) = C(u_2, u_2)C(u_1, u_1) = 0$, ie $X_u = 0$ as.

\item $B_t = X_{(t,1)}$ is a guaussian process. $E[B_t] = 0$ and $Cov(B_t, B_s) = Cov(X_{(t,1)}, X_{(s,1)}) = C_1(t, s)$, so $B_t$ is a two sided browninan motion

\item $Var(X_{(t, t)}) = C_1(t,t)^2 = |t|^{2}$
\end{enumerate}

\Q{3}
Let's first show the following lemma:
For every $X \in L_1$, there exist a sequence of simple function $Z_n$ bounded by $|X|$ and converging to $X$.
To show that, we write $X = X^+ - X^-$, and let $Z_n^+$ (resp. $Z_n^-$)  a sequence of positive simple functions converging to $X^+$ (resp. $X^-$) from below (resp. above). And set $Z_n = Z_n^+ - Z_n^-$, which verifies the lemma.


\begin{enumerate}
\item $X$ is $G$ mesurable, and trivially verifies the definition of conditional probability, so $E[X|G] = X$
\item $aE[X|G] + bE[Y|G]$ is $G$-measurable as sum of two functions that are $G$-measurable, and if $A \in G$:

\begin{align*}
E[ (aE[X|G]+bE[Y|G])1_A ] &= a E[E[X|G]1_A]+ bE[E[Y|G])1_A]
\\&= a E[E[X1_A|G]]+ bE[E[Y1_A|G]] &\text{because $A$ is $G$-measurable}
\\&= a E[X 1_A]+ b E[Y 1_A]
\\&=  E[(aX +bY) 1_A]
\end{align*}
 so $E[aX+bY|G] = aE[X|G]+bE[Y|G]$.
\item 
$E[X | G] - E[Y | G] = E[X - Y | G]$
Let $H := E[X - Y | G]$, and $A := \{H \le 0\}$. $A$ is $G$-measurable and by positivity of the expectency: $0 \ge E[H 1_A] = E[(X-Y) 1_A] \ge 0$.

Since $-H1_A \le 0$ a.s and its expectency is 0,  $H1_A = H^- = 0$ as, and therefore $H \ge 0$ as.
\item For $A \in H \subseteq G$, $E[E[X|G] | H]$ is $H$-measurable and :
\begin{align*}
E\left[1_A E[E[X|G] | H]\right] 
&= E[ 1_A E[X|G] ] 
\\&= E[ 1_A X ] 
\end{align*}


\item Let $A \in G$, and prove that $E[ 1_A Y E[X|G] ] = E[1_A XY ]$
If we denote $Z := 1_A Y$, this is equivalent to $E[ Z E[X|G] ] = E[ZX ]$.

 $Z$ is $G$-measurable and $|ZX| \le |YX| \in L_1$
\begin{itemize}
\item If $Z$ is a simple function $\sum_{i=0..n} \alpha_i 1_{A_i}$, where $A_i \in G$ for $i = 0..n$, then by linearity of the expectation:
$$E[ Z E[X|G] ] = \sum_i \alpha_i E[1_{A_i} E[X|G]] = \sum_i \alpha_i E[1_{A_i} X] = E[ZX]$$
\item If $X$ and $Y$ are non-negative, Let $Z_n$ be a sequence of non-negative simple $G$-measurable functions s.t. $Z_n \uparrow Z$ and therefore 
$|Z_nX| \le |ZX| \in L_1$. By monotnous convergence theorem:

$$ E[ZE[X|G]] = \lim E[Z_nE[X|G]] = \lim E[Z_nX] = E[ZX] $$


\item $X$ now can be in $L_1$. 

We use $h)$, to show that $|E[X|G]| < E[|X||G]$. (take $\phi : x \rightarrow |x|$)

Let $Z_n$ a sequence of simple functions converging to $Z$ and bounded by $|Z|$. Then 
$|Z_n X| \le |ZX| \in L_1$ and $|Z_nE[X|G]| = |E[Z_nX|G]| \le E[|XZ| | G] \in L_1$ because $EE[|XZ| | G] = E[|XZ|] < \infty$.

By dominated convergence theorem:

$$ E[ZE[X|G]] = \lim E[Z_nE[X|G]] = \lim E[Z_nX] = E[ZX] $$

\item If $Y \in L_1$, $Z = Z^+ - Z^-$, and by linearity

$$E[Z E[X|G]] = E[Z^+ E[X|G]] - E[Z^- E[X|G]] = EE[XZ^+|G] - EE[Z^-X|G] = E[XZ^+] - E[XZ^-] = E[XZ]$$


\end{itemize}

\item 


Let's first prove that if $A \in G$, $E[X1_A] = E[X] E[1_A]$.
\begin{enumerate}
\item If $X$ is an indicator function, then it follows from the definition of independence
\item If $X$ is a simple function it follows from the linearity of the expectation.
\item If $Z_n$ a sequence of simple functions converging to $X$ and uniformly bounded by an $|X|$, then by CVD:
$$E[X1_A] = \lim E[Z_n1_A] = \lim E[Z_n] E[1_A] = \lim E[X] E[1_A]$$
\end{enumerate}
So now we have:
$$E[1_A X] = E[1_A]E[X] = E[1_AE[X]]$$
$E[X]$ is a constant, so $G$-measurable.

\item 
$$E[X 1_{\emptyset}] = 0 = E[X] E[1_{\emptyset}]$$
$$E[X 1_{\Omega}] = E[X] = E[X] E[1_{\Omega}]$$

so $X$ is independent of $G$, and therefore $E[X|G] = E[X]$.

\item 

If $\varphi$ is affine $ = ax + b$, then by linearity
$E[\varphi(X) | G] = \varphi(E[X|G])$

If $\varphi$ is convex not linear, we can write $\varphi = \sup \varphi_n = \sup_n a_n x + b_n$ where $a_n, b_n \in R$, then $\forall n \, E[\varphi(X) | G] \ge E[\varphi_n(X)|G] \ge \varphi_n(E[X|G])$ as.
Let $\Omega_n$ the set where this equality holds, so on $\Omega' := \cap_n \Omega_n$ we have that:
 $$ E[\varphi(X) | G] \ge \sup_n \phi_n(E[X|G]) = \varphi(E[X|G]) \text{ on } \Omega'$$
and $P(\Omega') = 1 - P(\cup_n \Omega_n^c) \ge 1 - \sum_n P(\Omega_n^c) \ge 1$

\end{enumerate}

\Q{4}

\begin{itemize}
\item 

$E[X_n|Y]$ is non-decreasing, let's call $L := \lim E[X_n|Y]$, and prove that $L = E[X|G]$.

Since $Y \le X_n \uparrow X$, $Y\wedge n \le X \wedge n\uparrow X $  and $E[Y|G] \le E[X_n|G] \uparrow L$, by monotonous convergence theorem, for all $A \in G$:
\begin{align*}
E[1_A L] &= \lim_n E[1_A  E[X_n|G]] \\&= \lim_n E[1_A X_n] \\&= E[1_A X] \\&= \lim E[1_A (X \wedge k) ] \\&= \lim E[1_A E[X \wedge k]] \\&= E[1_A E[X|G]]
\end{align*}


Let's note $H := L - E[X|G]$ which is $G$-measurable because $L$ and $E[X | G]$ are both $G$-measurable, and we have $E[1_{H < 0} H] = 0$, so $H = 0$ as, ie $L = E[X|G]$

\item
Let's define $L_k := \inf_{n \ge k} X_n \le X_k$, so that 
\begin{align}\label{eq:1}
E[L_k|G]  \le E[X_k|G]
\end{align}

But $Y \le L_k \uparrow \lim \inf_n X_n$, by a) $E[L_k|G] \uparrow_k E[\lim \inf_n X_n]$, and by taking the $\lim\inf$ in the inequality \ref{eq:1} we have the result.

\item 

$X_n$ and $-X_n$ verify the conditions of the last quesiton, so:
$$\lim \inf E[-X_n|G] \ge E[\lim \inf -X_n|G] \Rightarrow \lim \sup E[X_n|G] \le E[\lim \sup X_n|G] $$
$$\lim \inf E[X_n|G] \ge E[\lim \inf X_n|G]$$

as a result 

$$  E[\lim \sup X_n|G] \ge  \lim \sup E[X_n|G] \ge \lim \inf E[X_n|G] \ge E[\lim \inf X_n|G]$$

Since $\lim \sup X_n = \lim \inf X_n = X$, we have the result.
\end{itemize}

\Q{5}

\begin{itemize}
\item 
$y \rightarrow p(y, A)$ is measurable because:

\begin{enumerate}
\item $(x, y) \rightarrow f(x, y)$ is measurable since it is a density
\item $N(y)$ is measurable by Fubini
\item $y \rightarrow \int \frac{f(x,y)}{N(y)}1_{0 <N(y) <\infty} + (1-1_{0 <N(y) <\infty}) \varphi(x)$ is also measurable by Fubini
\end{enumerate}

$p(Y, A)$ is then $\sigma(Y)$-measurable.


\item 

We have that:
\[N(y) f_y(x) = \left\{ \begin{array}{cc}
\phi(x) & \text{if $N(y) = 0$}\\
\phi(x) & \text{if $N(y) = \infty$}\\
f(x,y) & \text{otherwise}
\end{array}\right. \]


But since $N \in L_1$, the set $\{N = \infty\}$ is of measure 0, so $N(y) f_y(x) = 1_{N(y) \ne 0} f(x,y) + 1_{N(y) = 0}\phi(x)$ a.s.

Let $B \in B(R)$, all function integrated below are non negative, so:
\begin{align*}
E[p(Y, A) 1_{Y \in B}] &= \int_{R^2} p(y, A) 1_{y \in B}f(x,y)dx dy \\
&= \int p(y, A) 1_{y \in B} N(y) dy &\text{By Tonnelli} \\
&= \int_y 1_{y \in B} \int_x 1_{x \in A} N(y) f_y(x) dx dy  \\
&= \int 1_{y \in B} 1_{x \in A} 1_{N(y) \ne 0} f(x, y) dx dy  \\
&= \int 1_{y \in B} 1_{x \in A} f(x, y) dx dy & \text{because if $N(y) = 0$ then $\int_A f(x, y) = 0$} \\
&= E[1_{Y\in B} 1_{X \in A}]
\end{align*}
\end{itemize}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:





\documentclass[12pt]{article}

% packages
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}


\newcommand{\Q}[1]{\subsubsection*{Problem #1}}
\newcommand{\optimize}[4]
{
\begin{align*}
& \underset{#1}{\text{#4}}
& & #2 \\
& \text{subject to}
& & #3
\end{align*}
}



\newcommand{\union}[1]{\underset{#1}{\cup} }
\newcommand{\bigunion}[1]{\underset{#1}{\bigcup} \, }
\newcommand{\inter}[1]{\underset{#1}{\cap} }
\newcommand{\biginter}[1]{\underset{#1}{\bigcap} }

\newcommand{\minimize}[3]{\optimize{#1}{#2}{#3}{min}}
\newcommand{\maximize}[3]{\optimize{#1}{#2}{#3}{max}}


\newcommand{\icol}[1]{% inline column vector
  \left(\begin{smallmatrix}#1\end{smallmatrix}\right)%
}

\newcommand{\irow}[1]{% inline row vector
  \begin{smallmatrix}(#1)\end{smallmatrix}%
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% parameters
\geometry{hmargin=1cm,vmargin=1cm}
\title{ORF522 - Problem Set 5}
\author{Bachir EL KHADIR }

\begin{document}



\Q{1}
\begin{enumerate}
\item Let's consider $g: u \rightarrow \log(1+e^u)$.  $g$ is
  non-decreasing and convexe because
  $g'(u) = \frac{e^u}{1+e^u} = \frac1 {1+e^{-u}}$ is increasing.

  We notice that $f(x_1, x_2) = g(x_1 - x_2) + x_2$.

  \begin{itemize}
  \item $x_2 \rightarrow x_2$ is linear
  \item $x_2 \rightarrow x_1 - x_2$ is linear, $g$ convexe and
    non-decreasing, so $g(x_1 - x_2)$ is convexe
  \end{itemize}

  c/c: $f$ is convexe.

\item
  The following transformation is a bijection from $(2, 3) \times (0, \infty) \times (0, \infty)$ to  $(\frac{\log 2}2, \frac{\log 3}2) \times \mathbb R \times \mathbb R$
  \begin{align*}
    x_1 &= 2 \log x \\
    x_2 &= \log y - \log z\\
    x_3 &= \log y
  \end{align*}

  $\frac x y  = z^2 = e^{2\log z} = e^{2 \log y - 2 x_2} = e^{2 x_3 - 2 x_2} $
  Minimizing $\frac x y$ is the same as minimizing $a(x_1, x_2, x_3) := e^{2 x_3 - 2 x_2}$
  which is convexe as the composition of a linear function
  and a convexe and increasing one $\exp$.


  \begin{itemize}
  \item
    $\frac x y = z \iff \log x - \log y = \log z \iff  \frac12 x_1 - x_3
    = x_3 - x_2 \iff \frac12
    x_1 + x_2 - 2 x_3 = 0$ and
    $b(x_1, x_2, x_3) := \frac12 x_1 + x_2 - 2 x_3$ is linear.
  \item
    $x^2 + \frac y z \le \sqrt y \iff e^{x_1} + e^{x_2}  \le \sqrt e^{x_3}
    \iff \log(e^{x_1} + e^{x_2})  \le \frac12 x_3
    \iff f(x_1, x_2) - \frac12 x_3  \le 0
    $
    and $c(x_1, x_2, x_2) := f(x_1, x_2) - \frac12 x_3$ is convexe as the sum of two convexe functions
  \end{itemize}

  c/c: the optimization problem is equivalent to:
  $$\max a(x_1, x_2, x_2) \text{ s.t. } b(x_1, x_2, x_2) = 0, c(x_1, x_2, x_2) \le 0,
  (x_1, x_2, x_2) \in (\frac{\log 2}2, \frac{\log 3}2) \times \mathbb R \times \mathbb R $$
  which is a convexe problem.
\end{enumerate}

\Q{2}

\begin{itemize}
\item[$\Rightarrow)$] Let's suppose $f$ convexe.
  \begin{align*}
    \nabla f^T(x) (y-x)
    &= \lim_{\alpha 0} \frac{f(x + \alpha(y-x)) - f(x)}{\alpha}
    \\&= \lim_{\alpha 0} \frac{f((1-\alpha)x + \alpha y) - f(x)}{\alpha}
    \\&\le \lim_{\alpha 0} \frac{(1-\alpha)f(x) + \alpha f(y) - f(x)}{\alpha} &\text{(because $f$ convexe)}
    \\&\le f(x) - f(y)
  \end{align*}
\item[$\Leftarrow)$] Let's suppose $\forall x, y \, \nabla f^T (y-x) \le f(y) - f(x)$
  
  Let $\alpha \in (0, 1)$, and $u = (1-\alpha)x + \alpha y$
  
  $$f(x) - f(u) \ge \nabla f(u) (x-u)$$
  $$f(y) - f(u) \ge \nabla f(u) (y-u)$$

  By multiplying the first ineqaulity by $1-\alpha$ and the second one by $\alpha$ and summing, we get:
  $(1-\alpha) f(x) + \alpha f(y) - f(u) \ge 0$

  Which proves that $f$ convexe.
\end{itemize}

\Q{3}

\Q{4}
\begin{enumerate}
\item
  For $y \in \mathbb R^n$
  $\sup_y L(x, y) \ge L(x, u)$

  By taking the $\inf_x$:
  $\inf_x \sup_y L(x, y) \ge \inf_x L(x, u)$

  By taking the $\sup_u$:
  $\inf_x \sup_y L(x, y) \ge \sup_u \inf_x L(x, u)$

\item

  Let $f(x) := \max_y L(x, y)$
  We know that $x^* \in \argmin f$ and $f$ is convexe,
  so $\partial f(x^*) = 0$.

  $L$ is continuous the Danskin's theorem, we have that:
  $0 \in \{ \nabla_x L (x^*, y) | y \in \argmin L(x^*, y^*) \} = \{ \nabla_x L (x^*, y^*) \}$ , wich means that
  $\nabla_x L (x^*, y^*) = 0$, and symmetrically, $\nabla_y L (x^*, y^*) = 0$.

  c/c:
  \begin{align*}
    x^* &= x^* - \alpha \nabla_x L (x^*, y^*)\\
    y^* &= y^* - \alpha \nabla_y L (x^*, y^*)
  \end{align*}
  
\end{enumerate}

\Q{4}
\begin{enumerate}
\item By taylor equality, there exist $y \in X$ such that:
  $$f(x^k) - f(x^*) = \nabla f(x^*) (x^k - x^*) + \nabla^2 f(x^*)$$
\end{enumerate}

\Q{5}
\begin{enumerate}
\item Let's call $S_t$ the price of the stock at time t, and $C_t(S_t)$ the
  price of the corresponding american action (with strike $K$)
  \begin{itemize}
  \item State $x_t = S_t$, $t = 1..T$
  \item Action:
    \[
      u_t = \left\{ \begin{array}{cc}
                      1 & \text{meaning we exercise the option}\\
                      0 & \text{meaning we don't}
                    \end{array}
                  \right.
                \]
              \item Randomness: The change in the stock price
                $w_t = \frac{S_{t+1}}{S_t}$ s.t $x_{t+1} = w_t x_t$, $P(w_t = u) = 1 - P(w_t = d) = p$
              \item Transitional cost:
  
                $g(x_k, u_k=0, w_t) = 0$
                $g(x_k, u_k=1, w_t) = (x_t-K)^+ - C_{t+1}(x_t + w_t)$
                Explication: If we exercise the option, we gain
                $(x_t-K)^+$ but we lose the right to the option
                ($-C_{t+1}$).  $g(x_T) = (x_T - K)^+$: We are forced
                to exericse the option at time $T$
              \end{itemize}
              The price problem:
  $$C_k(x) = \max_{\mu} E[g(x_T) + \sum_{t=k}^{T-1} g(x_k, \mu_k(x_k), w_t) | x_k = x]$$

\item Bellman equation:
  $$C_k(x) = \max_{\mu_k} E[ g_k(x, \mu_k(x) , w_k) + C_{k+1}(w_kx_{k+1}) | x_k = x ]
  = \max \left\{ (x-K)^+ , p C_{k+1}(u x) + (1-p) C_{k+1}(d x) \right\}$$
  $$C_T(x) = (x-K)^+$$

\item
  
  LP:
  Let $J(t, S)$ be the price of the option at time $t$ is $S_t = S$,
  and we decide to adopt the strategy 

  $J$ verifies: $J(t, S) = \max \{ E[J(t+1, S_{t+1}) | S_t = S], S - K \} = [\max_{\mu} (P_{\mu}J + g_\mu) ](t,S) $
  where $\mu(t, S) \in \{ \text{HOLD}, \text{EXEC} \}$,
  \[ (P_\mu J)(t, S) = \left\{
      \begin{array}{cc}
        p J(t+1, uS) + (1-p)J(t+1, dS) & \text{ if $\mu(t, S) = $ HOLD } \\
        0 & \text{otherwise}
      \end{array}
    \right.
  \]
  \[
    g_\mu(t, S) = \left\{
      \begin{array}{cc}
        0 & \text{ if $\mu(t, S) = $ HOLD } \\
        S - K & \text{otherwise}
      \end{array}
    \right\}
  \]
                                                
\end{enumerate}

\Q{6}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

























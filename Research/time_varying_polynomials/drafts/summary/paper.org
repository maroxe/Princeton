#+LATEX_HEADER: \usepackage[margin=0.5in]{geometry}
#+LATEX_HEADER: \usepackage{listing}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER:\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
#+LATEX_HEADER: %\usepackage[noend]{algpseudocode}
#+LATEX_HEADER: \usepackage{amsmath} % assumes amsmath package installed
#+LATEX_HEADER: \usepackage{amssymb}  % assumes amsmath package installed
#+LATEX_HEADER: \usepackage{amsthm}


#+LATEX_HEADER: \theoremstyle{plain}  % Bold name, italics font
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{lemma}[theorem]{Lemma}
#+LATEX_HEADER: \newtheorem{proposition}[theorem]{Proposition}
#+LATEX_HEADER: \newtheorem{corollary}[theorem]{Corollary}
#+LATEX_HEADER: \newtheorem{definition}[theorem]{Definition}
#+LATEX_HEADER: \newtheorem{hyp}[theorem]{Hypothesis}
#+LATEX_HEADER: \newtheorem{idea}[theorem]{Idea}
#+LATEX_HEADER: \newtheorem{remark}[theorem]{Remark}

#+LATEX_HEADER: \theoremstyle{definition}
#+LATEX_HEADER: \theoremstyle{remark} % italics name, roman font
#+LATEX_HEADER: \newtheorem{examples}{Example}[section]

#+LATEX_HEADER: \algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%
#+LATEX_HEADER: \algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%


#+OPTIONS: toc:nil

#+TITLE: Time varying LP and SDPs
#+AUTHOR: Bachir El Khadir

* Introduction and notation
  #+NAME: img:example_tv_lp
  #+ATTR_LATEX: :float wrap :width 0.5\textwidth :placement {r}{0.4\textwidth}
  #+caption:Example of timevarying LP
  [[file:scripts/example_tv_lp.png]]

  
  In this paper we investigate time varying convex programs, i.e. programs for which the feasible set and the objective function depend continously on time $t \in [-1, 1]$. More specifically we study timevarying semidefinite and linear programs (abbreviated timevarying SDP and timevarying LP respectively):
  
  #+NAME: eqn:time_varying_sdp_l2
  \begin{equation*}
  \tag{$TV-SDP$}
  \begin{array}{ll@{}ll}
  \text{maximize} & \int_{-1}^1 \langle X(t), C(t) \rangle dt & \\
  \text{subject to}& X(t) \succeq 0 & \forall t \in [-1, 1]\\
  & \langle A_i(t), X(t) \rangle \le b_i(t) & \forall t \in [-1, 1]
  \end{array}
  \end{equation*}

  With $A_i(t) , X(t), C(t) \in \mathbb S_n, b_i(t) \in \mathbb R$ for all $t \in [-1, 1], i=1\ldots m$ and $\mathbb S_n$ is the set of $n \times n$ symmetric matrices.
  
  #+NAME: eqn:time_varying_lp_l2
  \begin{equation*}
  \tag{$TV-LP$}
  \begin{array}{ll@{}ll}
  \text{maximize} & \int_{-1}^1 \langle c(t), x(t) \rangle dt & \\
  \text{subject to}& A(t) x(t) \le b(t) & \forall t \in [-1, 1]
  \end{array}
  \end{equation*}
  
  with $A(t) \in \mathbb R^{m \times n}, x(t), c(t) \in \mathbb R^n$ for all $t \in [-1, 1]$

  
  The optimization variable is a function $x: [-1, 1]\rightarrow \mathbb R^n$. When such function verifies the program constraints for all $t \in [-1,1]$, we call it a /feasible function/. Note that timevarying LPs are a specific case of timevarying SDPs, hence properties that apply to the latter apply to the former as well. We focus on timevarying LPs however because they are easier to study and provide insights into the more general problem of SDPs.
  
  Notice that in general the optimal solution to a timevarying SDP is the function that we denote by $X^{opt}(t)$ that maximize the following program for almost all times $t \in [-1, 1]$ :

  #+NAME: eqn:time_varying_sdp_t
  \begin{equation*}
  \tag{$SDP_t$}
  \begin{array}{ll@{}ll}
  \text{maximize} & \langle X(t), C(t) \rangle & \\
  \text{subject to}& X(t) \succeq 0\\
  & \langle A_i(t), X(t) \rangle \le b_i(t) \; i=1\ldots m
  \end{array}
  \end{equation*}
  
Or in the case of LPs:

  #+NAME: eqn:time_varying_lp_t
  \begin{equation*}
  \tag{$LP_t$}
  \begin{array}{ll@{}ll}
  \text{maximize} & \langle c(t), x(t) \rangle & \\
  \text{subject to}& A(t)x(t) \le b(t)\\
  \end{array}
  \end{equation*}
  

  One numerical example of a time varying LP to consider is the following (illustrated in figure [[img:example_tv_lp]]):
  \[
A(t) = \begin{pmatrix}
2-t&-1-t\\
1+t&2-t\\
-(2-t)&-(-1-t))\\
-(1+t)&(2-t)\\
(2+t)&(2+t)
\end{pmatrix}
b(t) = \begin{pmatrix}1\\1\\2\\2\\-t\end{pmatrix}
c(t) = \begin{pmatrix}t \\ t^2\end{pmatrix} \]

  The blue circles represent the optimal solution $x^{opt}(t)$ for each time $t$ that maximizes $\langle c(t), x(t)\rangle$ under the constraint $A(t)x(t) \le b(t)$, and the dotted red-line represents the optimal polynomial solution $x^{poly}(t)$ of degree smaller than $11$. the feasible set for any time $t$ is delimited by blue lines, and the objective function $c(t)$ is represented by a black arrow.
  
  The optimal solution $x^{opt}$ lives on the vertices of the feasible set and occasionally jumps from one vertex to a different one, while the optimal polynomial solution moves continuously in the feasible set and tries to be as close as possible to $x^{opt}$.


  It is important to notice that in general the optimal solution to an timevarying LP (and more generally, to a timevarying SDP) is not polynomial, and might not even be continuous.
  
    A natural question here is how to describe the data of the problem: $(A_0, \ldots A_r, b, c)$. We made the choice of considering /polynomial/ functions because they are general enough to approximate a large set of functions, and at the same time simple enough to describe (in the monomial basis for instance) and to solve for (see section [[sec:lp_is_sdp]]). 

  In many applications, one looks for a feasible of function that is smooth and optimal. In this paper we discuss when solutions of this form are optimal, and in that case we use sum of squares optimization to obtain the best feasible solution whose components are polynomial of a bounded degree.

  Since we are looking specifically for polynomial feasible solutions, it is necessary to settle for a weaker notion of optimality than pointwise optimality. We say that continuous (resp. polynomial) solutions are near optimal if there is a sequence of continuous (resp. polynomial) feasible solution $X_n(t)$ that converges to the optimal solution $X^{opt}(t)$ in the $L_2$ sense.
  
  We first provide a necessary and sufficient condition for feasibility continuous solutions, and prove that in that case continuous solutions are optimal as well. We then prove that strict feasibility (see definition [[def:strict_feasibility]]) is enough to guarantee optimality of polynomial solutions. We also discuss the easy case where the left-hand sight of the constraints is independent of time (e.g. for timevarying LPs, the matrix $A(t)$ is always equal to $A(0)$).

  
** Notations
   - If $A$ a matrix of dimension $m \times n$, with $m \ge n$, and $B \subseteq \{1, \ldots n\}$, then $A_B$ is the submatrix $(A_{B_i,i})_{1 \le i \le m}$.
   - $\mathbb R_d[y]$ is the set of polynomial of degree at most $d \in \mathbb N$ in the variable $y$.
   - $SOS_d(y)$ denote the set of polynomials in the variable $y$ that can be written as sum of squares of some polynomials of degree at most $d$, e.g., $SOS_d(y) = \sum \mathbb R_d[y]^2$ . $y$ need not be univariate.
   - $\mathcal P_t$ is the feasible set of ([[eqn:time_varying_lp_t]]) at time $t$: $\mathcal P_t = \{x \in \mathbb R^n | A(t) x \le b(t) \}$.
   
** Assumptions
   Throughout the whole paper, we make the following assumption for all $t \in [-1, 1]$:
   - $\mathcal P_t \ne \emptyset$, i.e. there exist at least one feasible solution. We provide an algorithm [[alg:checking_feasibility]] to check for that.
   - $\mathcal P_t$ is bounded: In theorem [[thm:bound_equiv_uniform_bound]] we prove that this is in fact equivalent to $\mathcal P_t$ being bounded uniformly in $t$.


** Contribution of the paper
   In this paper we propose an efficient method to find the best polynomial solution to a timevarying linear program or semi-definite program, as well as a characterization for when polynomial solution are close to the overall optimal one. The paper is organized as follow:
   - In section 2 we describe the form of solutions to ([[eqn:time_varying_lp_t]]) that need not to be polynomial. We then prove that under strict feasibility conditions ([[thm:strict_feasibility_implies_polynomial_optimality]]) polynomial solutions exist and are optimal.
   - In section 3, we give a finite time algorithm to check for those conditions.
   - Section 4 presents an SDP formulation for finding the best polynomial solution.

   
* Timevarying LP

** Preliminary

   We present here some definitions are properties of linear programs that are going to be useful to us later for proofs (Theorem [[thm:continuity_perturbation]] for instance will help us decide the existence of continuous feasible solution to timevarying LP whose matrix constraint $A(t)$ is constant in time), or to make some simplifying assumptions, like to assume  that the feasible set $\mathcal P_t$ is uniformly bounded for all $t \in [-1, 1]$.
   
*** Limits of sets
    
    The following notion of limits of sets in time will be useful to as:

    #+BEGIN_definition
    $$\lim_{t \rightarrow t_0} P_t = \{v |\; v(t_n) \in \mathcal P_{t_n}, t_n \rightarrow t_0, \lim v(t_n) = v \}$$
    #+END_definition
   
*** Continuity of the optimal value of linear programs

    The following theorem provides some insights on continuity propoerties of linear programs that are subjected to data perturbations
    #+NAME: thm:continuity_perturbation
    #+BEGIN_theorem
    \cite{Martin1975}
    Consider the LP: $\max_{Ax \le b} \langle c, x \rangle$, and let $\Omega$ be the set of tuples $(A, b)$ for which the program $\{x, Ax \le b\}$ is feasible bounded.

   - The optimal value of this program is continuous with respect to perturbations in $c$ and $b$ as long as $(A, b) \in \Omega$
   - The Optimal value of same program is upper semi-continuous with respect to changes in $A, b, c$  as long as $(A, b) \in \Omega$.
   #+END_theorem


*** Boundedness of the feasible set
   We prove that as long as $\mathcal P_t$ is bounded, we can assume without loss of generality that $\mathcal P_t$ is uniformly bounded.

   #+NAME: thm:bound_equiv_uniform_bound
   #+BEGIN_theorem
   Suppose that $\mathcal P_t$ is feasible for all $t \in [-1, 1]$.

   Define the two statements:

   1. $dist(0, P_t) < \infty$ for all $t$.
   2. $\sup_t dist(0, \mathcal P_t) < \infty$

      Then 1. $\implies$ 2
   #+END_theorem

   #+BEGIN_proof
   Consider the program $f(t) = \max_{x \in \mathcal P_t} \sum_i |x_i|$. We have that $f(t)$ is finite for all $t$ and we want to prove that $f(t)$ can be uniformly bounded on $[-1, 1]$.
   
   Notice that this is an LP for all $t$, and that by assumption we made earlier, the set of solution is bounded for all $t$. As a result, all conditions for [[thm:continuity_perturbation]] are verified, and we conclude that  $f(t)$ is upper semi-continuous.
   
   Now, if $t_n$ is a convergent sequence such that $f(t_n) \rightarrow \sup_t f(t)$, and $t_0 = \lim t_n$, then: $\sup_t f(t) = \lim_n f(t_n) \le f(t_0) < \infty$
   #+END_proof

   Without loss of generality, we assume for the rest of this paper that we can amend the constraints $-M \le x_i \le M, i=1\ldots n$.
   This will allow us in particular to assume that $\mathcal P_t$ is in fact a polytope, i.e. it is equal to the convex hull of its vertices.

    
   
** Geometry of the feasible set
   
   We start be presenting the following theorem that describes the geometry of the feasible set $\mathcal P_t$. The theorem states that for except some finite number of times, the feasible set is a convex combination of points that move as piece-wise rational functions in time. More formally:
  
   #+NAME: thm:geometry_feasible_set_lp
   #+BEGIN_theorem
   There exist $N > 0$, and $-1 = t_1 < \ldots < t_N = 1$ such that, for all $i = 1 \ldots N$, there exist $B_1 \ldots B_r \in {[m] \choose n}$ such that:
   - $A_{B_j}(t)$ is invertible for every $t \in (t_i, t_{i+1})$, 
   - $\mathcal P_t = conv\{ A_{B_j}(t)^{-1}b(t), j=1 \ldots r \}$
   - $\lim_{t_i} \mathcal P_t \subseteq \mathcal P_{t_i}$
   - Call $\mathcal V_i = \{t \rightarrow A_{B_j}^{-1} (t) b(t)\}$, i.e. the set of vertices of $\mathcal P_t$ at the interval $(t_i, t_{i+1})$
   #+END_theorem

   #+NAME: proof:geometry_feasible_set_lp
#+BEGIN_proof 
At any given time $t$, $\mathcal P_t$ is a bounded polyhedron, so it is equal to the convex hull of its vertices. All vertices can be written as: $A_B(t)^{-1}b(t)$ for some $B \in [n]$, i.e. there exist a set of basis $\mathcal B(t)$ such that $\mathcal P_t = conv\{A_B(t)^{-1}b(t), B \in \mathcal B(t)\}$.

It remains to show that $\mathcal B(t)$ changes at most finitely many times. That's indeed true because that set changes at time $t_0$ only if one of these two things happen::
- Some nonzero polynomial of this form $t \rightarrow \det(A_B(t))$ equals $0$ at $t_0$
- when one of the components of $t \rightarrow b(t) - A_B(t)^{-1}b(t)$ changes sign at $t_0$.

Both things happen finitely many times.
  
#+END_proof

   Even though the previous theorem gives a description of the feasible set and ignores the objective function, it is not very hard to see that the optimal solution can also be chosen to be a piece-wise rational function in $t$. Indeed, there always exist an optimal solution of a linear program on a vertex, and if $c(t)$ is "nice" enough, e.g. a polynomial, optimality of any given vertex changes only finitely many time inside $[-1, 1]$.
  
   #+NAME: thm:form_optimal_solution_lp
   #+BEGIN_theorem
   There exist $N > 0$, and $0 = t_1 < \ldots < t_N = 1$ such that, for all $i = 1 \ldots N$, there exist $B \in {[m] \choose n}$ such that:
   - $A_{B}(t)$ is invertible for every $t \in (t_i, t_{i+1})$, 
   - $x^{opt}(t) = A_{B_j}(t)^{-1}b(t)$ is optimal.
   #+END_theorem

   We defined $x^{opt}$ everywhere except on the times $t_i$.
   We could extend it at $t_i$ by taking the right limit for example (that exist, since x^{opt} is a bounded rational function on $(t_i, t_{i+1})$). Call that function $\bar x^{opt}$. Even though feasibility will be preserved, optimality may not as the following example shows:

   #+BEGIN_examples
   $\max x(t)$ s.t. $-t \le tx(t) \le t, -2 \le x(t) \le 2$
   \[x^{opt}(t) = \left\{\begin{array}{cc}1&t \ne 0\\0&t = 0\end{array}\right.\]
   #+END_examples

   This is not a problem in our framework however, since we are mainly concerned by the average optimal value in time $\int_{-1}^1 \langle c(t), x(t) \rangle dt$, and changing $x(t)$ at the set of measure 0 will not change that value. In the case where we are interested in maximizing the worst case $\min_{t} \langle c(t), x(t) \rangle$, we can notice that $\langle c(t_i), \bar x^{opt}(t_i)\rangle \ge \min_{t} \langle c(t), x^{opt}(t) \rangle$, and therefore we don't lose by extending $x^{opt}$ in this way neither.

** Existence of continuous solutions
   We are interested in the existence of polynomial solutions, one natural question to ask is whether such solution always exist. The answer to that question is negative, and we prove that in fact even continuous solutions might not exist:

   #+BEGIN_examples
   Example where a continuous solution doesn't exist:
  
   $\mathcal P_t = \{ tx \ge 0, t(x-1) \ge 0\}$ doesn't have a continuous solution. One can see that by observing that
   $\mathcal P_t = [1, \infty)$ when $t > 0$ and   $\mathcal P_t = (-\infty, 0]$ when $t < 0$.
   #+END_examples

   The reason no continuous solution exist is that the $\mathcal P_t$ are ``disconnected" at 0, i.e. $\lim_{t < 0} \mathcal P_t \cap \lim_{t > 0} \mathcal P_t= \emptyset$, for a solution to exist, it has to "jump" at time 0. The following theorem formalizes this notion of continuity of sets and existence of continuous solutions.

   #+NAME: thm:existence_cont_solution
   #+BEGIN_theorem
   The following are equivalent:
   1. There exist a continuous solution.
   2. $dist(P_{t_i-\alpha}, P_{t_i+\alpha}) \rightarrow_{\alpha} 0$ for $i = 1 \ldots N$
   3. $conv \{ v(t_{i}), v \in \mathcal V_i\} \cap conv \{ v(t_{i}), v \in \mathcal V_{i+1}\} \ne \emptyset$
   4. $\min_{x \in \mathcal P_{t_i-\alpha}, x \in \mathcal P_{t_i+\alpha}} |x - y| \rightarrow 0$
   #+END_theorem

   #+BEGIN_proof
   We first start by noticing that 2., 3. and 4. are equivalent because
   \begin{align*}
   dist(P_{t_i-\alpha}, P_{t_i+\alpha}) &= dist(conv \{ v(t_{i}), v \in \mathcal V_i\}, conv \{ v(t_{i}), v \in \mathcal V_{i+1}\})
   \\&= \min_{x \in \mathcal P_{t_i-\alpha}, x \in \mathcal P_{t_i+\alpha}} |x - y|
   \end{align*}

   and the intersection between two compact sets is empty if and only if the distance between them is strictly positive.

   It remains to show that $1 \iff 2$, which we prove in two steps:

   (1 $\implies$ 2)

   Let $x_t$ be a continuous solution, then $dist(P_{t_i-\alpha}, P_{t_i+\alpha}) \le dist(x_{t_i-\alpha}, x_{t_i+\alpha}) \rightarrow 0$

   (2 $\implies$ 1)

   We are going to construct a continuous solution $x_i(t)$ that is defined for $t \in (t_{i-1}, t_i)$.
   Let $x_0 \in conv \{ v(t_{i}), v \in \mathcal V_i\} \cap conv \{ v(t_{i}), v \in \mathcal V_{i+1}\}$, i.e.
   $u = \sum_{v \in V_i}  \lambda_{v} v(t_i) = \sum_{v \in V_{i+1}}  \mu_{v} v(t_i)$, and define:

   \[x_i(t) = \left\{\begin{array}{cc}
   \sum_{v \in V_i} \lambda_v v(t) & t \le t_i\\
   \sum_{v \in V_{i+1}} \mu_v v(t) & t > t_i
   \end{array}\right.
   \]

   It is clear that $x_i$ is feasible and continuous, i.e. $\lim_{t < t_i} x_i(t) = \lim_{t > t_i} x_i(t) = u$.


   We get a continuous feasible solution on $[-1, 1]$ simply by ``connecting" two solution $x_i, x_{i+1}$ by interpolating from one to the other:

   $x_{i, i+1}(t) = \alpha(t) x_i(t) + (1-\alpha(t)) x_{i+1}(t)$, where $\alpha(t) = \frac{t - t_i}{t_{i+1} - t_i}$

   #+END_proof

   A particular special case that is worth mentioning is when $A(t)$ doesn't depend on $t$. In that case, continuous solutions always exist:
  
   #+BEGIN_theorem
   When $A(t)$ doesn't depend on $t$ there exist at least one continuous feasible solution for ([[eqn:time_varying_lp_l2]]).
   #+END_theorem 

   #+BEGIN_proof 
   Assume for the sake of contradiction that no continuous feasible solution exist for ([[eqn:time_varying_lp_l2]]), then there there exist $i \in [m]$ such that $conv \{ v(t_{i}), v \in \mathcal V_i\} \cap conv \{ v(t_{i}), v \in \mathcal V_{i+1}\} = \emptyset$, then there is a separating hyperplane with normal $u \in \mathbb R^n$ such that for some $\delta > 0$:

   - $\langle v(t_i) , u \rangle > \delta$ for $v \in \mathcal V_i$
   - $\langle v(t_i) , u \rangle < -\delta$ for $v \in \mathcal V_{i+1}$


   But that contradicts the fact that the following LP has a continuous solution (because of theorem [[thm:continuity_perturbation]] )when $\alpha \rightarrow 0$:
   $$\min_{x \in P_{t+\alpha}} \langle x, u \rangle$$
  
   #+END_proof

   Whenever there exist one feasible continuous solution, we can find near optimal continuous solution.
  
   #+NAME: thm:optimality_continuous_solution
   #+BEGIN_theorem
   Suppose ([[eqn:time_varying_lp_t]]) admits one feasible continuous solution $f_0$. i.e. there exist a continuous function $f_0: [-1, 1] \rightarrow \mathbb R^n$ such that $A(t)f_0(t) \le b(t)$, $\forall t \in [-1, 1]$
  
   For every $\varepsilon > 0$, there exist a continuous function $f: [-1, 1] \rightarrow \mathbb R^n$ such that:
   - $f(t)$ is feasible of all $t \in [-1, 1]$.
   - $\int_0^1 \langle c(t), x(t)\rangle - \int_0^1 \langle c(t), f(t)\rangle \le \varepsilon$.

     We say that $f$ is /near-optimal/.
   #+END_theorem
   
    #+BEGIN_proof
    Following the result of theorem [[thm:geometry_feasible_set_lp]], there exist a partition a partition $[-1, 1] = \cup_1^n [t_i, t_{i+1}]$ and an optimal solution $x^{opt}(t)$ that is a continuous on every $[t_i, t_{i+1}]$(in fact, a rational function).

    We want to construct a function that is as close as possible to $x^{opt}$ (in the $L_2$ sense) while staying continuous, which would prove the claim of theorem.
    
    For this purpose, define $I_i^{\alpha} = (t_i+\alpha, t_i -\alpha)$ for some $\alpha > 0$.

  Let $f^{\alpha}$ be the function that:
  - is equal to $x(t)$ on every $I_i^{\alpha}$.
  - is equal to $f_0$ on all the $t_i$.
  - interpolates linearly between $x(t)$ and $f_0(t)$ on $[t_i-\alpha, t_i+\alpha]$

    In a sense, $f^{\alpha}$ lives on the optimal vertex but "travels" to the continuous solution $f_0$ to get through the possibly problematic time $t_i$.
    
  As $\alpha \rightarrow 0$, $f^{\alpha}(t) \rightarrow x(t)$ almost surely. Given that $|f^{\alpha}(t)| \le |x(t)| + |f_0(t)|$, the Dominated convergence theorem gives $f^{\alpha}(t) \rightarrow_{L_2} x(t)$, and we conclude by Cauchy-Schwarz.
  #+END_proof
  
      
** From continuous to polynomial
   <<sec:condition_polynomials_optimal>>
   Now that we have established that the existence of continuous solution is a necessary condition that is not always verified, one might ask if such condition is also sufficient for existence and optimality of polynomial solutions. We rely for that on the fact that polynomials can approximate uniformly continuous solutions, and the hope is that if the approximation is good enough, the polynomial solution will be feasible as well.

   Once again, that is not always possible. We consider the following example:
  
   #+BEGIN_examples
   Examples where a continuous solution exists but a polynomial solution doesn't exist:
   $\mathcal P_t = \{ (1+t^2) x = 1\} = \{ \frac1{1+t^2} \}$ 
   #+END_examples

      
   What went wrong? $\mathcal P_t$ in this example is not ``full dimensional", and even though it contains a continuous solution, there is no "slackness" to approximate it with a polynomial. This motivates the following two definitions:

   
   #+NAME: def:strict_feasibility
   #+BEGIN_definition
[[eqn:time_varying_lp_t]] is strictly feasible if there exist a function (not necessarily continuous) :
  
$$[-1, 1] \rightarrow \mathbb R^n, t \rightarrow x^s(t)$$

and $\varepsilon > 0$ such that:

$$A(t)x^s(t) \le b(t) - \varepsilon 1$$

In this case we say that $x^s(t)$ is strictly feasible for ([[eqn:time_varying_lp_l2]]), and we denote by $\mathcal P_t^{\varepsilon}$ the (non-empty) polytope $\{A(t)x(t) \le b(t) - \varepsilon 1\}$.
#+END_definition


#+BEGIN_definition
([[eqn:time_varying_lp_l2]]) is continuously full dimensional if there exist $\delta > 0$ and a /continuous/ function:
  
$$[-1, 1] \rightarrow \mathbb R^n, t \rightarrow x^c(t)$$

Such that:

$$B(x^c(t), \delta) \subset \mathcal P_t$$
#+END_definition

The condition that $\delta$ doesn't depend on $t$ as well as continuity of $x^c(t)$ is important, as the following example shows:
   #+BEGIN_examples
$$A(t) = (1, -1, -t)^T, b(t) = (2, 2, 0)$$
Or equivalently:

$$-2 \le x(t) \le 2, tx(t) \ge 0$$

This program is not continuously full dimensional, but:
   - The feasible continuous solution $x(t) = t$ verifies $B(x(t), \delta_t) \subset \mathcal P_t$, with $\delta_t = \frac t2 + 1_{t = 0} > 0$ for $t \in [-1, 1]$.
   - The feasible (non-continuous) solution $x(t) = 1_{t > 0} - 1_{t < 0}$ verifies $B(x(t), \delta) \in \mathcal P_t$ with $\delta = 1$ for $t \in [-1, 1]$.
   #+END_examples



While the first definition provides slackness in the space of the constraints, the second definition provides slackness in the space of the variable $x^c(t)$, at the expense of $x^c(t)$ being continuous.


Furthermore, the definition we gave for full dimensionality was tailored to guarantee optimality of polynomial solutions:
   
   #+NAME: thm:optimality_poly_solution
   #+BEGIN_theorem
   Under the following assumption:
   - ([[eqn:time_varying_lp_t]]) is continuously full dimensional

   Then for every $\varepsilon > 0$, there exist a polynomial function $p: [-1, 1] \rightarrow \mathbb R^n$ such that:
   - $p(t)$ is feasible of all $t$, i.e. $A(t)p(t) \le b(t)$, $\forall t \in [-1, 1]$
   - $\int_{-1}^1 \langle c(t), x(t)\rangle - \int_{-1}^1 \langle c(t), p(t)\rangle \le \varepsilon$.
   #+END_theorem


   #+BEGIN_proof
   We start with a continuous solution $f$ that is near optimal to ([[eqn:time_varying_lp_t]]), whose existence is guaranteed by [[thm:optimality_continuous_solution]]. Ideally we would like to approximate $g$ uniformly by a polynomial, but $p$ might not be feasible. To remidy this problem, we replace $g$ by a convex combination of $g$ and $x^s$, the strictly feasible solution. Define $f = \lambda g + (1-\lambda) x^s$, and notice that for $\lambda < 1$, $g$ is strictly feasible, but when $\lambda$ is close to 1, $f$ is also near optimal.


   Now let $p(t)$ be a polynomial that approximates $g(t)$ uniformly, i.e., $\forall t \in [-1, 1] \; ||p(t) - f(t)||_2^2  \le \delta^2$, where $\delta$ is a constant we are going to fix latex.

   For $\delta$ smaller than $\varepsilon$, $p(t)$ is inside $\mathcal P_t$.
   
   Let's now examine the objective value of $f$. Similarly:
   $$\int_{-1}^1 \langle c(t), x(t)\rangle - \int_{-1}^1 \langle c(t), p(t)\rangle \le \int_{-1}^1 ||f(t) - p(t)||_2 ||c(t)||_2 dt = O(\delta)$$
   
   Again, taking $\delta$ small enough give the result.
   #+END_proof


The rest of this section compares the power of the two definitions.
   
One can easily see that strict feasibility of a continuous solution $x^c(t)$ implies full dimensionality when the rows of $A(t)$ never cancel.

#+BEGIN_theorem
If $A(t)$ rows never cancel and ([[eqn:time_varying_lp_t]]) is strictly feasible, then ([[eqn:time_varying_lp_t]]) is continuously full dimensional.
#+END_theorem

#+BEGIN_proof
Define
$$\varepsilon = \min_{i} \min_{t \in [-1, 1]} (b(t) - A(t)x^c(t))_i$$
$\varepsilon > 0$, because otherwise, if $\varepsilon = 0$, then by continuity the minimum is attained at some $(t_m, i_m)$ for which $b_{i_m}(t_m) - A_{i_m}(t_m)x^c(t_m) = 0$. By strict feasibility of $x^c(t)$, if $u \in \mathbb R^n$ has norm smaller than  $\delta$, then $b_{i_m}(t) - A_{i_m}(t_m)(x^c(t_m) + u) \ge 0$, which leads to $A_i(t_m)^Tu \ge 0$, and to $A_i(t_m) = 0$.

We have just proved that $(\forall t \in [-1, 1]) \; A(t) x^c(t) \le b(t) - \varepsilon 1$ for some $\varepsilon > 0$..
#+END_proof

Perhaps the more surprising result is that the converse is also true (unconditionnaly):

#+BEGIN_theorem
Strict feasibility of ([[eqn:time_varying_lp_t]]) implies Continuous full dimensionality of ([[eqn:time_varying_lp_t]]).
#+END_theorem

#+BEGIN_proof
Assume strict feasibility of ([[eqn:time_varying_lp_t]]).
We aim to construct a function $x^{c}(t)$ that is continuously full dimensional.

Recall the theorem that says that there exist $N > 0$, and $-1 = t_1 < \ldots < t_N = 1$ such that, for all $i = 1 \ldots N$, there exist $r$ rational functions $\mathcal V_i = \{u_1(t), \ldots, u_r(t)\}$ (the vertices) such that :
   $$\mathcal P_t = conv\{ u_j(t), j=1 \ldots r \}$$


We provide a construction of $x^c(t)$ on two steps depending on whether we are near the problematic points $t_i$, $i = 2 \ldots N-1$ or far away from them, then we connect these patches by interpolating between them. We then extend $x^c(t)$ by continuity on $t_1$ and $t_N$.

*Near the problematic points $t_i$:*

$\mathcal P_{t_i}^{\varepsilon}$ is not empty by strict feasibility, let $w$ be one of its extreme points. Then there exist a basis $B$ such that $w = A_B(t_i)^{-1}(b(t_i) - \varepsilon 1)$

Now define $w^{near}(t) = A(t)^{-1}(b(t) - \varepsilon 1)$, then there exist a neighborhood of $t_i$, $[t_i-\alpha_i, t_i+\alpha_i]$ such that:
- $w^{near}(t)$ is well defined: $\det(A_B(t_i)) \ne 0$ implies, that $\det(A_B(t)) \ne 0$ in the vicinity of $t_i$
- $w^{near}(t)$ is continuous.
- $w^{near}(t)$ is strictly feasible: Since $A(t_i)w^{near}(t_i) \le b(t_i) - \varepsilon 1$, then on a neighborhood of $t_i$: $A(t_i)w^{near}(t_i) \le b(t_i) - \frac{\varepsilon}2 1$


*Far away from the $t_i$:*

On $(t_i, t_{i+1})$, let $w^{right}(t) = \frac{\sum_{u \in \mathcal V_i} u(t)}{|\mathcal V_i|} \in \mathcal P_t$, and similarly, on $(t_{i-1}, t_{i})$, let $w^{left}(t) = \frac{\sum_{u \in \mathcal V_{i-1}} u(t)}{|\mathcal V_i|} \in \mathcal P_t$.

Let's prove that  $w^{right}$ is strictly feasible on $J := [t_{i-1}+\beta, t_i-\beta]$, with $\beta$ equal to (say) $\min_{i=2\ldots N-1} \frac{t_i+t_{i-1}}{10}$.

Define 
$$\varepsilon^{right} = \min_{t \in J, i=1\ldots m} (b(t) - A(t)w^{right}(t))_i$$
$\varepsilon^{right} > 0$, otherwise, by continuity, there exist $i$ and $t \in J$ such that $(b(t) - A(t)w^{right}(t))_i = 0$, which means that 
$0 = b_i(t) - A_i^T(t)w^{right}(t) = \frac1{|\mathcal V_i|} \sum_{u \in \mathcal V_i} \underbrace{(b_i(t) - A_i(t)^Tu(t))}_{\ge 0}$, i.e. all $\mathcal P_t$ 's vertices belong to same affine hyper plane $A_i(t)^T x = b_i(t)$, which contradicts the existence of a strictly feasible point $x^s(t)$.

Similarly, we define $\varepsilon^{left} > 0$.


*Connecting the patches:*

We get a continuous feasible solution on $[-1, 1]$ simply by "connecting" the solutions $w^{left}, w^{right}, w^{near}$ by interpolating from one to the other. To ease notation, define the function $I_a^b(t)$ to be the linear function equal to $0$ at $t = a$, and to $1$ at $t = b$.

Define:



   \[x^c(t) = \left\{\begin{array}{cc}
   w^{left}(t) & t_{i-1}+\beta \le t \le t_{i} - \beta\\
   I_{t_{i} - \beta}^{t_{i} - \beta/2}(t) (w^{left}(t) - w^{near}(t)) + w^{near}(t) & t_{i}-\beta < t \le t_{i} - \beta/2\\
   w^{near}(t) & t_{i}-\beta/2 < t \le t_{i} + \beta/2\\
   I_{t_{i} + \beta/2}^{t_{i+1} - \beta}(t) (w^{near}(t) - w^{right}(t)) + w^{right}(t)   & t_{i}-\beta < t \le t_{i} - \beta/2\\
   w^{right}(t) & t_{i}+\beta < t \le t_{i+1} - \beta\\
   \end{array}\right.
   \]


   It is easy to see that:
   - $x^c(t)$ is continuous.
   - at all times $t$, $x^c(t)$ is a convex combination of solutions that are strictly feasible, so at any given point in time $t$, $A(t) x^c(t) \le b(t) - \underbrace{\min(\varepsilon^{right}, \varepsilon^{left}, \varepsilon)}_{\varepsilon'} 1$

     Now, if $y \in B(x^c(t), \delta)$ for some $\delta > 0$, then:
     \begin{align*}
     A(t)y &= A(t)x(t) + A(t) (y - x(t))
     \\&\le b(t) - \varepsilon' 1 + \delta \max_{t \in [-1, 1]} ||A(t)||_2 1
     \\&\le b(t) & \text{(whenever $\delta \max_{t \in [-1, 1]} ||A(t)||_2 \le \varepsilon'$)}
     \end{align*}

     Which proves continuous full dimensionality with $\delta =  \frac{\varepsilon'}{1 + \max_{t \in [-1, 1]} ||A(t)||_2} > 0$
#+END_proof

      
   
We are now ready to present the main characterization for the existence and optimality of polynomial solutions. Indeed, we have reduced that question to the feasibility of the following time varying LP:

$$A(t)x^s(t) \le b(t) - \varepsilon 1$$


#+NAME: thm:strict_feasibility_implies_polynomial_optimality
   #+BEGIN_theorem
   Under the strict feasibility assumption, i.e. that the following timevarying LP is feasible:
   
   $$A(t)x(t) \le b(t) - \varepsilon 1, t \in [-1, 1]$$
   
   For every $\varepsilon > 0$, there exist a /polynomial/ function $p: [-1, 1] \rightarrow \mathbb R^n$ such that:
   - $p(t)$ is feasible of all $t$, i.e. $A(t)p(t) \le b(t)$, $\forall t \in [-1, 1]$
   - $\int_{-1}^1 \langle c(t), x(t)\rangle - \int_{-1}^1 \langle c(t), p(t)\rangle \le \varepsilon$.
   #+END_theorem


   
   
* Decidability of the sufficient conditions for existence and optimality of polynomial solutions to LPs

  This section present finite time algorithms to decide the conditions discussed above that guarantee feasibility of time varying LP as well as optimality of polynomial solutions.
  
  Theorem [[thm:form_optimal_solution_lp]] showed that the feasible set of a time varying LP can be fully described by giving the times $t_1, \ldots t_N$ as well as the coefficients of the rational functions in the set $\mathcal V_i$ for all $i=1, \ldots N$. We propose an algorithm that does exactly that.
  
  Notice that since the algorithm produces a vertex description of the moving polytope $\mathcal P_t$, getting an optimal solution for all $t \in [-1, 1]$ is straightforward.
  
  
** Feasibility and strict feasibility

   We present an algorithm that decides whether a time varying LP is feasible, and if yes, then for all times $t_1, \ldots, t_N$  described by Theorem [[thm:form_optimal_solution_lp]], produces the set of basis $B_1, \ldots, B_r$.
   

   The following lemma is going to be very useful to us later on.
   #+BEGIN_lemma
   The roots of a univariate polynomial are computable.
   #+END_lemma

   Based on theorem [[thm:form_optimal_solution_lp]], one can solve the problem ([[eqn:time_varying_lp_t]]) directly using the following algorithm:
   
   For all $B \in {[m]\choose n}$, consider the matrix polynomial in $t$: $A_B(t)$.
    
   Define $\det_B(t) = \det(A_B(t))$, if it is not identically 0, then it has finitely many zeros that we denote by $\mathcal U_B$, and for $t$ outside that set, definite $u_B(t) = A_B(t)^{-1}b(t)$.

   Let $\mathcal U$ be the set of such times, i.e. $\mathcal U = \cup_{B \in {[m]\choose n}} \mathcal U_B$.
    
   All such $u_B(t)$ change feasibility status (i.e become feasible or infeasible) finitely many times, because that correspond to a zero of one the polynomial components of $b(t) - A(t)u_B(t)$. Add all such times to the set $\mathcal U$.

   It is clear that between two consecutive times in $\mathcal U$, the basis of the vertices of the feasible set do not change. Thus we can take $\{t_1, \ldots t_N\}$ to be $\mathcal U$.
    
#+NAME: alg:checking_feasibility
 #+begin_algorithm
\caption{Check feasibility}
\begin{algorithmic}[1]
\State \text{Compute} $\mathcal U$ \text{like described above (amounts to finding the roots of polynomials)}
    
\For{ $i=0 \ldots \operatorname{len}(\mathcal U)$}
\State $t \gets \frac{\mathcal U[i] + \mathcal U[i+1]}2$
\State \text{Outputs all} $B \in {m \choose [n]}$ \text{such that} $\det(A_B(t)) \ne 0, A(t)A_B(t)^{-1}b(t) \le b(t)$
\State \text{If no such} $B$ \text{exists, the problem is infeasible}
\EndFor
\end{algorithmic}
#+end_algorithm
   
** Solving a time-varying LP exactly
   
   Finding the optimal solution can be implemented in the same fashion, and the following algorithm is an adaptation of algorithm [[alg:checking_feasibility]].

   #+NAME: alg:solving-time-varying-lp-exactly
   #+begin_algorithm
   \caption{Find optimal solution}
   \begin{algorithmic}[1]
   \Procedure{Solve Pt}{}
   \State $B[]$ array
   \State $t[]$ array
   \State $t[1] \gets 0$
   \State $i \gets 0$
   \Do
   \State \text{Solve} $P(t[i])$, $B[i] \gets \textit{The optimal basis}$
   \State $i \gets i+1$
   \State $t[i] \gets \arg \max_{s \ge t}\{\det A_B(s) \ne 0, \; A(s)A_B^{-1}(s)b(s) \le b(s), \;  c_B(s)A_B^{-1}(s) \le 0 \; \}$
   \doWhile{$t[i] \le 1$}
   \EndProcedure
   \end{algorithmic}
   #+end_algorithm

   Algorithm [[alg:solving-time-varying-lp-exactly]] outputs the time $t_1, \ldots t_N$ at which the jumps occur described by [[thm:form_optimal_solution_lp]], as well as the optimal basis at any one of the those times.

   #+comment: We conjecture that the number of jumps $N$ is not polynomial in the size of the input polynomials $(A, b, c)$.

   #+begin_theorem
   Algorithm [[alg:solving-time-varying-lp-exactly]] terminates after finitely many steps and gives the correct optimal solution to [[eqn:time_varying_lp_t]].
   #+end_theorem
    
   #+begin_proof
   The number of steps of the loop is bounded by the number of roots of the following polynomials:
   $\{\det A_B(s) \ne 0, \; A(s)A_B^{-1}(s)b(s) \le b(s), \;  c_B(s)A_B^{-1}(s) \le 0 \; | B \in [n]\}$

   Correctness is obtained because at any given time $t$, the point $x(s) = A(s)A_B^{-1}(s)$ is:
   - feasible, i.e. $A(s)A_B^{-1}(s)b(s) \le b(s)$
   - optimal, because of dual feasibility, i.e. $c_B(s)A(s)A_B^{-1}(s)b(s) \le 0$
   #+end_proof



** Deciding strict feasibility of a time-varying LP
   We seek to decide whether the following LP is feasible or not for some $\varepsilon > 0$:
   $$A(t)x(t) \le b(t) - \varepsilon 1$$

   Which can be reformulated as:
   \begin{equation*}   
   \begin{array}{ll@{}ll}
   \text{max} & \varepsilon & \\
   \text{s.t}& A(t)x(t) \le b(t) - \varepsilon 1
   \end{array}
   \end{equation*}

   The previous section explains how to solve the problem above.

   
** Deciding feasibility of continuous solutions to a time-varying LP
   
   Using characterization [[thm:existence_cont_solution]], we can decide whether there exist a continuous solution that lives inside $\mathcal P_t$ for all $t \in [-1, 1]$. To do that, we look at times $t_{2}, \ldots t_{N-1}$ given by algorithm [[alg:solving-time-varying-lp-exactly]], as well as the set of vertices $\mathcal V_1, \ldots, \mathcal V_N$ provided by the same algorithm, and for $2 \le i \le N-1$, we check that the following polytope is not empty:
   $$\operatorname{conv}(v(t_i), v \in \mathcal V_i) \cap \operatorname{conv}(v(t_i), v \in \mathcal V_{i+1})$$

   And this can be done in efficiently using standard linear programming algorithms.
   

** COMMENT Full dimensionality
   Full dimensionality can also be checked in the same fashion, we look at times $t_{1}, \ldots t_{N-1}$ given by the previous algorithm, and for $1 \le i \le N-1$, we check that the polytope $\operatorname{conv}(v(t), v \in \mathcal V_i)$ is full dimensional for all $t \in [t_i, t_{i+1}]$.

   [Deal with endpoints]
   
   To do that, it is enough to check that  for all $t \in [t_i, t_{i+1}]$, there exist a subset of $\{v_1, \ldots, v_n\} \subseteq \mathcal V_i$, such that $v_1(t) \wedge \ldots \wedge v_n(t) \ne 0$.

   Equivalently, this verified if and only if at least one the following polynomials is not 0 for all times $t \in [t_i, t_{i+1}]$:  $$\{ t \rightarrow v_1(t) \wedge \ldots \wedge v_n(t), \{v_1, \ldots, v_n\} \subseteq \mathcal V_i\}$$. One can do that simply by checking that those polynomials do not have common roots.

* Time-varying LP is an SDP
    <<sec:lp_is_sdp>>

  Algorithm [[alg:solving-time-varying-lp-exactly]] of the previous section proves that one can solve exactly a time-varying LP, and get the optimal solution in finite time, even though the solution is not continuous. The algorithm takes at least exponential time[fn::the time complexity of algorithms described in this paper is always with respect to the size of the input $(A, b, c)$ for timevarying LPs and $((A_i)_{i=1}^m, (b_i)_{i=1}^m, C)$ for timevaryign SDPs] as it checks all the vertices of the polytope.
  
  This section describes how one can find the best /polynomial/ solution of a given degree, and in fact describes an algorithm that is efficient (in fact polynomial). Indeed, we prove that we can turn a time-varying LP into an semi-definite. The idea behind such a reduction is that a univariate polynomial $p(t)$ is non-negative on some interval, say $[-1, 1]$ if and only if it can be written as a sum of square of two polynomials $q(t), s(t)$, potentially weightted by $(1-t)$ and/or $(1+t)$, and searching for $q(t)$ and $s(t)$ can be done efficiently.
  
  
  #+begin_theorem
  A polynomial $p$ of degree $n$ is nonnegative over $[-1,1]$ if and only if it can be written as a weighted sum of squared polynomials, either in the form of
  \begin{equation}
  p(t)=(1+t)q(t)+(1-t)r(t), \quad q\in SOS_{k-1}(t),\; s\in SOS_{k-1}(t) \qquad \text{if }n=2k-1,\label{eq:wsos-odd}
  \end{equation}
  or in the form
  \begin{equation}
  p(t)=(1+t)(1-t)q(t)+s(t), \quad q\in SOS_{k-1}(t),\; s\in SOS_k(t), \qquad \text{if }n=2k.\phantom{-1 }\label{eq:wsos-even}
  \end{equation}
  #+end_theorem

  As a result of this theorem, we can now rewrite ([[eqn:time_varying_lp_t]]) as (non time-varying) SDP:
  
  #+begin_theorem
  The following SDP find the best polynomial solution of degree $\le 2d+1$:

  #+NAME: eqn:Ppoly
  \begin{equation*}
  \begin{array}{ll@{}ll}
  \text{maximize} & \int_{-1}^1 \langle c(t), x(t) \rangle dt & \\
  \text{subject to}& b(t) - A(t) x(t) = (1-t) \sigma_-(t) + (1+t) \sigma_+(t)
  \end{array}
  \end{equation*}

  $\sigma_-, \sigma_+ \in \text{SOS}_d(t)$
  #+end_theorem

  To see that this indeed an SDP, one can note that the equality between two polynomials of degree at most $d$ can be written as the equality of the value they take at $d+1$ different times (which is linear in their coefficients), and the condition that a polynomial $\sigma(t)$ is sum of square can be expressed as a PSD condition using the following proposition:

  #+BEGIN_theorem
  [\cite{Parrilo2004}]
  
  Let $t_0< \ldots < t_{2k} \in \mathbb R$,  $p_0, \ldots, p_k$ a basis for $\mathbb R_k[t]$, and $A_{ij}^{(l)} = p_i(t_l)p_j(t_l)$
  
   $q \in SOS_k$ if and only if there exist $X \succeq 0$ such that
$$q(t_l) = \langle X, A^{(l)} \rangle, \forall l \le 2k$$

#+END_theorem

  Choosing the times $(t_i)_0^{2k}$ to be the Chebyhev points of the first kind and the basis $(p_j(t))_0^k$ to be the scaled Chebyshev polynomials makes the columns of the matrix $A^{(l)}$ orthonormal, which allows for better numerical stability. See section [[sec:numeric]] for an example.

  
* Time-varying SDPs 
  
  We seek a characterization for optimality of polynomial solutions to a semi definite program similar to one we found for linear programs. It turns out again that strict feasibility is enough for that. The definition is as follow:

     #+BEGIN_definition
([[eqn:time_varying_sdp_l2]]) is strictly feasible if there exist a function (not necessarily continuous):
  
$$[-1, 1] \rightarrow \mathbb R^{n \times n}, t \rightarrow X^s(t)$$

Such that:
- $X^{s}(t) \succeq \varepsilon I$
- $A_i(t)X^s(t) \le b_i(t) - \varepsilon 1$ for $i = 1, \ldots, m$

In this case we say that $X^s(t)$ is ($\varepsilon$)-strictly feasible for [[eqn:time_varying_sdp_l2]].
#+END_definition

The proof technique relies on the fact that spectrahedrons, the feasible sets of semi-definite programs, can be approximated within arbitrary accuracy by polyhedrons, and we generalize this result to the time varying-case when the strict feasibility condition is verified.
     
  We also provide an efficient algorithm to find the best polynomial solution relying once again on sum of squares techniques.

** Approximating spectrahedrons by polyhedrons

   
   Let $N(\varepsilon)$ be an $\varepsilon$ -covering of the compact set $\{X \succeq 0, ||X|| = 1\}$. Then for any $X \succeq 0$, we can find an element $Y$ of the finite set $N(\varepsilon)$ such that $||X - Y|| \le \varepsilon ||X||$. The idea now is to inner approximate the feasible set of ([[eqn:time_varying_sdp_l2]]):
   $$S^+(t) = \{ X(t) \;| \; X(t) \succeq  0, \; \langle A_i(t), X(t) \rangle \le b_i(t), \; i=1\ldots m\}$$
by the polyhedron
$$P(t) = \{ \alpha(t) \in (\mathbb R^+)^n \; | \; X(t) = \sum_{Y \in N(\varepsilon)} \alpha_Y Y, \; \langle A_i(t), X(t) \rangle \le b_i(t), \; i = 1\ldots m\}$$
Where we replaced the psd condition $X \succeq 0$ by the stronger condition of $X$ being a sum of elements of the $\varepsilon$ -covering with positive coefficients.

#+NAME: thm:strict_feasibility_implies_polynomial_optimality_sdp
  #+begin_theorem
  If [[eqn:time_varying_sdp_l2]] is strictly feasible, i.e. there exist a function $x(t)$ and $\delta > 0$ such that $X(t) \succeq \delta I$ and $\langle X(t), A_i(t) \rangle  \ge b_i(t) - \delta$ for $t \in [-1, 1]$, then:
  For every $\varepsilon > 0$, there exist a /polynomial/ function $p: [-1, 1] \rightarrow \mathbb R^{n \times n}$ such that:
   - $p(t)$ is feasible of all $t$.
   - $\int_{-1}^1 \langle c(t), x(t)\rangle - \int_{-1}^1 \langle c(t), p(t)\rangle \le \varepsilon$.
  #+end_theorem


To prove the theorem, let's assume ([[eqn:time_varying_sdp_l2]]) is strictly feasible, and consider the following time varying LP:

  #+NAME: eqn:approx_lp_eps
  \begin{equation*}
  \tag{$APPROX-LP_{\varepsilon}$}
  \begin{array}{ll@{}ll}
  \text{maximize}_{Z, \alpha} & \int_{-1}^1 \langle Z(t), C(t) \rangle dt & \\
  \text{subject to}& Z(t) =  \sum_{Y \in N(\varepsilon)} \alpha_Y(t) Y \\
  & \langle A_i(t), Z(t) \rangle \le b_i(t) 
  \end{array}
  \end{equation*}

  We claim that the proof follow from this two lemmas:

  #+NAME: lem:approx_lp_converge_tv_sdp
  #+begin_lemma
  As $\varepsilon \rightarrow 0$, the optimal value of ([[eqn:approx_lp_eps]]) converges to the optimal value of ([[eqn:time_varying_sdp_l2]]). 
  #+end_lemma
  
  #+NAME: lem:optimality_polynomial_approx_lp
  #+begin_lemma
  Polynomial solutions are near optimal for ([[eqn:approx_lp_eps]]) 
  #+end_lemma

  Before we present the proofs of this two lemmas, let us argue why they imply theorem [[thm:strict_feasibility_implies_polynomial_optimality_sdp]]. Denote by $\phi_{sdp}$ the optimal value for ([[eqn:time_varying_sdp_l2]]), and $\phi_{\varepsilon}$ the optimal value of ([[eqn:approx_lp_eps]]), and let $\alpha > 0$.

  For $\varepsilon$ small enough, the first lemma above gives that $|\phi_{\varepsilon} - \phi_{sdp}| \le \frac{\alpha}2$. The second lemma proves the existence of a polynomial feasible solution $Z(t)$ for which $|\phi_{\varepsilon} - \int_{-1}^1 \langle Z(t), C(t) \rangle dt| \le \frac \alpha 2$.

  Now, it is not hard to see that $Z(t)$ is also feasible for ([[eqn:time_varying_sdp_l2]]), and furthermore, by triangular inequality, $|\phi_{sdp} - \int_{-1}^1 \langle Z(t), C(t) \rangle dt| \le \alpha$. Which concludes the proof of the theorem.

  We still need to prove the two lemmas. For lemma [[lem:optimality_polynomial_approx_lp]] to hold, it is enough for us to construct a strictly feasible solution to ([[eqn:approx_lp_eps]]), and then use theorem [[thm:strict_feasibility_implies_polynomial_optimality]] to conclude. To that effect, we start with a strictly feasible solution to ([[eqn:time_varying_sdp_l2]]) $X^s(t)$. For $t \in [-1, 1]$, Let $\alpha_Y(t) = ||X^{s}(t)||$ if $Y \in N(\varepsilon)$ is the closest point to $\frac{X^s(t)}{||X^{s}(t)||}$ in the epsilon cover $N(\varepsilon)$, and $\alpha_Y(t) =\frac{\varepsilon}{|N(\varepsilon)|}$ otherwise.

  $Z(t)= \sum_{Y \in N(\varepsilon)} \alpha_Y(t) Y$  is guaranteed to be within $2M \varepsilon$  of $X^{s}(t)$ by property of the $\varepsilon$ covering and triangular inequality.[fn::$M$ is the uniform bound on the norm of feasible solutions to [[eqn:time_varying_sdp_l2]]] Let's now check that $Z(t)$ is indeed a strict feasible solution to ([[eqn:approx_lp_eps]]):
  - $\alpha(t) \ge \frac{\varepsilon}{|N(\varepsilon)|} 1$  
  - Since $||Z(t) - X_s(t)|| \le 2 \varepsilon M$ and $\langle A_i(t), X_s(t) \rangle \le b_i(t) - \delta 1$ for all $t \in [-1, 1]$, then by taking $\varepsilon = \frac{\delta}{2M}$, we have that $\langle A_i(t), X_s(t) \rangle \le b_i(t) -\delta 1$.

  
We now prove [[lem:approx_lp_converge_tv_sdp]]. We start with an optimal solution to  $X^*(t)$ be an optimal solution of  ([[eqn:time_varying_sdp_l2]]), and we approximate it by $Z(t)$ feasible for ([[eqn:approx_lp_eps]]) using the exact same construction as the previous paragraph so that $||Z(t) - X^*(t)||$ is uniformly bounded in $t$ by quantity going to 0 of $\varepsilon$ goes to 0, thus the same applies the difference of the objective function of $Z(t)$ and $X^*(t)$ by Cauchy-Schwarz.


** Reformulation of time varying SDPs
   <<sec:sdpt_is_sdp>>
   
  Like we did for LPs, the following theorem restate the time-varying SDP [[eqn:time_varying_sdp_l2]] in terms of non-varying SDP:
  
  #+begin_theorem
  (See Theorem 5.1 in \cite{DetteStudden})
  
  For $X(t)$ polynomial, the following two statements are equivalent:
  - $X(t)  \succeq 0 , \; t \in [-1, 1]$
  - $u^TX(t)u \in (1+t) SOS(t, u) + (1-t) SOS(t, u)$
  #+end_theorem
  

   #+BEGIN_theorem
  The following SDP find the best polynomial solution of degree $\le 2d+1$:

  \begin{equation*}
  \begin{array}{ll@{}ll}
  \text{maximize} & \langle X(t), C(t) \rangle & \\
  \text{subject to}& u^TX(t)u = SOS(t, u)\\
  & \langle A_i(t), X(t) \rangle \le b_i(t) 
  \end{array}
  \end{equation*}

  $\sigma_-, \sigma_+ \in \text{SOS}_d$

   #+END_theorem

  
  
* Numerical results
   <<sec:numeric>>
   
  We present two numerical examples to illustrate the techniques presented in this paper. The first one is time-varying max-flow problem, where the graph is fixed but the capacities are varying with time, and we seek a the best polynomial flow.
  
** Max flow
  #+ATTR_LATEX: :width 0.5\textwidth
  #+ATTR_LATEX: :float wrap :width 0.38\textwidth :placement {r}{0.4\textwidth}
  #+caption: Maxflow instance
  file:graph.png

   
  We identify the nodes with $\{1, \ldots n\}$, where 1 is the source, and $n$ is the target. $b_{i,j}(t) \in \mathbb R$ is the capacity of the edge $i \rightarrow j$ at time $t$ for $i, j \le n$ and  $f_{i,j}(t)$ is the flow on the same node. We can thus formulate the problem of finding the best flow in time as:

  #+NAME: eqn:maxflow
  \begin{equation*}
  \tag{MAXFLOW}
  \begin{array}{ll@{}ll}
  \text{maximize} & \int_{-1}^1  \sum_{j=1}^n f_{1,j}(t) dt & \\
  \text{subject to}& \sum_j f_{i, j}(t) - f_{j, i}(t) = 0\\
  & 0 \le f_{i,j}(t) \le b_{ij}(t) \\
  \end{array}
  \end{equation*}
     

  Using the results from section [[sec:lp_is_sdp]], we parametrize the polynomial $f_{ij}$ and $b_{ij}$ by the value they take at the times $(t_l)_0^{d}$.


   $f_{i,j}(t) \approx \begin{pmatrix}f_{i,j}(t_0)\\\vdots\\f_{i,j}(t_d)\end{pmatrix} := \begin{pmatrix}f_{i,j,0}\\\vdots\\f_{i,j,d}\end{pmatrix}$

The quantity $\int_{-1}^1  \sum_{j=1}^n f_{1,j}(t) dt$ is also linear in the $f_{1,j,l}$. Indeed, one can express it as $\sum_{l=0}^d \sum_j  f_{1,j,l} w_l$ where the $w_l$ can be found by solving a simple linear system.


   \begin{equation*}
   \begin{aligned}
   & \text{maximize}
   & & \sum_j \sum_{l=0}^d f_{1,j,l} w_l \\
   & \text{subject to}\\
   &&& \sum_{j=1}^N f_{i,j,l} - f_{j,i,l} &=& 0                                                          & \forall l, \forall i \ne s, t &: (c_{i,l})\\
   &&& f_{i,j,l}                          &=& \langle A^{(l)}, (1-t_l) X_{ij} + (1+t_l) X'_{ij} \rangle  &\forall i,j,l &: (x_{ijl})\\
   &&& b_{i,j,l} - f_{i,j,l}              &=& \langle A^{(l)}, (1-t_l) Z_{ij} + (1+t_l) Z'_{ij} \rangle\ &\forall i,j,l &: (z_{ijl})\\
   &&& X_{ij}, X'_{ij}, Z_{ij}, Z'_{ij} \succeq 0\\
   \end{aligned}
   \end{equation*}




** Time varying certificate of stability

   We want to certify that a the following system is stable:
   
   $$\dot x = A(t) x$$

   Where $A(t)$ is varying with time.

   We can prove that the system is stable if and only if the matrix $A(t)$ is Herwitz, and we can check for the later by solving the following SDP:
   
   $$\forall t \; \exists P \succeq I, P^TA(t) + A(t)^T P \succeq 0$$

   $P$ is called a certificate of stability.

   Following the framework presented in this paper, we can look efficiently for a certificate $P(t)$ that depend polynomially on $t$.

   $$P(t) \succeq I, P(t)^TA(t) + A(t)^T P(t) \succeq 0$$

   
   
* Conclusion and open questions   

This paper presented a natural method to optimize over polynomial solutions to timevarying convex program using the sum of squares framework. We note that even though there exist polynomial algorithms for sum of squares optimization, the best known algorithms scale very poorly as the number of variables the polynomials depend on grow. One notable exception is certifying non-negativity of univariate polynomials, which can be done efficiently using an appropriate basis. We exploit this fact in the case of timevarying linear programs.

The paper also provided sufficient conditions under which polynomial solutions are optimal. It is worth mentioning that the main characterization given here might be asking for too much in certain cases, since it doesn't cover the case of /equality constraints/.




\bibliographystyle{plain}
\bibliography{citations}



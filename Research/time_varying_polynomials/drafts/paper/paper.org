* Introduction

  In this paper we investigate time-varying convex programs, i.e. optimization problems for which the feasible set and the objective function depend  on time over a compact interval.
  

  #+BEGIN_COMMENT
  ---or the decision problem that airline companies face when assigning crew to flights throughout the day while making sure each flight is covered maximizing comfort for the crew members---.
  #+END_COMMENT
  
  More specifically, a time-varying linear program (abbreviated [[eqn:time_varying_lp_l2]]) is defined as follow:
  
  #+NAME: eqn:time_varying_lp_l2
  \begin{equation*}
  \tag{TV-LP}
  \begin{array}{ll@{}ll}
  \underset{x(t)}{\text{maximize}} & \int_{-1}^1 \langle c(t), x(t) \rangle dt & \\
  \text{subject to}& A(t) x(t) \le b(t) & \forall t \in [-1, 1],
  \end{array}
  \end{equation*}
  
\noindent  with $A(t) \in \mathbb R^{m \times n}, x(t), c(t) \in \mathbb R^n$ and $b(t) \in \mathbb R^m$ for all $t \in [-1, 1]$, and moreover, the components of $A(t)$, $b(t)$, $c(t)$ are polynomial functions time $t$.

  We consider the data of the problem $(A(t), b(t), c(t))$ to belong to the set of /polynomial/ functions because on the one hand, this set is dense in the set of continuous functions defined on $[-1, 1]$, and on the other hand, polynomials can be parameterized in a natural way (in the monomial basis for instance) and are more suitable to efficient algorithmic operations (e.g. Section [[sec:lp_is_sdp]]). 

  The optimization variable is a function $x: [-1, 1]\rightarrow \mathbb R^n$. When such function verifies the program constraints for all $t \in [-1,1]$, we call it a /feasible/ solution.

  In this paper, we restrict our search to the set of feasible solutions to TV-LPs, or more generally to TV-SDPs that are going to be introduced later, that are themselves polynomial functions of time. Our motivation for doing so is twofold.
  The first motivation comes from the need of a smooth solution to problems that arise in practice. Concrete applications of this kind of problems include scheduling an electric power generation when users daily consumption is known in advance or finding the optimal control of a robotic arm. In the first case, insuring the smoothness of the power generating process is important to avoid the deterioration of the system. In the second example, large fluctuations of the movement of the arm are not desirable.
  Our second motivation is algorithmic. Indeed, the optimization over polynomial functions of time of a given degree satisfying linear inequalities, whose coefficients are themselves function of time, can be cast exactly as a semidefinite program of small size. We rely on sum of squares techniques to convert non negativity constraints of a polynomial $x(t)$ to sum of squares constraints (See Section [[sec:lp_is_sdp]] for more details).

  
    As a numerical example of a TV-LP, consider the problem illustrated in Figure [[img:example_tv_lp]], and given by data
  \[
A(t) = \begin{pmatrix}
2-t&-1-t\\
1+t&2-t\\
-2+t&1+t\\
-1-t&2-t\\
2+t&2+t
\end{pmatrix},
b(t) = \begin{pmatrix}1\\1\\2\\2\\-t\end{pmatrix}, \text{and }
c(t) = \begin{pmatrix}t \\ t^2\end{pmatrix}. \]

  #+NAME: img:example_tv_lp
  #+ATTR_LATEX:  :width 0.75\textwidth
  #+caption:An example of a TV-LP
  [[file:includes/tvlp_non_annoted.png]]

  
  The blue circles represent the optimal solution $x^{opt}(t)$ that maximizes the linear objective $\langle c(t), x(t)\rangle$ under the constraint $A(t)x(t) \le b(t)$ at time $t \in [-1, 1]$. The dotted red line represents the optimal polynomial solution $x^{poly}(t)$ of degree $11$. The feasible set $\{x \in \mathbb R^n\; |\;  A(t)x \le b(t)\}$ for any time $t$ is delimited by blue lines. As time progresses, the number of facets of this set changes. The objective function $c(t)$ is represented by a black arrow. 

  Notice that in general the optimal solution to a TV-LP is any function $x^{opt}: [-1, 1] \longrightarrow \mathbb R^n$ for which $x^{opt}(t)$ is a solution to the following linear program almost everywhere in $[-1, 1]$:

  #+NAME: eqn:time_varying_lp_t
  \begin{equation*}
  \tag{$LP_t$}
  \begin{array}{ll@{}ll}
  \underset{x \in \mathbb R^n}{\text{maximize}} & \langle c(t), x \rangle & \\
  \text{subject to}& A(t)x \le b(t).\\
  \end{array}
  \end{equation*}
  

  It is important to notice that in general an optimal solution to a TV-LP $x^{opt}(t)$ is not polynomial, and might not even be continuous. In the example of Figure [[img:example_tv_lp]] for instance, the optimal solution $x^{opt}(t)$ lives on the vertices of the feasible set and occasionally jumps from one vertex to a different one, or in other terms, there are times when the set of indices of constraints that are tight for $x^{opt}(t)$ changes, while the optimal polynomial solution moves continuously in the feasible set and tries to be as close as possible to $x^{opt}(t)$.
  
  
  In this paper, we are looking specifically for polynomial feasible solutions, it is necessary to settle for a weaker notion of optimality than point-wise optimality. We say that a continuous solution $f$ for a TV-LP is $\varepsilon\text{-near}$ optimal if $\int_{-1}^1 \langle f(t), c(t)\rangle dt - \int_{-1}^1 \langle x^{opt}(t), c(t)\rangle dt \le \varepsilon$. If for all positive $\varepsilon$, there exists a continuous (resp. polynomial) feasible solution that is $\varepsilon\text{-near}$ optimal for the TV-LP, we say that continuous (resp. polynomial) solutions are near optimal for the TV-LP.

  
  A time-varying semidefinite program (abbreviated [[eqn:time_varying_sdp_l2]]) is the following optimization problem:
  
  #+NAME: eqn:time_varying_sdp_l2
  \begin{equation*}
  \tag{TV-SDP}
  \begin{array}{ll@{}ll}
  \underset{X(t)}{\text{maximize}} & \int_{-1}^1 \langle X(t), C(t) \rangle dt & \\
  \text{subject to}& X(t) \succeq 0 &\; \forall t \in [-1, 1]\\
  & \langle A_i(t), X(t) \rangle \le b_i(t) &\; \forall t \in [-1, 1], \; \forall i \in \{1, \ldots, m\},
  \end{array}
  \end{equation*}

\noindent where $A_i(t) , X(t), C(t)$ are symmetric $n \times n$ matrices, and $b_i(t)$ is a scalar for all $t \in [-1, 1], i=1, \ldots, m$. The components of the matrices $A_i(t), b_i(t)$ for $i=1,\ldots,m$ and $C(t)$ are polynomial functions of time.

  Sum of square techniques can be extended to cover the constraint $X(t) \succeq 0$ as well, where $X(t)$ is a matrix polynomial (see \cite{DetteStudden}). Section [[sec:sdpt_is_sdp]] explains how one can optimize over polynomial solution of a given degree by solving a non varying semidefinite program of small size.
  
  Like for TV-LPs, an optimal solution to a TV-SDP is any function $X^{opt}: [-1, 1] \longrightarrow \mathbb R^{n \times n}$ for which $X^{opt}(t)$ is a solution to the following semidefinite program almost everywhere in $[-1, 1]$:

  #+NAME: eqn:time_varying_sdp_t
  \begin{equation*}
  \tag{$SDP_t$}
  \begin{array}{ll@{}ll}
  \underset{X(t)}{\text{maximize}} & \langle X(t), C(t) \rangle & \\
  \text{subject to}& X(t) \succeq 0&\; \forall t \in [-1, 1]\\\\
  & \langle A_i(t), X(t) \rangle \le b_i(t)&\; \forall t \in [-1, 1], \; \forall i \in \{1, \ldots, m\}.
  \end{array}
  \end{equation*}
  

  Note that TV-LPs are a special case of TV-SDPs, hence properties that apply to the latter apply to the former as well. In particular, even though we are interested in polynomial solutions, $X^{opt}(t)$ might not be polynomial, and we settle of near optimality (see Definition [[def:strict_feasibility_sdp]]). We focus for the most part on TV-LPs however because they are easier to study and provide insights into the more general problem of SDPs. Section [[sec:timevaryingsdp]] generalizes those results to TV-SDPs as well.
  

#+LATEX: \renewcommand\labelitemi{{\boldmath$\cdot$}}

** Notations and basic assumptions
   - For a integer $n$, we denote by $[n]$ the set $\{0, \ldots, n\}$.
   - For an integer $n$ and a set $S$, we denote by ${S \choose n}$ the set of all subsets of $S$ of exactly $n$ elements.
   - The $n \times 1$ vector whose component are all equal to 1 is denoted by a bold $\textbf{1}$.
   - For a matrix $A$ of dimension $m \times n$, $A_i$ is the $i^{th}$ row of $A$ for $i=1, \cdots, n$. If $m \ge n$, given an index set $B = (b_1, \ldots, b_n)$, then $A_B$ is the $n \times n$ sub-matrix $(A_{b_i,j})_{1 \le i, j \le n}$.
   - We denote the set of $n \times n$ symmetric (resp. positive semidefinite) matrices by $\mathcal S_n$ (resp. $\mathcal S_n^+$).
   - For $d \in \mathbb N$, $\mathbb R_d[t]$ is the set of polynomials in the variable $t$ with real coefficients that have degree  at most $d$.
   - For $d \in \mathbb N$, $\mathbb R^{n \times n}_d[t]$ is the set of $n \times n$ matrices whose components are polynomials in the variable $t$ with real coefficients that have degree  at most $d$.
   - We denote by $SOS_d$ the set of polynomials in $\mathbb R_d[t]$ that can be written as sum of squares of some polynomials.
   - We denote by $MSOS_d$ the set of symmetric matrix polynomials $X(t)$ in $\mathbb R^{n \times n}_d[t]$ that can be written as $Y(t)^TY(t)$, where $Y(t) \in \mathbb R^{n \times s}[t]$ and $s \in \mathbb N$.
   - The set $\mathcal P_t$ is the feasible set of a [[eqn:time_varying_lp_l2]] with data $(A(t), b(t), c(t))$ at time $t$, i.e. $\mathcal P_t = \{x \in \mathbb R^n | A(t) x \le b(t) \}$.
   - For a subset $C$ of $\mathbb R^n$, $conv(C)$ denotes is convex hull.
   - The ball of radius $r$ around $x \in \mathbb R^n$ is denoted by $B(x, r)$.
     
   Throughout the paper, we assume that the data to a time-varying program (i.e. $(A(t), b(t), c(t))$ in the linear case, $(A_i(t), b_i(t), c(t))_{i=1,\ldots, m}$ for the semidefinite case) is a polynomial function of time. We also assume that there exists at least one feasible solution to the program (Assumption 1). This Assumption can be checked in finite time for TV-LPs using Algorithm [[alg:checking_feasibility]]. Finally, we assume that for all $t \in [-1, 1]$, $\mathcal P_t$ is bounded (Assumption 2). We show in Theorem [[thm:bound_equiv_uniform_bound]] that the bound can be made independent of $t$ for TV-LPs.

** Organization and Contributions of the paper
   In this paper we propose an efficient method to find the best polynomial solution to a time-varying linear program or semidefinite program, as well as a characterization of when polynomial solutions are close to being optimal. The paper is organized as follow.
   In Section [[sec:timevaryinglp]], we first show that solutions to a [[eqn:time_varying_lp_l2]] are piecewise rational functions of time (Theorem [[thm:geometry_feasible_set_lp]]). Then, we give equivalent conditions under which continuous solutions are feasible and near optimal for a [[eqn:time_varying_lp_l2]] (Proposition [[prop:existence_cont_solution]]), and we prove that in fact, optimality and feasibility of continuous solutions are the same (Theorem [[thm:optimality_continuous_solution]]). We also mention the special case when the constraints matrix $A(t)$ is independent of time (e.g.  $A(t)$ is always equal to $A(0)$), in which case optimal polynomial solutions exist unconditionally (Theorem [[thm:A_constant]]). Finally, we prove that under strict feasibility conditions, polynomial solutions exist and are optimal (Theorem [[thm:strict_feasibility_implies_polynomial_optimality]]).
   In Section [[sec:decidabilityconditions]], we give a finite time algorithms for checking the feasibility of TV-LP (Algorithm [[alg:checking_feasibility]]), checking strict feasibility of TV-LP (Section [[sec:deciding_strict_feasibility]]), solving a TV-LP exactly (Algorithm [[alg:solving-time-varying-lp-exactly]]), and deciding feasibility of continuous solutions to a TV-LP (Section [[sec:deciding_feasibility_of_continuous]]).
   Section [[sec:lp_is_sdp]] presents an SDP formulation for finding the best polynomial solution of a [[eqn:time_varying_lp_l2]] (Theorem [[thm:tvlp_is_sdp]]).
  Section [[sec:timevaryingsdp]] discusses the case of a [[eqn:time_varying_sdp_l2]]s, and proves that under similar condition to [[eqn:time_varying_lp_l2]]s , polynomial solutions exist and are optimal, (Theorem [[thm:strict_feasibility_implies_polynomial_optimality_sdp]]). Moreover, the search problem for such polynomials with bounded degree can be cast as non varying SDP (Theorem [[thm:tvsdp_as_sdp]]).
   Section [[sec:numeric]] presents three applications of time-varying convex problems. The first one is TV-LP deciding the number of production units in an economy for which dependence of sectors and external demand vary with time (Section [[sec:leontief]]). The second one is a TV-LP that finds the maximum flow in a graph whose edge capacities change in time (Section [[sec:maxflow]]). The last one is a TV-SDP that solves the wireless coverage problem of regions moving in space (Section [[sec:wireless]]).
     
     
* Time-varying linear programs 
   <<sec:timevaryinglp>>

** Geometry of the feasible set of a TV-LP
   
   We start by presenting the following theorem that describes the geometry of the feasible set $\mathcal P_t$ of a [[eqn:time_varying_lp_l2]]. The theorem states that, except for some finite number of times, the feasible set is a the convex hull of points that move as rational functions in time. More formally:
  
   #+NAME: thm:geometry_feasible_set_lp
   #+BEGIN_thm
   Consider a [[eqn:time_varying_lp_l2]] with data $(A(t), b(t))$ and feasible set $\mathcal P_t$ at time $t \in [-1, 1]$ that satisfies Assumptions 1 and 2.
   
   There exist $N$ break points $-1 = t_1 < \cdots < t_N = 1$ and $N-1$ finite sets of rational functions $\mathcal V_1, \ldots, \mathcal V_{N-1} \subset \mathbb R^n(t)$ such that, for every $i \in \{ 1, \ldots, N-1\}$, for $t \in (t_i, t_{i+1})$, the feasible set $\mathcal P_t$ is the convex hull of the set of vertices $\{v(t), \; v \in \mathcal V_i\}$. Furthermore, for every $i$ in $\{ 1, \ldots, N-1\}$, every element $v$ of the set $\mathcal V_i$ can be associate with a subset $B_v \subseteq [m]$ such that $v(t) = A_{B_v}(t)^{-1}b_{B_v}(t)$ for  $t \in (t_i, t_{i+1})$.
   #+END_thm

   #+NAME: proof:geometry_feasible_set_lp
#+BEGIN_proof 
At any given time $t \in [-1, 1]$, $\mathcal P_t$ is a bounded polyhedron, so it is equal to the convex hull of its vertices. All vertices can be written as: $A_B(t)^{-1}b_B(t)$ for some $B \in  {[m] \choose n}$, i.e. for all $t \in [-1, 1]$, there exists a finite set $\mathcal B(t)$ such that $\mathcal P_t = conv\{A_B(t)^{-1}b_B(t), B \in \mathcal B(t)\}$.

It remains to show that $\mathcal B(t)$ changes at most finitely many times, which would prove the claim of the theorem. Indeed, that set changes at time $t_0$ only if one of these two things happen for some index set $B \in  {[m] \choose n}$: A nonzero polynomial of the form $t \rightarrow \det(A_B(t))$ equals $0$ at $t_0$, or one of the components of $t \rightarrow b(t) - A(t)A_B(t)^{-1}b_B(t)$ changes sign at $t_0$. Both scenarios happen only finitely many times.
#+END_proof

   Even though the previous theorem gives a description of the feasible set and ignores the objective function, it is not very hard to see that the optimal solution can also be chosen to be a piecewise rational function in $t$. Indeed, there always exists an optimal solution of a linear program on a vertex, and if $c(t)$ is ``nice'' enough, e.g. a polynomial, optimality of any given vertex changes only finitely many time inside $[-1, 1]$.
  
   #+NAME: thm:form_optimal_solution_lp
   #+BEGIN_thm
   Consider a [[eqn:time_varying_lp_l2]] with data $(A(t), b(t), c(t))$ that satisfies Assumptions 1 and 2. There exist breakpoints $-1 = t_1 < \cdots < t_N = 1$ and $N-1$ sets of rational functions $\mathcal V_1, \ldots, \mathcal V_{N-1}$ such that the following holds:

  For all $i = 1, \ldots, N$, there exist $v \in \mathcal V_i$ such that for every $t \in (t_i, t_{i+1})$, the optimal value at time $t$ of the [[eqn:time_varying_lp_l2]] is achieved at the point $v(t)$.

   In other terms, we can take the optimal solution  $x^{opt}(t)$ of the [[eqn:time_varying_lp_l2]] to be equal to $A_{B_i}(t)^{-1}b(t)$ for $t \in (t_i, t_{i+1})$, where $B_i \subseteq [m]$ is a set of $n$ indices.
   #+END_thm

   The theorem defines $x^{opt}(t)$ everywhere except on the times $t_i$. We could extend it at $t_i$ by taking the left or right limit for example (that exist, since $x^{opt}$ is a bounded piecewise rational function), call this function $\bar x^{opt}(t)$. Even though feasibility of $\bar x^{opt}(t)$ will be preserved on the interval $[-1, 1]$, point-wise optimality (i.e. optimality with respect to the objective $\langle c(t), \bar x^{opt}(t) \rangle$ for all $t \in [1-, 1]$ ) may not be as the following example shows.

   #+BEGIN_myexample
   Consider a [[eqn:time_varying_lp_l2]] with objective $c(t) = 1$ and two constraints $-t \le tx(t) \le t, -2 \le x(t) \le 2$.
   The unique point-wise optimal solution $x^{opt}(t)$  to this [[eqn:time_varying_lp_l2]] is
   
   \[x^{opt}(t) = \left\{\begin{array}{cc}1&t \ne 0\\2&t = 0\end{array}\right..\]

   The value $x^{opt}(t)$ takes at $0$ is neither the left nor the right limit at that point.
   #+END_myexample

   This is not a problem in our framework however, since we are mainly concerned by the average optimal value in time: $\int_{-1}^1 \langle c(t), x^{opt}(t) \rangle dt$, and changing $x^{opt}(t)$ at a set of measure 0 will not change that value. In the case where we are interested in maximizing the worst case: $\min_{t \in [-1, 1]} \langle c(t), x(t) \rangle$, we can notice that $$\langle c(t_i), x^{opt}(t_i)\rangle \ge \min_{t \in [-1, 1] \setminus \{t_1, \ldots, t_N\}} \langle c(t), x^{opt}(t) \rangle, \; \forall i \in \{1, \ldots, N\}.$$
   Therefore we do not lose by extending $x^{opt}$ in this way neither.

** Existence of continuous feasible solutions
   We are interested in the existence of polynomial solutions. One natural question to ask is whether such a solution always exists. The answer to that question is negative, and we prove that in fact even continuous solutions might not exist.

   #+BEGIN_myexample
   Consider the [[eqn:time_varying_lp_l2]] with two constraints: $tx \ge 0$ and $t(x-1) \ge 0$ for $t \in [-1, 1]$. The [[eqn:time_varying_lp_l2]] does not have a continuous feasible solution. We can see that by observing that the feasible set of this [[eqn:time_varying_lp_l2]]  is $[1, \infty)$ when $t > 0$ and $(-\infty, 0]$ when $t < 0$.
   #+END_myexample

   The reason no continuous solution exist is that the $\mathcal P_t$ are ``disconnected'' as time passes through 0. For a solution to exist, it has to ``jump'' at time 0. The following theorem formalizes this notion of continuity of sets and existence of continuous solutions.

   #+NAME: prop:existence_cont_solution
   #+BEGIN_prop
   Fix a [[eqn:time_varying_lp_l2]] with data $(A(t), b(t), c(t))$ with feasible set at time $t$ denoted by $\mathcal P_t$. Let $-1 = t_1 < \cdots < t_N = 1$ and $\mathcal V_1, \ldots, \mathcal V_{N-1}$ be the break points and the sets of rational functions defined by Theorem [[thm:geometry_feasible_set_lp]].
   
   The following statements are equivalent:
   1. The [[eqn:time_varying_lp_l2]] admits a continuous feasible solution.
   2. $\underset{\alpha \rightarrow 0}{\lim} dist(\mathcal P_{t_i-\alpha}, \mathcal P_{t_i+\alpha}) = 0$ for $i = 1, \ldots, N-1$.
   3. $conv \{ v(t_{i}), v \in \mathcal V_i\} \cap conv \{ v(t_{i}), v \in \mathcal V_{i+1}\} \ne \emptyset$ for $i = 1, \ldots, N-1$.
   #+END_prop

   #+BEGIN_proof
   We prove the theorem by proving the three implications   1 $\implies$ 2 $\implies$ 3 $\implies$ 1.
   
   (1 $\implies$ 2)
   Let $x(t)$ be a continuous solution to our [[eqn:time_varying_lp_l2]], then $\underset{\alpha \rightarrow 0}{\lim} dist(P_{t_i-\alpha}, P_{t_i+\alpha}) \le \underset{\alpha \rightarrow 0}{\lim} dist(x(t_i-\alpha), x(t_i+\alpha))= 0$

   (3 $\implies$ 1)
   Fix $i$ in $\{1, \ldots N-1\}$.
   We are first going to construct a continuous solution $x_i(t)$ that is defined for $t \in (t_{i-1}, t_{i+1})$.
   By Assumption 1, the intersection of $conv \{ v(t_{i}), v \in \mathcal V_i\}$ and $conv \{ v(t_{i}), v \in \mathcal V_{i+1}\}$ is non empty, therefore there exist two sets of non negative coefficients $(\lambda_v)_{v \in \mathcal V_i}$ and $(\mu_v)_{v \in \mathcal V_{i+1}}$, each one of them sums up to one in a such a way that:
   $$\sum_{v \in \mathcal V_i}  \lambda_{v} v(t_i) = \sum_{v \in \mathcal V_{i+1}}  \mu_{v} v(t_i).$$
   
   For $t \in (t_{i-1}, t_{i+1})$, define $x_i(t)$ to be the following function

   \[x_i(t) \coloneqq \left\{\begin{array}{cc}
   \sum_{v \in \mathcal V_i} \lambda_v v(t) & t \le t_i\\
   \sum_{v \in \mathcal V_{i+1}} \mu_v v(t) & t > t_i
   \end{array}\right. .
   \]

   It is clear that $x_i$ is feasible for our [[eqn:time_varying_lp_l2]] and continuous on its domain, i.e. $\lim_{t < t_i} x_i(t) = \lim_{t > t_i} x_i(t)$.


   We get a continuous feasible solution $f(t)$ on $[-1, 1]$ simply by ``connecting'' two solution $x_i$ and $x_{i+1}$ by interpolating from one to the other linearly. For example, let $\alpha = \min_{i=, 1\ldots, N-1} \frac{t_{i+1}-ti}3$, and define $f(t)$ to be the following function
      \[f(t) \coloneqq \left\{\begin{array}{cc}
   x_i(t) & t_i-\alpha \le t \le t_i+\alpha\\
   \frac{t - (t_{i+1} - \alpha)}{t_{i+1} - t_i - 2\alpha} x_i(t) + \frac{t - (t_i + \alpha)}{t_{i+1} - t_i - 2\alpha} x_{i+1}(t) & t_i+\alpha < t < t_{i+1}-\alpha
   \end{array}\right. ,
   \]
   with the convention that $t_{N+1} = 1$ and $x_{N+1}(t) = 0$.

   (2 $\implies$ 3)
   Fix $i$ in $\{1, \ldots N-1\}$, and let $\alpha_p \coloneqq \frac1p$ for $p \in \mathbb N$.
   
   By assumption, $\underset{p \rightarrow \infty}{\lim} dist(\mathcal P_{t_i-\alpha_p}, \mathcal P_{t_i+\alpha_p}) = 0$. Let   $(x_p)_{p \in \mathbb N}$ and $(y_p)_{p \in \mathbb N}$ be two sequences such that for all positive integers $p$, $x_p \in \mathcal P_{t_i-\alpha_p}$, $y_p \in \mathcal P_{t_i+\alpha_p}$ and $\underset{p \rightarrow \infty} {\lim}{||x_p - y_p||} = 0$. Furthermore, by taking a convergent subsequence, assume that $(x_p)$ and $(y_p)$ have a limit $\alpha=0$. Call $z$ their common limit.

   By definition of $\mathcal V_i$, there exists a linear combination

   $$x_p = \sum_{v \in \mathcal V} \lambda^p_v v(t_i-\alpha),$$
   such that $\lambda_v^p \ge 0$ for all $v \in \mathcal V$ and $\sum_{v \in \mathcal V} \lambda_v^p = 1$.
   
   Again, by taking a convergent subsequence, assume for all $v \in \mathcal V_i$ that the sequence $(\lambda_v^p)_p$ converges to a scalar $\lambda_v$. As a result of taking limits of both side of the previous equality, we get that
   $$z = \sum_{v \in V} \lambda_v v(t_i).$$

   In the same way, we prove that there exist non negative coefficients $\{\mu_v, v \in \mathcal V_{i+1}\}$ that sum up to 1 such that $z = \sum_{v \in \mathcal V_{i+1}} \mu_v v(t_i)$.

   We have just proved that $conv \{ v(t_{i}), v \in \mathcal V_i\} \cap conv \{ v(t_{i}), v \in \mathcal V_{i+1}\} \ne \emptyset$.
   #+END_proof

   A special case that is worth mentioning is when the matrix of constraints $A(t)$ of a [[eqn:time_varying_lp_l2]] does not depend on the time variable $t$. In that case, continuous feasible solutions always exist.

   #+NAME: thm:A_constant
   #+BEGIN_thm
   If the constraints matrix $A(t)$ of a TV-LP does not depend on $t$, then this TV-LP admits at least one continuous feasible solution.
   #+END_thm 

   Before we present the proof, we present a lemma due to D. H. Martin. It characterizes the continuity of the optimal value to an LP under perturbations to its data.

    #+NAME: lem:continuity_perturbation
    #+BEGIN_lemma
    (See \cite{Martin1975}).
    Consider the LP
    
    #+NAME: eq:lp
    \begin{equation*}
    \tag{LP}
    \begin{array}{ll@{}ll}
    \underset{x \in \mathbb R^n}{\text{maximize}} & \langle c, x \rangle\\
    \text{subject to}& A x \le b
    \end{array}
    \end{equation*}
    
    Let $\Omega$ be the set of tuples $(A, b)$ for which the set $\{x \in \mathbb R^n, Ax \le b\}$ is non empty and bounded, and $opt(A, b, c)$ the optimal value of [[eq:lp]] defined for $(A, b, c) \in \Omega \times \mathbb R^n$.

    The function $opt$ is continuous with respect to the variables $b$ and $c$ and upper semi-continuous with respect to the variable $A$.
   #+END_lemma

   #+BEGIN_proof 
   Assume for the sake of contradiction that no continuous feasible solution exist for a [[eqn:time_varying_lp_l2]] with a constant constraints matrix $A(t)$, then, by Proposition [[prop:existence_cont_solution]], there  exists $i \in [N]$ such that the two polytopes $conv \{ v(t_{i}), v \in \mathcal V_i\}$ and $conv \{ v(t_{i}), v \in \mathcal V_{i+1}\}$ have empty intersection. As a result, there is a separating hyperplane with normal $u \in \mathbb R^n$ and a positive scalar $\delta$ such that $\langle v(t_i) , u \rangle > \delta$ for $v \in \mathcal V_i$ and $\langle v(t_i) , u \rangle < -\delta$ for $v \in \mathcal V_{i+1}$.


   That contradicts the fact that the following LP has an optimal value that is continuous with respect to the parameter $\alpha$ in the neighborhood of 0 (because of Lemma [[lem:continuity_perturbation]]):
   $$\underset{x \in P_{t_i+\alpha}}{\text{minimize}} \langle x, u \rangle.$$
  
   #+END_proof

   Now that we have established conditions for the existence of feasible continuous solution to a [[eqn:time_varying_lp_l2]], one might wonder what additional conditions are needed to also guarantee near optimality of continuous solutions.

   It turns out that  whenever there exists one feasible continuous solution, there also exists near optimal continuous solution.
  
   #+NAME: thm:optimality_continuous_solution
   #+BEGIN_thm
   Suppose a [[eqn:time_varying_lp_l2]] with data $(A(t), b(t), c(t))$ admits a feasible continuous solution $f_0$, i.e. there exists a continuous function $f_0: [-1, 1] \rightarrow \mathbb R^n$ such that $A(t)f_0(t) \le b(t)$, $\forall t \in [-1, 1]$.
  
   Then, continuous solutions are near optimal for the TV-LP, i.e. for every $\varepsilon > 0$, there exists a continuous function $f: [-1, 1] \rightarrow \mathbb R^n$ that is feasible and $\int_0^1 \langle c(t), x(t)\rangle - \int_0^1 \langle c(t), f(t)\rangle \le \varepsilon$.
   #+END_thm
   
    #+BEGIN_proof
Fix a [[eqn:time_varying_lp_l2]] that has a feasible continuous solution $f_0$ on $[-1, 1]$.
Following the result of Theorem [[thm:geometry_feasible_set_lp]], there exists  a partition  of $[-1, 1]$ with break points $t_1, \ldots, t_N$ and an optimal solution $x^{opt}(t)$ that is continuous on every interval $(t_i, t_{i+1})$.

    We want to construct a function that is as close as possible to $x^{opt}(t)$ while staying continuous, which would prove the claim of the theorem.
    
    For this purpose, define the interval $I_i^{\alpha} \coloneqq (t_i-\alpha, t_i+\alpha) \cap [-1, 1]$ for some positive scalar $\alpha$.

  Let $f^{\alpha}$ be the function that is equal to $x^{opt}(t)$ on every $I_i^{\alpha}$, equal to $f_0$ on all the $t_i$ and interpolates linearly between $x(t)$ and $f_0(t)$ on $[t_i-\alpha, t_i+\alpha]$.

    In a sense, $f^{\alpha}$ lives on the optimal vertex but ``travels'' to the continuous solution $f_0$ to get through the possibly problematic time $t_i$.
    
    As $\alpha \rightarrow 0$, $f^{\alpha}(t) \rightarrow x^{opt}(t)$ almost surely on $[-1, 1]$. Given that the inequality $|f^{\alpha}(t)| \le |x^{opt}(t)| + |f_0(t)|$ holds for all $t \in [-1, 1]$, the Dominated Convergence theorem gives $\int_{-1}^1 (f^{\alpha}(t)- x^{opt}(t)^2 dt \rightarrow 0$, and we conclude by Cauchy-Schwarz that for any $\varepsilon > 0$, if we take $\alpha$ small enough, $f^{\alpha}$ is $\varepsilon\text{-near}$ optimal.
  #+END_proof
      

** A simple condition that guarantees existence and optimality of continuous solutions

   In this section we present a simple condition under which continuous feasible solutions to a [[eqn:time_varying_lp_l2]] exists. The condition can be stated as a feasibility problem of a new [[eqn:time_varying_lp_l2]], described in the following definition, with slightly tighter constraints.
   
   #+NAME: def:strict_feasibility
   #+BEGIN_definition
A [[eqn:time_varying_lp_l2]] is \emph{strictly feasible} if there exists a (not necessarily continuous) function $x^s: [-1, 1] \rightarrow \mathbb R^n$   and a scalar $\varepsilon > 0$ such that

$$A(t)x^s(t) \le b(t) - \varepsilon \textbf{1}, \; \forall t \in [-1, 1].$$

\noindent In this case we say that $x^s(t)$ is strictly feasible for our [[eqn:time_varying_lp_l2]].
#+END_definition
   
The condition of existence of continuous solution to a [[eqn:time_varying_lp_l2]] can now be formulated as follow:

#+NAME: thm:strict_feasibility_implie_continuous_optimality
#+BEGIN_thm
If a [[eqn:time_varying_lp_l2]]  is strictly feasible, then it has a continuous near optimal solution.
#+END_thm

We will in fact prove a stronger statement. We will prove that any TV-LP that admits a strictly feasible solution, also admists a /continuous/ strictly feasible solution.

#+BEGIN_proof
Assume strict feasibility of a [[eqn:time_varying_lp_l2]].

By Theorem [[thm:optimality_continuous_solution]], it is enough to prove the existence of a continuous feasible solution $x^c(t)$ to our [[eqn:time_varying_lp_l2]].

Recall from Theorem [[thm:geometry_feasible_set_lp]] that there exists an integer $N > 0$, and breakpoints $-1 = t_1 < \cdots < t_N = 1$ such that, for all $i = 1, \ldots, N$, there exists a finite set of rational functions $\mathcal V_i$ (the vertices) such that $\mathcal P_t = conv\{ u(t), u \in \mathcal V_i \}$ for all $t \in (t_i, t_{i-1})$.

We provide a construction of $x^c(t)$ in two steps depending on whether we are near the problematic points $t_i$, $i = 2, \ldots, N-1$ or far away from them, then we connect these patches by interpolating between them. 

\paragraph{Near the problematic points $t_i$:}

For some positive $\varepsilon$, the polytope $\{x \in \mathbb R^n |  A(t_i)x \le b(t_i) - \varepsilon 1\}$ is not empty by strict feasibility. Let $w$ be one of its extreme points. Then there exists a basis $B$ such that $w = A_B(t_i)^{-1}(b_B(t_i) - \varepsilon \textbf{1})$.

Now define $w_i^{near}(t) \coloneqq A_B(t)^{-1}(b_B(t) - \varepsilon 1)$, then there exists a neighborhood of $t_i$, $[t_i-\alpha, t_i+\alpha]$, such that (i) $w_i^{near}(t)$ is a well defined continuous function and (ii) $w_i^{near}(t)$ is strictly feasible.

Indeed, (i) is true because  $\det(A_B(t_i)) \ne 0$ implies that $\det(A_B(t)) \ne 0$ in the vicinity of $t_i$. To see why (ii) is true, we observe that since $A(t_i)w_i^{near}(t_i) \le b(t_i) - \varepsilon 1$, the inequality $A(t)w_i^{near}(t) \le b(t) - \frac{\varepsilon}2 1$ remains true when $t$ is arbitrarily close to $t_i$.

Furthermore, since the number of breakpoints $t_i$ s is finite, we can make the same choice of $\alpha$ for all $i = 1, \cdots, N$.

\paragraph{Far away from the $t_i$:}

For  $t \in (t_i, t_{i+1})$, let $w_i^{far}(t) \coloneqq \frac{\sum_{u \in \mathcal V_i} u(t)}{|\mathcal V_i|} \in \mathcal P_t$.

#+BEGIN_COMMENT
Similarly, for $t \in (t_{i-1}, t_{i})$, let $w_i^{far}(t) \coloneqq \frac{\sum_{u \in \mathcal V_{i-1}} u(t)}{|\mathcal V_{i-1}|} \in \mathcal P_t$. Notice that $w_{i+1}^{left} = w_i^{far}$ for $i=1,\cdots,N-1$.
#+END_COMMENT

\noindent Let's prove that  $w_i^{far}$ is strictly feasible on $J_i \coloneqq [t_i+\beta, t_{i+1}-\beta]$, with $\beta$ equal to (say) $\min_{i=2,\ldots, N-1} \frac{t_{i+1}-t_i}{3}$.

\noindent Let
$$\delta_i^{} \coloneqq \min_{t \in J_i, j=1,\ldots, m} (b(t) - A(t)w_i^{far}(t))_j.$$
Observe that $\delta_i > 0$. Otherwise, by continuity, there exist $\hat j$ and $\hat t \in J_i$ such that $(b(\hat t) - A(\hat t)w^{far}(\hat t))_{\hat j} = 0$, which means that 
$0 = b_{\hat j}(\hat t)- A_{\hat j}^T(\hat t)w^{far}(\hat t) = \frac1{|\mathcal V_i|} \sum_{u \in \mathcal V_i} \underbrace{(b_{\hat j}(\hat t) - A_{\hat j}(\hat t)^Tu(\hat t))}_{\ge 0}$, i.e. all $\mathcal P_t$ 's vertices belong to same affine hyper plane $\{x \in \mathbb R^n |\; A_{\hat j}(\hat t)^T x = b_{\hat j}(\hat t) \}$, which contradicts the existence of a strictly feasible point $x^s(t)$.

\paragraph{Connecting the patches:}

We get a continuous feasible solution on $[-1, 1]$ simply by ``connecting'' the solutions $w_i^{far}, w_i^{near}$ by interpolating from one to the other. 

To ease notation, we can assume without loss of generality that $\alpha = 2 \beta$. We also define the function $I_a^b(t)$ to be the linear function equal to $0$ at $t = a$, and to $1$ at $t = b$.


Define $x^c(t)$ to be the continuous function defined as follow:



   \[x^c(t) = \left\{\begin{array}{cc}
   w_i^{far}(t) & t_{i-1}+2\beta < t \le t_{i} - 2\beta\\
   I_{t_{i} - 2\beta}^{t_{i} - \beta}(t) (w_i^{near}(t) - w_i^{far}(t)) + w_i^{far}(t) & t_{i}-2\beta < t \le t_{i} - \beta\\
   w_i^{near}(t) & t_{i}-\beta < t \le t_{i} + 2\beta\\
   \end{array}\right.
   \]


   It is easy to see that $x^c(t)$ is continuous on $[-1, 1]$. Furthermore, at all times $t \in [-1, 1]$, $x^c(t)$ is a convex combination of solutions that are strictly feasible, so that $x^c(t)$ is also $\varepsilon'-$ strictly feasible with  $\varepsilon' \coloneqq \min(\varepsilon/2, \min_{i=1,\cdots,N} \delta_i)$.
#+END_proof



** From continuous solutions to polynomial solutions
   <<sec:condition_polynomials_optimal>>
   Our goal in this section is to understand when a [[eqn:time_varying_lp_l2]] has a near optimal /polynomial/ solution. Existence of near optimal /continuous/ solutions is a necessary condition but unfortunately not sufficient as the following simple example shows. 

   #+BEGIN_myexample
   Consider the following [[eqn:time_varying_lp_l2]] with two constraints: $(1+t^2) x(t) \le 1, -(1+t^2) x(t) \le -1, \forall t \in [-1, 1]$. Clearly the only feasible solution is the continuous function $x(t) = \frac1{1+t^2}$. However, this [[eqn:time_varying_lp_l2]] does not admit a feasible (let alone optimal) polynomial solution.
   #+END_myexample

   To avoid such examples we need to make sure that the continuous solution can be approximated with a polynomial function that stays inside the feasible set. This motivates the following definition

#+NAME: def:continuous_full_dimensionality
#+BEGIN_definition
A [[eqn:time_varying_lp_l2]] with feasible set $\mathcal P_t$ at time $t \in [-1, 1]$ is \emph{continuously full-dimensional} if there exists a scalar $\delta > 0$ and a /continuous/ function $x^c: [-1, 1] \rightarrow \mathbb R^n$ such that $B(x^c(t), \delta) \subset \mathcal P_t, \; \forall t \in [-1, 1]$.
#+END_definition


The condition that $\delta$ does not depend on $t$, as well as continuity of $x^c(t)$, are important. The following example demonstrates that.

   #+BEGIN_myexample
Consider a [[eqn:time_varying_lp_l2]] with two constraints $-2 \le x(t) \le 2, tx(t) \ge 0$ for all $t \in [-1, 1]$. The feasible set here at time $t$, $\mathcal P_t$, is $\mathbb R^+$ for $t > 0$, $\mathbb R^-$ for $t < 0$, and the whole real line $\mathbb R$ when $t=0$.

This program is not continuously full-dimensional. Indeed, every continuous solution $x(t)$ to this program has to be equal to 0 at $t=0$. Now for every $\delta > 0$, for $t>0$ arbitrarily close to $0$, $x(t) < \frac{\delta}2$, and therefore the ball $B(x(t), \delta)$ cannot stay inside the feasible set at time of this [[eqn:time_varying_lp_l2]].

Notice however that the feasible continuous solution $x(t) = t$ verifies $B(x(t), \delta_t) \subset \mathcal P_t$, with $\delta_t = \frac t2$ for $t \ne 0$ and $\delta_0 = 1$. Moreover, the feasible (non continuous) solution
\[x(t) = \left\{\begin{array}{cc}1 & t > 0\\0&t=0\\-1&t<0\end{array}\right.\]
verifies $B(x(t), 1) \subset \mathcal P_t$ with  for $t \in [-1, 1]$.
   #+END_myexample

   We show next that full-dimensionality is exactly what is needed for the existence of the optimality of polynomial solutions.
   
   #+NAME: prop:optimality_poly_solution
   #+BEGIN_prop
   Suppose a [[eqn:time_varying_lp_l2]] is continuously full-dimensional, and denotes its optimal value by $opt$.

   Then, for every $\varepsilon > 0$, there exists a polynomial function $p: [-1, 1] \rightarrow \mathbb R^n$ such that  $p(t)$ is feasible to our [[eqn:time_varying_lp_l2]], and $\int_{-1}^1 \langle c(t), p(t)\rangle dt - opt \le \varepsilon$.
   #+END_prop


   #+BEGIN_proof
   We start with a continuous solution $g$ that is $\varepsilon/3$ -near optimal to our [[eqn:time_varying_lp_l2]],  whose existence is guaranteed by Theorem [[thm:optimality_continuous_solution]]. Ideally we would like to approximate $g$ uniformly by a polynomial $p$, but $p$ might not be feasible. To correct this problem, we replace $g$ by a convex combination of $g$ and $x^s$, a strictly feasible solution. Define $f \coloneqq \lambda g + (1-\lambda) x^s$, and notice that for $\lambda < 1$, $g$ is strictly feasible, but when $\lambda$ is close to 1, $f$ is also $\varepsilon/2$ -near optimal. 


   Weierstrass approximation theorem proves the existence of $p(t)$, a polynomial that approximates $g(t)$ uniformly, i.e., $\forall t \in [-1, 1] \; ||p(t) - f(t)||_2^2  \le \delta^2$, where $\delta$ is a constant we are going to fix latex.

   For $\delta$ smaller than $\varepsilon/2$, $p(t)$ is inside $\mathcal P_t$ for all $t \in [-1, 1]$.
   
   Let's now examine the objective value of $f$:
   $$\int_{-1}^1 \langle c(t), p(t)\rangle \le  \int_{-1}^1 \langle c(t), f(t)\rangle + \int_{-1}^1 ||f(t) - p(t)||_2 ||c(t)||_2 dt \le opt + \varepsilon/2 + \delta \int_{-1}^1 ||c(t)||_2 dt$$
   
   Again, taking $\delta < \frac{\varepsilon/2}{1+\int_{-1}^1 ||c(t)||_2 dt}$ gives the result.
   #+END_proof



   A natural question here is how Definition [[def:continuous_full_dimensionality]] of continuous full-dimensionality compares to Definition [[def:strict_feasibility]] of strict feasibility, and if strict feasibility also guarantees the optimality of polynomial solutions as it does for continuous solutions. The rest of this section is devoted to this two questions.
   
   While Definition [[def:strict_feasibility]] provides slackness in the space of the constraints, Definition [[def:continuous_full_dimensionality]] requires the existence of a continuous solution with a ball with fixed radius around it that stays feasible for all times.

   We can easily see that for any [[eqn:time_varying_lp_l2]], full-dimensionality of a continuous solution implies strict feasibility when for all $t \in [-1, 1]$, no row of the constraints inequality $A(t)$ is identically zero.

#+BEGIN_prop
If a [[eqn:time_varying_lp_l2]] is continuously full-dimensional and has a constraint matrix with non identically zero rows for all $t \in [-1, 1]$, then the [[eqn:time_varying_lp_l2]] is strictly feasible.
#+END_prop

#+BEGIN_proof
Fix a continuously full-dimensional [[eqn:time_varying_lp_l2]] with data $(A(t), b(t), c(t))$ and feasible set $\mathcal P_t$ at time $t \in [-1, 1]$. Let $\delta$ be positive scalar and  $x^c: [-1, 1] \rightarrow \mathbb R^n$ a continuous feasible solutions for this [[eqn:time_varying_lp_l2]] such that $B(x^c(t), \delta) \subset \mathcal P_t$ for all $t \in [-1, 1]$.


Let's define
$$\varepsilon \coloneqq \min_{i=1, \ldots, n} \min_{t \in [-1, 1]} (b(t) - A(t)x^c(t))_i.$$

Observe that $\varepsilon > 0$, because otherwise, if $\varepsilon = 0$, then by continuity the minimum is attained at some $(t_m, i_m) \in [-1, 1] \times \{1, \ldots, n\}$ for which $b_{i_m}(t_m) - A_{i_m}(t_m)x^c(t_m) = 0$. By continuous full-dimensionality of $x^c(t)$, if $u \in \mathbb R^n$ has norm smaller than  $\delta$, then $b_{i_m}(t) - A_{i_m}(t_m)(x^c(t_m) + u) \ge 0$, which leads to $A_i(t_m)^Tu \ge 0$, and to $A_i(t_m) = 0$.

We have just proved that $(\forall t \in [-1, 1]) \; A(t) x^c(t) \le b(t) - \varepsilon 1$ for some $\varepsilon > 0$.
#+END_proof

Perhaps the more surprising result is that the converse is also true (unconditionally):

#+BEGIN_prop
If a [[eqn:time_varying_lp_l2]] is strictly feasible then it is also continuously full-dimensional.
#+END_prop

#+BEGIN_proof
Under the strict feasibility condition, we know from Theorem [[thm:strict_feasibility_implie_continuous_optimality]] that the [[eqn:time_varying_lp_l2]] admits a strict feasible continuous solution $x^c(t)$ defined on $[-1, 1]$, i.e. there exists a scalar $\varepsilon > 0$ such that $A(t)x^c(t) \le b(t) - \varepsilon 1,\; \forall t \in [-1, 1]$.


Now let $\delta \coloneqq \frac{\varepsilon}{\max_{t \in [-1, 1]} ||A(t)||_2}$, and fix a time $t \in [-1, 1]$ and $y \in B(x^{c}(t), \delta)$. The inequalities below prove that $y \in \mathcal P_t$. As a consequence, our [[eqn:time_varying_lp_l2]] is continuously full-dimensional.

\begin{align*}
A(t)y &= A(t)x(t) + A(t) (y - x(t))
\\&\le b(t) - \varepsilon 1 + \delta \max_{t \in [-1, 1]} ||A(t)||_2 1
\\&\le b(t)
\end{align*}
#+END_proof


We are now ready to present the main characterization for the existence and optimality of polynomial solutions.

#+NAME: thm:strict_feasibility_implies_polynomial_optimality
   #+BEGIN_thm
If a [[eqn:time_varying_lp_l2]] is strictly feasibility, then for every $\varepsilon > 0$, there exists a polynomial function that is $\varepsilon-$ near optimal.
   #+END_thm


** On the assumption of boundedness of the feasible set

   We end this discussion by proving that if the feasible set $\mathcal P_t$ to a TV-LP is bounded for all times $t \in [-1, 1]$, then the bound can be made uniform in $t$.

   #+NAME: thm:bound_equiv_uniform_bound
   #+BEGIN_thm
   Suppose that $\mathcal P_t$ is feasible for all $t \in [-1, 1]$. If $\underset{x \in \mathcal P_t}{\sup} ||x|| < \infty$ for all $t \in [-1, 1]$, then  $\underset{x \in \underset{t \in [-1, 1]}{\cup} \mathcal P_t}{\sup} ||x|| < \infty$.
   #+END_thm

   #+BEGIN_proof
   For $t \in [-1, 1]$, and $u \in \{-1, 1\}^n$, consider the following maximization program $$\underset{x \in \mathcal P_t}{\text{maximize}} \sum_{i=1}^n u_i x_i$$ and denote its optimal value by $f_u(t)$. The function $f_u(t)$ is finite for all $t \in [-1, 1]$ and we want to prove that $f_u(t)$ can be uniformly bounded on $[-1, 1]$.

   Notice that $f_u(t)$ is the optimal value of an LP for all $t \in [-1,1]$.

   By the assumption made earlier, those LPs have their sets of solution bounded for all $t \in [-1, 1]$. As a result, all conditions for Lemma [[lem:continuity_perturbation]] are verified, and we conclude that  the function $f_u(t)$ is upper semi-continuous.
   
   Now, if $(t_n)_{n \in \mathbb N}$ is a convergent sequence such that $t_0 = \underset{n \rightarrow \infty}{\lim} t_n$ and $ \sup_t f_u(t) =  \underset{n \rightarrow \infty}{\lim} f_u(t_n)$, then: $\sup_t f_u(t) = \lim_n f_u(t_n) \le f_u(t_0) < \infty$. We have just found a uniform bound for the function $f_u(t)$ on $[-1, 1]$.

   We conclude by noting that $\underset{x \in \underset{t \in [-1, 1]}{\cup} \mathcal P_t}{\sup} ||x||_1 = \underset{u \in \{-1, 1\}}{\max} f_u(x) < \infty$.
   #+END_proof

   
   
* Decidability of the sufficient conditions for existence and optimality of polynomial solutions to LPs
<<sec:decidabilityconditions>>
  This section presents finite time algorithms to decide the conditions discussed in the previous section that guarantee feasibility of TV-LP as well as optimality of polynomial solutions. 
  
  Theorem [[thm:form_optimal_solution_lp]] showed that the feasible set of a TV-LP can be fully described by giving the times $t_1, \ldots, t_N$ as well as the coefficients of the rational functions in the set $\mathcal V_i$ for all $i=1, \ldots, N$. We propose an algorithm that does exactly that. 
  
  Notice that since the algorithm produces a vertex description of the moving polytope $\mathcal P_t$, getting an optimal solution for all $t \in [-1, 1]$ is straightforward.

  We note that this algorithm is not practical, and its only purpose is to prove that one can solve a TV-LP exactly. We present later in Section [[sec:lp_is_sdp]] an efficient algorithm that relies on semidefinite programming and finds the best polynomial solution of a bounded degree to a TV-LP.
  
** Feasibility of a TV-LP

   We present an algorithm that decides whether a TV-LP is feasible. If that's the case, we know from Theorem [[thm:geometry_feasible_set_lp]] that we can characterize the feasible set of the TV-LP in time fully by giving the breakpoints $T = \{t_1, \ldots, t_N\}$ and the sets of vertices $\mathcal V_1, \ldots, \mathcal V_{N-1} \subset \mathbb R^n(t)$ described in the same theorem.

    Following the proof of Theorem [[thm:geometry_feasible_set_lp]], we can take the set of breakpoints $T$ to be the times where one of the following univariate rational functions changes sign:
    $$P \coloneqq \{p: t \rightarrow \det(A_B(t))\;| \; B \in {[m]\choose n}, p \ne 0\},$$
    $$Q \coloneqq \{q: t \rightarrow b(t) - A_B(t)^{-1}b_B(t) \;| \; B \in {[m]\choose n}, q \ne 0\}.$$

   It is clear that between two consecutive times $t_i$ and $t_{i+1}$ in $T$, the subset of constraints $B_v$ that are tight for the extreme points $v \in \mathcal V_i$ of the feasible set $\mathcal P_t$ of the TV-LP do not change. It is therefore sufficient to find those subsets at time (say) $\frac{t_i+t_{i+1}}2$.

   #+BEGIN_remark
   From a computation point of view, it is important to decide on the desired accuracy when calculating the elements of the set $T$. Indeed, we need to distinguish between the case when two roots $t$ and $u$ are equal or are just very close. In other terms, we need a lower bound on $\min_{t, u \in T, t \ne u} |u - t|$. The following lemma provides just that:
   
   #+BEGIN_lemma
   [See \cite{Mahler1964}]

   For $n \in \mathbb N$, there exists a universal constant $C_n$ such that the following holds:

   If $p(t) = \sum_{i=0}^n a_i t^i$ is a univariate polynomial in the variable $t$ with integer coefficients $(a_0, \ldots, a_n)$, and $\alpha$ and $\beta$ are two distinct roots of $p(t)$, then

   $$|\alpha - \beta| \ge C_n \frac1{\max |a_i|^n}.$$
   #+END_lemma


   If we apply this lemma to the polynomial obtained by taking the product of all the rational functions in $P$ and $Q$ as well as their common denominator, we get the accuracy needed for computing the elements of $T$.
   #+END_remark
   
   We propose Algorithm [[alg:checking_feasibility]], that takes as input the data of a TV-LP $(A, b, c)$, computes the set $T$ described earlier, and outputs the sets $\mathcal V_i, i=1,\ldots N-1$.
    
#+NAME: alg:checking_feasibility
 #+BEGIN_algorithm
\caption{Check feasibility}
\begin{algorithmic}[1]
\Procedure{Check Feasibility of a TV-LP with data $(A(t), b(t), c(t))$}{}
\State \text{Compute $T$ , the finite set of points where the}
\State \text{rational functions in the sets $P$ and $Q$ change signs.}
\State \text{(amounts to finding the roots of polynomials)}
\For{ $i=1, \ldots, \operatorname{len}(T)-1$}
\State $t \gets \frac{T[i] + T[i+1]}2$
\State \text{Find the extreme points $V$ of the set $\mathcal P_t = \{x \in \mathbb R^n A(t)x\le b(t)\}$.}
\State \text{For every set of constraints $B_v$ that is tight for one these extreme points}
\State \text{$v \in V$, output the rational function $A_{B_v}(t)^{-1}(t)b(t)$ defined on $(t_i, t_{i+1})$.}
\EndFor
\EndProcedure
\end{algorithmic}
#+END_algorithm
   
** Solving a TV-LP exactly

   This section describes how to obtain the optimal solution $x^{opt}(t)$ of a TV-LP with data $(A(t), b(t), c(t))$.   We know from Theorem [[thm:form_optimal_solution_lp]] that we can take $x^{opt}(t)$ to be piecewise rational function. Furthermore, using Algorithm [[alg:checking_feasibility]], we know that there exist times $t_1, \ldots t_N$ and sets $\mathcal V_1, \ldots, \mathcal V_{N-1}$, such that, for $t$ between two consecutive times $t_i$ and $t_{i+1}$, the extreme points of the feasible set of the TV-LP at time $t$ are $\{ v(t), \; v \in \mathcal V_i\}$. Therefore, we can take $x^{opt}(t)$ to be equal to the extreme point with the highest objective value $p_v(t) \coloneqq \langle c(t), v(t) \rangle$, with $v \in \mathcal V_i$. The set of constraint that are tight for that extreme point change finitely many times on the interval $(t_i, t_{i+1})$. That is because a change can only happen on the roots of one the following polynomials $\{q = p_v - p_w \; | \; v, w \in \mathcal V_i, q \ne 0\}$. Call this set of roots $R_i \coloneqq \{r_1, \ldots r_{|R|}\} \subset (t_i, t_{i+1})$.

   Algorithm [[alg:solving-time-varying-lp-exactly]] takes (A, b, c) as input, loops through all $t_i \in T$, and all $r_j \in R_i$, and finds the optimal solution to the TV-LP on every interval of the form $(r_j, r_{j+1})$, for $j=1, \ldots |R|-1$.

   #+NAME: alg:solving-time-varying-lp-exactly
   #+BEGIN_algorithm
   \caption{Find optimal solution}
   \begin{algorithmic}[1]
   \Procedure{Solve TV-LP with data $(A(t), b(t), c(t))$}{}
   \State \text{Compute the breakpoints $T = \{t_1, \ldots, t_N\}$ and the }
   \State \text{sets $\mathcal V_1, \ldots, \mathcal V_{N-1}$ using Algorithm 1}.
   \For{ $i=1, \ldots, \operatorname{len}(T)-1$}
   \For{ $j =1, \ldots, \operatorname{len}(R_i)-1$}\label{alg:continuousfor}
   \State $r \gets \frac{R_i[j] + R_i[j+1]}2.$
   \State $v^* \gets \argmax_{v \in \mathcal V_i} p_v(r).$
   \State \text{$x^{opt}(t) \gets v^*(t)$ on $(R_i[j], R_i[j+1])$.}
   \EndFor
   \EndFor
   \EndProcedure
   \end{algorithmic}
   #+END_algorithm


** Deciding strict feasibility of a TV-LP
   <<sec:deciding_strict_feasibility>>
   We seek to decide whether the following LP is feasible or not for some $\varepsilon > 0$:
   $$A(t)x(t) \le b(t) - \varepsilon 1$$

   This is the case if and only if the optimal solution $(\varepsilon(t), x(t))$ to the following TV-LP
   
   \begin{equation*}   
   \begin{array}{ll@{}ll}
   \underset{x(t) \in \mathbb R^n, \varepsilon(t) \in \mathbb R}{\text{maximize}} & \int_{-1}^1 \varepsilon(t) dt & \\
   \text{s.t}& A(t)x(t) \le b(t) - \varepsilon(t) 1
   \end{array},
   \end{equation*}
   
verifies $\inf_{t \in [-1, 1]} \varepsilon(t) > 0$. We can use Algorithm [[alg:solving-time-varying-lp-exactly]] to solve this TV-LP.

   
** Deciding feasibility of continuous solutions to a TV-LP
   <<sec:deciding_feasibility_of_continuous>>
   Using characterization [[prop:existence_cont_solution]], we can decide whether there exists a continuous solution that lives inside $\mathcal P_t$ for all $t \in [-1, 1]$. To do that, we look at times $t_{2}, \ldots, t_{N-1}$ given by algorithm [[alg:solving-time-varying-lp-exactly]], as well as the set of vertices $\mathcal V_1, \ldots, \mathcal V_N$ provided by the same algorithm, and for $2 \le i \le N-1$, we check that the following polytope is not empty:
   $$\operatorname{conv}(v(t_i), v \in \mathcal V_i) \cap \operatorname{conv}(v(t_i), v \in \mathcal V_{i+1})$$

   And this can be done in efficiently using standard linear programming algorithms.
   



* Finding the best polynomial solution to a TV-LP via SDP
    <<sec:lp_is_sdp>>

  Algorithm [[alg:solving-time-varying-lp-exactly]] of the previous section proves that one can solve exactly a TV-LP, and get the optimal solution in finite time, even though the solution is not continuous. The algorithm takes at least exponential time[fn::the time complexity of algorithms described in this paper is always with respect to the size of the input $(A, b, c)$ for TV-LPs and $((A_i)_{i\in[m]}, (b_i)_{i=1}^m, C)$ for timevaryign SDPs] as it checks all the vertices of the polytope.
  
  This section describes how one can find the best /polynomial/ solution of a given degree. We describe an algorithm that is polynomial in time. Indeed, we prove that we can turn a TV-LP into an semidefinite program. The idea behind such a reduction is that a univariate polynomial $p(t)$ is non negative on some interval, say $[-1, 1]$, if and only if it can be written as a sum of square of two polynomials $q(t), s(t)$, potentially weightted by $(1-t)$ and/or $(1+t)$, and searching for $q(t)$ and $s(t)$ can be done efficiently. Formally:
  
  
  #+BEGIN_thm
  [See Section 3. of \cite{Papp}]
  A polynomial $p$ of degree $n$ is nonnegative over $[-1,1]$ if and only if it can be written as a weighted sum of squared polynomials, either in the form of
  \begin{equation*}
  p(t)=(1+t)q(t)+(1-t)r(t), \quad q\in SOS_{n-1},\; s\in SOS_{n-1} \qquad \text{if $n$ is odd},
  \end{equation*}
  or in the form
  \begin{equation*}
  p(t)=(1+t)(1-t)q(t)+s(t), \quad q\in SOS_{n-2},\; s\in SOS_n, \qquad \text{if $n$ is even.}
  \end{equation*}

  Denote the set of univariate non negative polynomials on the interval $[-1, 1]$ by $ISOS$.
  #+END_thm

  As a result of this theorem, we can now rewrite a [[eqn:time_varying_lp_l2]] as (non time-varying) SDP:

  #+NAME: thm:tvlp_is_sdp
  #+BEGIN_thm
  Fix an integer $d$. The following SDP find the best polynomial solution of degree smaller or equal than  $2d+1$.

  #+NAME: eqn:Ppoly
  \begin{equation*}
  \begin{array}{ll@{}ll}
  \underset{x(t) \in \mathbb R^n_d[t]}{\text{maximize}} & \int_{-1}^1 \langle c(t), x(t) \rangle dt & \\
  \text{subject to}& b(t) - A(t) x(t) \in ISOS\\
  \end{array}
  \end{equation*}
  #+END_thm

  To see that this is indeed an SDP, we need to prove that the set $ISOS \cap \mathbb R_d[t]$ is a projected spectrahedron, or in other terms a set defined by linear and positive semidefinite constraints (i.e, constraint of the form $X \succeq 0$). one can note that the equality between two polynomials of degree at most $d$ can be written as an equality of the values they take at $d+1$ different times (which are linear functions of their coefficients), and the condition that a polynomial $\sigma(t)$ is sum of squares can be expressed as a positive semidefinite constraint using the following proposition:

  #+NAME: thm:tvlp-to-sdp
  #+BEGIN_thm
  [See \cite{Parrilo2004}]
  
  Consider breakpoints $t_0< \cdots < t_{2k}$, a basis of $\mathbb R_k[t]$, $p_0, \ldots, p_k$, and define $A^{(l)}$ to be the $(k+1) \times (k+1)$ matrix whose $(i, j)$ entry  $A_{ij}^{(l)}$ is equal to $p_i(t_l)p_j(t_l)$ for $0 \le i, j \le k$.
  
   A polynomial $q(t)$ is in $SOS_k$ if and only if there exists $X \in \mathcal S_{k+1}$ such that
$$q(t_l) = \langle X, A^{(l)} \rangle \quad \forall l \in [2k].$$

#+END_thm

  In practice, the choice of a suitable basis of polynomials $p_1, \ldots, p_k$ and interpolation points $t_0, \ldots t_{2k}$ is important for implementation. The discussion in \cite{Parrilo2004} suggests the following choice. Take the times $(t_i)_{i \in [2k]}$ to be the Chebyshev points of the first kind, i.e.
  #+NAME: eqn:cheby-first-kind
  \begin{equation}
  t_i = \cos((i+\frac12)\frac{\pi}{2k+1}) \; \text{for} \; i\in [2k],
  \end{equation}

  and the basis $(p_j(t))_{j \in [k]}$ to be as follow: $p_0 = \sqrt{\frac{1}{2k+1}}T_0$ and $p_j = \sqrt{\frac{2}{2k+1}}T_j$ for $j=1,\ldots,k$. The polynomials $T_j$ are the Chebyshev polynomial defined by the recursive relationship
  $$T_0(t) = 1, T_1(t) = t, T_i(t) = 2tT_{i-1}(t) - T_{i-2}(t)  \; \text{for} \; i=2,3 \ldots$$

  This particular choice is interesting because it makes the columns of the matrix $A^{(l)}$ orthonormal, which allows for better numerical stability. See \cite{Parrilo2004} for the proof and Section [[sec:numeric]] for an example.

  
* Time-varying SDPs 
  <<sec:timevaryingsdp>>
  We seek a characterization for optimality of polynomial solutions to a semidefinite program similar to the one we found for linear programs. It turns out again that strict feasibility is enough for that. The definition is as follow:

  #+NAME: def:strict_feasibility_sdp
     #+BEGIN_definition
A [[eqn:time_varying_sdp_l2]] with data $((A_i)_{i \in [m]}, b, c)$ is strictly feasible if there exists a (not necessarily continuous) function $X^{s}: [-1, 1] \rightarrow \mathbb R^{n \times n}$ and a positive scalar $\varepsilon$ such that for all $t \in [-1, 1]$, $X^{s}(t) \succeq \varepsilon I$ and $\langle A_i(t), X^s(t) \rangle \le b_i(t) - \varepsilon$ for $i = 1, \ldots, m$.

In this case we say that $X^s(t)$ is $\varepsilon$ -strictly feasible for our [[eqn:time_varying_sdp_l2]].
#+END_definition

The proof technique relies on the fact that spectrahedrons, the feasible sets of semidefinite programs, can be approximated within arbitrary accuracy by polyhedrons, and we generalize this result to the time-varying-case when the strict feasibility condition is verified.
     
  We also provide an efficient algorithm to find the best polynomial solution relying once again on the sum of squares techniques.

** Approximating a spectrahedron by a polyhedron

   
   Let $N(\varepsilon)$ be an $\varepsilon$ -covering of the compact set $\{X \succeq 0, ||X|| = 1\}$, i.e. a finite subset of it that is whithin a distance of at most $\varepsilon$ of all its elements. Then for any $X \in \mathcal S_n^+$, we can find an element $Y$ of the finite set $N(\varepsilon)$ such that $||X - Y|| \le \varepsilon ||X||$. The idea now is to inner approximate the feasible set of a [[eqn:time_varying_sdp_l2]]:
   $$S^+(t) = \{ X \;| \; X \succeq  0, \; \langle A_i(t), X \rangle \le b_i(t), \; i=1,\ldots, m\}$$
by the polyhedron:
$$P(t) = \{ X \; | \; \alpha \in (\mathbb R^+)^{N(\varepsilon)},   X = \sum_{Y \in N(\varepsilon)} \alpha_Y Y, \; \langle A_i(t), X \rangle \le b_i(t), \; i = 1,\ldots, m\}$$
\noindent where we replaced the psd condition $X \succeq 0$ by the stronger condition of $X$ being a sum of elements of the $\varepsilon$ -covering with positive coefficients.

#+NAME: thm:strict_feasibility_implies_polynomial_optimality_sdp
  #+BEGIN_thm
  If a [[eqn:time_varying_sdp_l2]] is strictly feasible, i.e. if there exists a function $X(t)$ and $\delta > 0$ such that $X(t) \succeq \delta I$ and $\langle X(t), A_i(t) \rangle  \ge b_i(t) - \delta$ for all $t \in [-1, 1]$ and $i\in[m]$, then for every positive scalar $\varepsilon$, there exists a /polynomial/ function $p: [-1, 1] \rightarrow \mathbb R^{n \times n}$ that is $\varepsilon\text{-near}$ optimal.
  #+END_thm


To prove the theorem, let's fix a [[eqn:time_varying_sdp_l2]] and assume it is strictly feasible, and consider the following TV-LP:

  #+NAME: eqn:approx_lp_eps
  \begin{equation*}
  \tag{$APPROX-LP_{\varepsilon}$}
  \begin{array}{ll@{}ll}
  \underset{\alpha(t) \in (\mathbb R^+)^{N(\varepsilon)}}{\text{maximize}} & \int_{-1}^1 \langle \sum_{Y \in N(\varepsilon)} \alpha_Y(t) Y, C(t) \rangle dt & \\
  \text{subject to}
  & \langle A_i(t), \sum_{Y \in N(\varepsilon)} \alpha_Y(t) Y \rangle \le b_i(t), & i=1,\ldots,m \; \forall t \in [-1, 1]
  \end{array}
  \end{equation*}

  We claim that the proof follow from this two lemmas:

  #+NAME: lem:approx_lp_converge_tv_sdp
  #+BEGIN_lemma
  As $\varepsilon \rightarrow 0$, the optimal value of [[eqn:approx_lp_eps]] converges to the optimal value of the [[eqn:time_varying_sdp_l2]]. 
  #+END_lemma
  
  #+NAME: lem:optimality_polynomial_approx_lp
  #+BEGIN_lemma
  Polynomial solutions are near optimal for [[eqn:approx_lp_eps]].
  #+END_lemma

  Before we present the proofs of this two lemmas, let us argue why they imply theorem [[thm:strict_feasibility_implies_polynomial_optimality_sdp]]. Denote by $\phi_{sdp}$ the optimal value of a fixed [[eqn:time_varying_sdp_l2]], and $\phi_{\varepsilon}$ the optimal value of the corresponding [[eqn:approx_lp_eps]], and let $\alpha$ be a positive scalar.

  For $\varepsilon$ small enough, the first lemma above gives that $|\phi_{\varepsilon} - \phi_{sdp}| \le \frac{\alpha}2$. The second lemma proves the existence of a polynomial feasible solution $\alpha(t)$ for which $|\phi_{\varepsilon} - \int_{-1}^1 \langle \sum_{Y \in N(\varepsilon)} \alpha_Y(t) Y, C(t) \rangle dt| \le \frac \alpha 2$.

  Now, it is not hard to see that $Z(t) \coloneqq \sum_{Y \in N(\varepsilon)} \alpha_Y(t) Y$ is also feasible for the [[eqn:time_varying_sdp_l2]], and furthermore, by triangular inequality, $|\phi_{sdp} - \int_{-1}^1 \langle Z(t), C(t) \rangle dt| \le \alpha$. Which concludes the proof of the theorem.

  We still need to prove the two lemmas. For Lemma [[lem:optimality_polynomial_approx_lp]] to hold, it is enough for us to construct a strictly feasible solution to the [[eqn:approx_lp_eps]], and then use theorem [[thm:strict_feasibility_implies_polynomial_optimality]] to conclude. To that effect, for $\delta > 0$, let $X^{s}(t)$ be a $\delta-$ strictly feasible solution to the [[eqn:time_varying_sdp_l2]]. For $t \in [-1, 1]$ and $Y \in N(\varepsilon)$, define $\alpha_Y(t)$ as follow:

  \[\alpha_Y(t) \coloneqq \left\{\begin{array}{ll}||X^{s}(t)|| & \text{if $Y$ is the closest point to $\frac{X^s(t)}{||X^{s}(t)||}$ in the epsilon cover $N(\varepsilon)$.}\\\frac{\varepsilon}{|N(\varepsilon)|} & \text{otherwise.}\end{array}\right.\]

  The vector $Z(t) = \sum_{Y \in N(\varepsilon)} \alpha_Y(t) Y$ is guaranteed to be within a distance $2M \varepsilon$[fn::$M$ is the uniform bound on the norm of feasible solutions to the [[eqn:time_varying_sdp_l2]]] of $X^{s}(t)$ by property of the $\varepsilon$ covering and triangular inequality. Now we claim that $\alpha(t)$ is indeed a $\delta/2-$ strict feasible solution to [[eqn:approx_lp_eps]] whenever $\varepsilon \le \frac{\delta} {4(M+1) \underset{i\in[m], t \in [-1, 1]}{\max}||A_i(t)||}$. Indeed, for $t \in [-1, 1]$ and $i \in [m]$,

  $$\alpha_Y(t) \ge \min(||X^{s}(t)||, \frac{\varepsilon}{|N(\varepsilon)|} \ge \delta/2,$$

  and
  \begin{align*}
  \langle A_i(t), Z(t) \rangle
  &\le \langle A_i(t), X_s(t) \rangle + |\langle A_i(t), X_s(t)-Z(t) \rangle|
  \\&\le \langle A_i(t), X_s(t) \rangle + \max_{i\in[m], t \in [-1, 1]}||A_i(t)|| ||X_s(t)-Z(t)||
  \\&\le b_i(t) - (\delta - 2M \varepsilon \max_{i\in[m], t \in [-1, 1]}||A_i(t)||) \textbf{1}
  \\&\le b_i(t) - \frac{\delta}2 \textbf{1}
  \end{align*}

  
We now prove Lemma [[lem:approx_lp_converge_tv_sdp]]. We start with an optimal solution to  $X^*(t)$ of the [[eqn:time_varying_sdp_l2]], and we approximate it by a function $Z(t)$ feasible for the corresponding [[eqn:approx_lp_eps]] using the exact same construction as the previous paragraph so that $||Z(t) - X^*(t)||$ is uniformly bounded in $t$ by quantity going to 0 of $\varepsilon$ goes to 0, thus the same applies the difference of the objective function of $Z(t)$ and $X^*(t)$ by Cauchy-Schwarz.



#+BEGIN_COMMENT
#+NAME: rem:two_varibles
  #+BEGIN_remark
The proof of Theorem [[thm:strict_feasibility_implies_polynomial_optimality_sdp]] could be generalized in the following sense. We consider (TV-SDP-2) to be the following optimization problem with two variables $X^1(t), X^2(t)$ instead of only one variable:

\begin{equation*}
  \tag{TV-SDP-2}
  \begin{array}{ll@{}ll}
  \underset{X_1(t), X_2(t)}{\text{maximize}} & \int_{-1}^1 \langle X^1(t), C^1(t) \rangle + \langle X^2(t), C^2(t) \rangle dt & \\
  \text{subject to}& X^1(t), X^2(t) \succeq 0 & \forall t \in [-1, 1]\\
  & \langle A^1_i(t), X^1(t) \rangle  + \langle A^2_i(t), X^2(t) \rangle\le b_i(t) & \forall t \in [-1, 1]
  \end{array}
  \end{equation*}
\noindent with $A^j_i(t) , X^j(t), C^j(t) \in \mathcal S_n, b_i(t) \in \mathbb R$ for all $t \in [-1, 1], i=1, \ldots, m, j=1,2$.


We say that (TV-SDP-2) is /strictly feasible/ 
if there exist two (not necessarily continuous) functions $X^1, X^2: [-1, 1] \rightarrow \mathbb R^{n \times n}$ and a positive scalar $\varepsilon$ such that for all $t \in [-1, 1]$, $X^1(t) \succeq \varepsilon I, X^2(t) \succeq \varepsilon I$ and $\langle A^1_i(t), X^1(t) \rangle  + \langle A^2_i(t), X^2(t) \rangle\le b_i(t) - \varepsilon$ for $i = 1, \ldots, m$.

We claim that if (TV-SDP-2) is strictly feasible, then polynomial solutions are near optimal.

The proof could be obtained by straightforwardly adapting the proof of Theorem [[thm:strict_feasibility_implies_polynomial_optimality_sdp]].

We use this formulation in the application of Section [[sec:time-varying-certificate-of-stability]].
#+END_remark
#+END_COMMENT


** Reformulation of TV-SDPs as simple SDPs
   <<sec:sdpt_is_sdp>>
   
  Like we did for [[eqn:time_varying_lp_l2]]s,we restate a [[eqn:time_varying_sdp_l2]] in terms of non-varying SDP. The following two theorems turn the search for positive semidefinite polynomial matrices on $\mathbb R$ (Proposition [[prop:positivestellnaz_sdp]]) or $[-1, 1]$ (Proposition [[prop:positivestellnaz_sdp_finite]]) into a search for sum of squares polynomials, which itself can be cast as an SDP.
  
#+NAME: prop:positivestellnaz_sdp
 #+BEGIN_prop
  [See Theorem 2.5 in \cite{DetteStudden}]
  
  For $d \in \mathbb N$, and a symmetric polynomial matrix $X(t) \in \mathbb R_{2d+1}^{n \times n}[t]$, the following statements are equivalent: (i) $X(t)  \succeq 0 , \; \forall t \in \mathbb R$ (ii) $X(t) \in SOSM_{d}$, (iii) The scalar polynomial $y^TX(t)y$ is a sum of squares in $\mathbb R[t, y]$.
  #+END_prop

  #+NAME: prop:positivestellnaz_sdp_finite
   #+BEGIN_prop
  [See Theorem 2.5 in \cite{DetteStudden}]
  
  For $d \in \mathbb N$, and a symmetric polynomial matrix $X(t) \in \mathbb R_{2d+1}^{n \times n}[t]$, $X(t)  \succeq 0 , \; \forall t \in [-1, 1]$ if and only if
  $$X(t) \in (1+t) SOSM_d + (1-t) SOSM_d, \quad \text{if $n$ odd},$$
  or
  $$X(t) \in (1+t)(1-t) SOSM_d + SOSM_d, \quad \text{if $n$ even}.$$

  Define the symmetric polynomial matrices that are positive semidefinite on $[-1, 1]$ by $ISOSM$.
  #+END_prop


#+NAME: thm:tvsdp_as_sdp
   #+BEGIN_thm
For $d \in \mathbb N$, the following SDP finds the best polynomial solution of degree $d$ to a [[eqn:time_varying_sdp_l2]] with data $((A_i)_{i\in[m]}, b, C)$.

  \begin{equation*}
  \begin{array}{ll@{}ll}
  \underset{X(t) \in \mathbb R[t]^{d \times d}}{\text{maximize}} & \int_{-1}^1 \langle X(t), C(t) \rangle dt & \\
  \text{subject to}\\& X(t) &\in ISOSM\\
  &  b_i(t) - \langle A_i(t), X(t) \rangle &\in  ISOS& \forall i \in [m]
  \end{array}
  \end{equation*}
  
   #+END_thm

A multivariate polynomial $\sigma(y)$ is in $SOS_d$ if and only if there exists a symmetric matrix Q such that $\sigma(y) = z^TQz, Q \succeq 0$, where $z$ is the vector of monomials in $y$ of degree up to $d$. This proves that the optimization problem formulated in the previous theorem is indeed an SDP. we call $Q$ the /Gram/ matrix of the polynomial $\sigma(y)$.


** COMMENT Application: time-varying certificate of stability
   <<sec:time-varying-certificate-of-stability>>
   As an application of the characterization in this paper, we consider the problem of certifying the stability of a linear system.
   More concretely , we want to certify that the following system is stable:

   #+NAME: eq:system
   \begin{equation}
   \tag{SYSTEM}
   \frac{d f(u)}{du} = A(t) f(u)
   \end{equation}

   

\noindent where $A(t) \in \mathbb R^{n \times n}$ is varying polynomially in time $t \in [-1, 1]$, and $f(u)$ is a column vector of univariate functions in $u$.

   We can prove that the system is stable if and only if the matrix $A(t)$ is Herwitz for all $t \in [-1, 1]$, which is equivalent to the existence of an matrix $P_t \succeq I$, called certificate of stability, such that $- P_t^TA(t) - A(t)^T P_t \succeq 0$ for all $t \in [-1, 1]$.

   Following the framework presented in this paper, we can look efficiently for a certificate $P(t)$ that depends polynomially on $t$. We are going to prove that whenever a certificate of stability exists and is bounded in time $t$, then a /polynomial/ certificate of stability also exists.
   
   Now fix $t \in [-1, 1]$, and consider the following sets
   $$S_1(t) = \{P \in \mathcal S_n^+ |\;  P \succeq I, - P^TA(t) - A(t)^T P \succeq I\}$$
   $$S_2(t) = \{(P, Q) \in \mathcal (S_n^+)^2 \; | \;  P \succeq I, Q \succeq 2I, ||Q + P^TA + A^T P||_1 \le  1 \}$$
   $$S_3(t) = \{(P, Q) \in \mathcal (S_n^+)2 \; | \;  P \succeq 2I, Q \succeq 3I, ||Q + P^TA + A^T P||_1 \le \frac12 \}.$$

    Using this new notation, [[eq:system]] is stable if and only if $S_1(t)$ is not empty for all $t \in [-1, 1]$. We claim that in fact, for all $t\in[-1, 1]$, $S_1(t) \ne \emptyset$ is equivalent to $S_2(t) \ne \emptyset$ and to $S_3(t) \ne \emptyset$.

    We will prove this in three steps: (i) $S_1(t) \ne \emptyset \implies S_3(t) \ne \emptyset$, (ii) $S_3(t) \ne \emptyset \implies S_2(t) \ne \emptyset$, (iii) $S_2(t) \ne \emptyset \implies S_1(t) \ne \emptyset$.
    
   Indeed, one can see that $S_1(t) \ne \emptyset \implies S_3(t) \ne \emptyset$ because if $P \in S_1(t)$, then $(3P, -3P^TA-3A^TP) \in S_2(t)$. The implication $S_3(t) \ne \emptyset \implies S_2(t) \ne \emptyset$ is trivial. For the remaining implication, $S_2(t) \ne \emptyset \implies S_1(t) \ne \emptyset$, let $(P, Q) \in S_2(t)$, i.e. $P \succeq I, Q \succeq I$ and $||Q + P^TA + A^T P||_1 \le 1$, and let's prove that  $-P^TA(t) - A(t)^T P \succeq I$. Let $u$ be a vector of $\mathbb R^n$ of norm 1, then
   $$u^T(-P^TA(t) - A(t)^T P - I)u \ge u^T(-Q - P^TA(t) - A(t)^T P)u  + u^T(Q-I)u \ge -1 + 1 \ge 0,$$
\noindent which  proves that $P \in S_1(t)$. Which finishes the proof of the claim.

   As a consequence of the claim, a certificate of stability of [[eq:system]] exists if and only if $S_2(t) \ne \emptyset$ for all $t \in [-1, 1]$. We write the latter condition as a TV-SDP feasiblity problem
   $$\{(P(t), Q(t)) \in \mathcal (S_n^+)^2 | P(t) \succeq I, Q(t) \succeq 2I, ||Q(t) + P(t)^TA(t) + A(T)^T P(t)||_1 \le 1\}.$$
   
   This program is feasible if and only if it is $\frac12-$ strictly feasible. (Because $S_2(t)$ and $S_3(t)$ are either both empty or both non empty). If we further assume that a certificate of stability that is bounded in time $t \in [-1, 1]$ exists, then  all conditions for theorem [[thm:strict_feasibility_implies_polynomial_optimality_sdp]] (and Remark [[rem:two_varibles]]) are verified, and therefore a /polynomial/ (and afortiori continuous) certificate $P(t)$ of stability of [[eq:system]]  exists.
   


* Numerical results
   <<sec:numeric>>
   
  We present three numerical examples to illustrate the techniques presented in this paper. The first one is the Leontief input-output production model (see \cite{Leontief55}), where the dependence between sectors vary in time. The second one is time-varying max-flow problem, where the graph is fixed but the capacities are varying with time, and we seek a the best polynomial flow. The last example is the problem of minimizing the transmission power while guaranteeing the wireless coverage of a region of space moving in time.
  

** Leontief model
   <<sec:leontief>>
   
   \begin{table}[!htbp]
   \caption {\label{tbl:leontief} Time-varying Data for \ref{eq:leontief} }
   \begin{tabular}{|l|c|c|c|c|c|c|}\hline
   &Agriculture&Transportation& Petroleum & Demand & Cost & Maximum limit\\
   \hline
   Agriculture&$t^2/3$&$(1+t)/10$&$1/10$&$1$&$1+t$&3 \\
   \hline
   Transportation&$1/10$&$(1+t^5)/5$&$0$&$1$&$(t+1/10)^2$&3\\
   \hline
   Petroleum&$1/10$&$1/10$&$0$&$1$&$1+t$&3\\
   \hline
   \end{tabular}
   \end{table}

   
   #+NAME: fig:leontief-solution
   #+ATTR_LATEX: :width 0.5\textwidth
   #+caption: \ref{eq:leontief} best polynomial solution of degree 9. The corresponding objective value is 6.
   file:includes/leontief.png

   In this first example, we consider an economy divided into $n$ sectors $1, \ldots, n$.  For each sector $i=1,\ldots,n$, there is a demand for $b_i$ (fractional) units of type $i$, and producing an extra unit of type $i$ requires $a_{ij}$ units from sector $j$, for $j\in \{1, \ldots n\}$, and costs an amount $c_i$. We must decide how many units of sector $i$, $x_i$, to produce. We need to find the quantities $x_i$ so that the total cost $\sum_{i=1}^n x_i c_i$ is minimized while the demand is met, i.e. $x_i \ge \sum_{j=1}^n a_{ij} x_j + d_i$, without exceeding a maximum limit $M$, i.e. $x_i \le M$.
   
   We allow the production vector $x \coloneqq (x_i)_{i\in[n]}$, the technology matrix $A \coloneqq (a_{ij})_{i,j\in[n]}$, the demand vector $b \coloneqq (b_i)_{i\in[n]}$ and the cost vector $c \coloneqq (c_i)_{i \in [n]}$ to vary with time $t \in [-1, 1]$.
   
   We can formulate the problem above as the following TV-LP:


   \begin{equation*}
   \label{eq:leontief}
   \tag{LEONTIEF}
   \begin{array}{ll@{}ll}
   \underset{x(t) \in \mathbb R^n}{\text{minimize}} & \int_{-1}^1 \langle c(t), x(t) \rangle \\
   \text{subject to}&&\\
   &x(t) \ge A(t)x(t) + b(t) \quad \forall t \in [-1, 1]\\
   &M \ge x(t) \ge 0 \quad \forall t \in [-1, 1]\\
   \end{array}
   \end{equation*}

   As a numerical example, we consider an economy with $n=3$ sectors, whose technology matrix, demand vector and cost vector are detailed in Table \ref{tbl:leontief}.


   Figure [[fig:leontief-solution]] shows the best polynomial solution of degree $9$, $x(t)$, to \ref{eq:leontief}.
   
   We check that this solution is strictly feasible, which proves that polynomial solutions to \ref{eq:leontief} are near optimal by means of Theorem  [[thm:strict_feasibility_implies_polynomial_optimality]].
   
   
   
** Max-Flow
<<sec:maxflow>>

  #+INCLUDE: includes/maxflowgraph.tikz
  

  Consider a graph with set of nodes $V \coloneqq \{1, \ldots n\}$, where 1 is the source, and $n$ is the target, and set of edges  $E \subseteq [n]^2$. For an edge $(i, j) \in E$ and $t \in [-1, 1]$, $b_{i,j}(t)$ is the capacity of the edge at time $t$ and $f_{i,j}(t)$ is the flow on the same node. We can thus formulate the problem of finding the flow with maximum average value in time as the following:

  #+NAME: eqn:maxflow
  \begin{equation*}
  \tag{MAXFLOW}
  \begin{array}{ll@{}ll}
  \underset{(f_{ij}(t))_{(i, j) \in E}}{\text{maximize}} & \int_{-1}^1  \sum_{(1,j) \in E} f_{1,j}(t) dt & \\
  \text{subject to}& \sum_{(1, j) \in E} f_{i, j}(t) - f_{j, i}(t) = 0& \; &\forall t \in [-1, 1]\\
  & 0 \le f_{i,j}(t) \le b_{ij}(t), & \forall (i, j) \in E, \; &\forall t \in [-1, 1] \\
  \end{array}
  \end{equation*}
     

  Using the results from Section [[sec:lp_is_sdp]], we parameterize the polynomials $f_{ij}(t)$ and $b_{ij}(t)$ by the values they take at the times $(t_l)_{l \in [d]}$ defined by Equation [[eqn:cheby-first-kind]]. Formally, we identify the polynomial $f_{i,j}(t)$ (resp. $b_{i,j}(t)$)  with the $(d+1) \times 1$ vector $\begin{pmatrix}f_{i,j}(t_0)\\\vdots\\f_{i,j}(t_d)\end{pmatrix} \coloneqq \begin{pmatrix}f_{i,j,0}\\\vdots\\f_{i,j,d}\end{pmatrix}$ (resp. $\begin{pmatrix}b_{i,j}(t_0)\\\vdots\\b_{i,j}(t_d)\end{pmatrix} \coloneqq \begin{pmatrix}b_{i,j,0}\\\vdots\\b_{i,j,d}\end{pmatrix}$)

The objective value $\int_{-1}^1  \sum_{j=1}^n f_{1,j}(t) dt$ is linear in the $f_{1,j,l}$, i.e. there exist weights $(w_l)_{l \in [d]}$ such that $\int_{-1}^1  \sum_{(1,j) \in E} f_{1,j}(t) dt = \sum_{l=0}^d \sum_{(1,j) \in E}  f_{1,j,l} w_l$. The weights $w_l$ can be found by solving a linear system.

Given the observations above, considering only polynomial solutions up to some degree $d \coloneqq 2k+1$, where $k \in \mathbb N$, Theorem [[thm:tvlp-to-sdp]] guides us to cast the problem of finding the best polynomial solution to  [[eqn:maxflow]] as the following SDP:

#+NAME: eqn:maxflow-sdp
\begin{equation*}
\tag{MAXFLOW-SDP}
\begin{array}{ll@{}ll}
\underset{f_{i,j} \in \mathbb R,  X_{ij}, X'_{ij}, Z_{ij}, Z'_{ij} \in S_k^+ \; (i, j) \in E}{\text{maximize}}
& \sum_{(1,j) \in E} \sum_{l=0}^d f_{1,j,l} w_l \\
\text{subject to}
& \sum_{j=1}^N f_{i,j,l} - f_{j,i,l} = 0  & \forall l, \forall i \ne s, t \\
& f_{i,j,l}                          = \langle A^{(l)}, (1-t_l) X_{ij} + (1+t_l) X'_{ij} \rangle  &\forall (i,j) \in E, \forall l \in [d]\\
& b_{i,j,l} - f_{i,j,l}              = \langle A^{(l)}, (1-t_l) Z_{ij} + (1+t_l) Z'_{ij} \rangle\ &\forall (i,j) \in E, \forall l \in [d] \\
\end{array}
\end{equation*}

#+BEGIN_remark
Notice that in example, the constraint matrix $A(t)$ is fixed in time, and the null flow (i.e. $f_{ij}(t) = 0 \; \forall t \in [-1, 1], \forall (i, j) \in E$) is a continuous feasible solution, Theorem [[thm:A_constant]] states that continuous solutions are near optimal for [[eqn:maxflow-sdp]].
#+END_remark

As a numerical example, we consider the graph on Figure \ref{fig:maxflow-graph} with capacities plotted with dotted lines on each edge. We solve [[eqn:maxflow-sdp]] with $k=4$ (which makes $d = 9$). The optimal polynomial solution $(f_{ij}(t))_{(i,j) \in E}$ is plotted on the same figure with full lines. The corresponding objective value is $85.42$


  # #+NAME: fig:graphcap-maxflow
  # #+ATTR_LATEX: :width 0.8\textwidth
  # #+caption: Entry $(i, j)$ is a plot in time of the capacity $b_{ij}(t)$ (dotted-lines) and the optimal polynomial flow of degree $9$, $f_{ij}(t)$, (in full lines).
  # file:includes/graphmatrix.png



In order to know if this can be improved substantially by allowing a higher upper bound on the degree of the polynomial solution, we consider the corresponding time-varying min-cut problem, whose objective value provides an upper bound on the objective value of [[eqn:maxflow]]:

  #+NAME: eqn:mincut
  \begin{equation*}
  \tag{MINCUT}
  \begin{array}{ll@{}ll}
  \underset{(d_{ij}(t))_{(i,j) \in E}, (p_i(t))_{i \in V}}{\text{minimize}} & \int_{-1}^1  \sum_{(i,j) \in E} b_{ij}(t) d_{ij}(t) dt & \\
  \text{subject to}& d_{ij}(t) - p_i(t) + p_j(t)  \geq 0 & \forall (i, j) \in E & \; \forall t \in [-1, 1]\\
  &p_1(t) - p_n(t) \geq 1 && \; \forall t \in [-1, 1] \\
  &p_i(t) \geq 0 & \forall i \in V & \; \forall t \in [-1, 1]\\
  & d_{ij}(t)  \geq 0 & \forall (i, j) \in E & \; \forall t \in [-1, 1]
  \end{array}
  \end{equation*}
  
  We turn this optimization problem into an SDP using the same techniques that we use for [[eqn:maxflow]]. Allowing polynomial solutions of degree up to 9, we get an upper bound of $85.52$ on the optimal value of [[eqn:maxflow]].

  Notice that the solution given by $p_1(t) = 1+\varepsilon$, $p_i(t) = \varepsilon$, $d_{jk}(t) = \varepsilon + |p_j(t) - p_k(t)|$ for $i \in V \setminus \{1\}, (j, k) \in E$  and $t \in [-1, 1]$ is $\varepsilon-$ strictly feasible to [[eqn:mincut]] for any $\varepsilon > 0$. Furthermore, since [[eqn:mincut]] is an (exact) LP relaxation for an integer program, there always exists an optimal solution that takes only values $0$ and $1$, and therefore we can amend the constraints $p_i(t) \le 2, d_{jk}(t) \le 2, \quad \forall i \in V, (j, k) \in E, t \in [-1, 1]$ without changing the optimal value. As a consequence, we can consider the feasible set of [[eqn:mincut]] to be uniformly bounded in time. All assumptions for Theorem [[thm:strict_feasibility_implies_polynomial_optimality]] are then met, we know therefore that if we increase the degree of the polynomial solutions to [[eqn:mincut]] enough, we can get a feasible solution with an objective value arbitrarily close to the optimal one.
  

** Wireless Coverage problem
   <<sec:wireless>>
   
   In this problem our goal is to cover an moving area while minimizing the power needed. We have two wireless electromagnetic transmitters located at positions $\bar T_1 = (\bar x_1, \bar y_1)$ and $\bar T_2 = (\bar x_2, \bar y_2)$. Each transmitter $i = 1, 2$ is an omnidirectional power source emitting energy $E_i(t, x, y)$ at time $t$ in the location $(x, y)$ of space. Electromagnetics laws give the following expression for $E_i$, $i=1,2$:
   
   $$E_i(x, y, t)= \frac{c_i(t)}{(x - \bar x_i)^2 + (y - \bar y_i)^2}$$
   
\noindent where $c_i(t)$ is the transmission power of the transmitter $i$ at time $t$, also equal to the power needed to run the transmitter.

For $j=1, 2$, we define the regions  $\mathcal B_j$ by $k_j$ polynomial inequalities, where $k_j \in \mathbb N$:
$$\mathcal B_j = \{ (x, y, t) \in \mathbb R^2 \times \mathbb R^2 \times [-1, 1] |\quad  g_{j, 1}(x, y, t) \ge 0, \ldots, g_{j, k_j}(x, y, t) \ge 0\}.$$

Our goal is to make the total energy $E(x, y, t) = E_1(x, y, t) + E_2(x, y, t)$ at time $t$ greater than some fixed positive threshold $C$ for each $(x, y, t) \in \mathcal B_1 \cup \mathcal B_2$ while minimizing the total cost $\int_{-1}^1 c_1(t) + c_2(t) dt$.

To fix ideas, we take the following numerical example. We fix the required coverage level $C$ to 1 without loss of generality, and we let the positions of the transmitters to be $\bar T_1 = (0, 0)$ and $\bar T_2 = (5, 5)$ and the maximum allowed power $\gamma_1 = \gamma_2 = 50$. The regions $\mathcal B_1$ and $\mathcal B_2$ are circles of radius $1$ and centers $z_1(t)$ and $z_2(t)$ moving polynomially in time, i.e.
$$\mathcal B_j = \{(x, y, t), \quad ||\begin{pmatrix}x\\y\end{pmatrix} - z_j(t)|| \le 1\}, j=1,2.$$
\noindent where the centers $z_1(t), z_2(t)$ are defined as $z_1(t) \coloneqq  \begin{pmatrix}4t+1\\4t\end{pmatrix}$, $z_2 \coloneqq \begin{pmatrix}-4t\\-4t\end{pmatrix}$.

We can formulate the problem as a time-varying optimization problem in the variables $(c_1, c_2)$:


  \begin{equation*}
  \begin{array}{ll@{}ll}
  \underset{(c_1, c_2) \in \mathbb R[t]}{\text{minimize}} && \int_{-1}^1 (c_1(t) + c_2(t)) dt \\
  \text{subject to}&&&\\
  &c_1(t) &\le \gamma_i & i =1, 2 \quad \forall t \in [-1, 1]\\
  &E(x, y, t) &= \sum_{i=1}^2 \frac{c_i(t)}{(x - \bar x_i)^2 + (y - \bar y_i)^2} \ge C & \forall (x, y, t) \in \mathcal B_j
  \end{array}
  \end{equation*}


  Notice that the last inequality can be formulated equivalenty as a polynomial inequality:
    
    $$p(x, y, t) \coloneqq -C \prod_{i=1}^2 [(x - \bar x_i)^2 + (y - \bar y_i)^2] + \sum_{i=1}^2  [(x - \bar x_i)^2 + (y - \bar y_i)^2] c_i(t) \ge 0 \quad \forall (x, y, t)\in \mathcal B_1 \cup \mathcal B_2.$$


    Since we are dealing with bivariate polynomials that are non negative on bounded regions of space, Putinar's Positivstellensatz result states that we can rewrite the last inequality as
    $$p = \sigma^{(j)}(t) + \mu^{(j)}(t) (1 - (x - \bar x_j)^2 - (y - \bar y_j)^2 ) \quad j=1,2$$
    \noindent where $\sigma^{(j)}(t)$ and $\mu^{(j)}(t)$ are sum of squares of polynomials in $\mathbb R[x, y]$ for all $t \in [-1, 1]$.

    Denote by $P^{(j)}(t)$ and $Q^{(j)}(t)$ the Gram matrices of $\sigma^{(j)}(t)$ and  $\mu_1^{(j)}(t)$ respectively. (See Section [[sec:sdpt_is_sdp]] for the definition of Gram matrices.)

    If we allow only polynomials up to some degree $d_1$, then $P^{(j)}(t)$ and $Q^{(j)}(t)$ have fixed sizes, and this optimization problem becomes a time-varying SDP, where the variables are $c_1, c_2, P^{(j)}, Q^{(j)}$ all assumed to vary with time $t$:
    \begin{equation*}
    \begin{array}{lllll}
    &\underset{c_j(t), P^{(j)}(t), Q^{(j)}(t), j=1,2}{\text{minimize}}  \int_{-1}^1 (c_1(t) + c_2(t)) dt \\
    \quad&\text{subject to}&&&\\
    &c_i(t) \le \gamma_i & i =1, 2 \quad \forall t \in [-1, 1]\\
    &p(x, y, t) = \sigma^{(j)}(t)(x, y) + \mu^{(j)}(t)(x, y) (1 - (x - \bar x_1)^2 - (y - \bar y_1)^2 ) & j=1,2, \forall t \in [-1, 1], \forall (x, y) \in \mathbb R^2 \\
    &P^{(j)}(t), Q^{(j)}(t) \succeq 0 & \forall t \in [-1, 1]
    \end{array}
    \end{equation*}

    In the following, we allow this matrices $P^{(j)}$ and $Q^{(j)}$ for $j=1,2$ to depend polynomially on the variable $t$, and we fix the degree in $t$ to be no more than some integer $d$. Using Proposition [[prop:positivestellnaz_sdp]], for $j=1,2$, the condition $P{(j)}(t), Q{(j)}(t) \succeq 0 \; \forall t \in [-1, 1]$ can be written equivalently as $P^{(j)}(t), Q^{(j)}(t) \in (1-t)MSOS_{d_1} + (1+t) MSOS_{d_1}$ if $d$ is odd, and  $P^{(j)}(t), Q^{(j)}(t) \in (1+t)(1-t)MSOS_{d_1} + MSOS_{d_1}$ otherwise.

    Table \ref{tbl:wireless} reports the optimal values found for $d=0, 1, 2, 7$, and $d_1=7$.
    
    A version of this problem where the transmission powers $c_1$ and $c_2$ do not depend on $t$ appeared in \cite{ahmadi2016}, which corresponds to $d=0$.
    
    \begin{table}
    \caption {\label{tbl:wireless} Optimal polynomial solutions $(c_1, c_2)$ to the time-varying wireless coverage problem.}
    \begin{tabular}{|l|l|l|l|}\hline
    $d$ & $c1(t)$ & $c2(t)$ & $\int_{-1}^1 (c_1(t) + c_2(t)) dt$\\\hline
    0 & 26.46 & 26.46 & 105.83\\ \hline
    1 & $23.18+14.96 t$ & $28.9-25.82 t$ & 104.15\\ \hline
    2 & $31.92-2.12 t-24.13 t^2$ & $21.8-1.37 t-16.57 t^2$ & 80.31\\ \hline
    7 & $31.12-5.64 t-28.31 t^2+4.95 t^3$ & $23.54-0.54 t-28.12 t^2-8.06 t^3$& 77.40\\
    \; &$+0.14 t^4-1.48 t^5+4.89 t^6$ & $+14.10 t^4+7.83 t^5-4.89 t^6$  &\\
    \hline
    \end{tabular}
    \end{table}

\begin{figure}[htp]
\centering
\includegraphics[width=.3\textwidth]{includes/wireless-0.png}\quad
\includegraphics[width=.3\textwidth]{includes/wireless-2.png}
\includegraphics[width=.3\textwidth]{includes/wireless-5.png}
\caption{\label{img:wireless} Three snapshots of the solution to a time-varying wireless coverage problem at times $t=-1, 0, 1$. In red, the two circular regions $\mathcal B_1, \mathcal B_2$ that need to be covered at all times $t \in [-1, 1]$. The background color at a point in the plane indicates the level of energy at that point (blue for low, yellow for high).}
\end{figure}


* Conclusion and open questions   

This paper presented a natural method to optimize over polynomial solutions to time-varying convex program using the sum of squares framework. We note that even though there exist polynomial algorithms for sum of squares optimization, the best known algorithms scale very poorly as the number of variables the polynomials depend on grow. One notable exception is certifying non negativity of univariate polynomials, which can be done efficiently by expressing them in an appropriate basis. We exploit this fact in the case of time-varying linear programs.

The paper also provided sufficient conditions under which polynomial solutions are optimal. It is worth mentioning that the main characterization given here might be asking for too much in certain cases, since it does not cover the case of /equality constraints/.


\bibliographystyle{plain}
\bibliography{citations}



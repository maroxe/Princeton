\documentclass[12pt]{article}

% packages
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}

% custom commands
\newcommand{\Q}[1]{\subsubsection*{Question #1}}

\newcommand{\Es}[1]{\mathbb{E}(#1)}
\newcommand{\deriv}[2]{\frac{\partial #1}{\partial \theta_{#2}}}





% parameters
\geometry{hmargin=1cm,vmargin=1cm}
\title{ORF524 - Problem Set 3}
\author{Bachir EL KHADIR }

\begin{document}

\maketitle

\Q{1}
\begin{enumerate}

\item

\begin{align}
\mathcal L \lambda a + (1-\lambda)b 
&= ||\lambda (a-\theta) + (1-\lambda)(b-\theta) ||_p
\\&= ||\lambda (a-\theta)||_p + ||(1-\lambda)(b-\theta) ||_p &\text{By Minkowsky}
\\&= \lambda \mathcal L(a) + (1-\lambda) \mathcal(b)
\end{align}


\item 
for $q > 1$, let's denote $f_q: x \rightarrow x^q$ for $x > 0$.
$f_q$ is convexe and non decreasing because $f_q'(x) = q x^{q-1} > 0$ and $f_q''(x) = q(q-1)x^{q-2} > 0$.

\begin{align}
\mathcal L \lambda a + (1-\lambda)b 
&= f_q(||\lambda (a-\theta) + (1-\lambda)(b-\theta) ||_p)
\\&\leq f_q(\lambda || (a-\theta)|| + (1-\lambda)||(b-\theta) ||_p)&\text{because $f_q$ non decreasing}
\\&\leq \lambda f_q( || (a-\theta)||) + (1-\lambda)f_q(||(b-\theta) ||_p) & \text{because $f_q$ convexe}
\end{align}


\end{enumerate}


\Q{2}
\begin{enumerate}
\item
The $X_i$ have the same distribution and play symetric roles, so:
$$\tilde p = E [\hat p | T(X)] = E[ X_1 | \sum_i X_i] = \frac1 n E[\sum X_i | \sum_i X_i] = \frac {T(X)} n$$

\item $$E[ (\hat p - p)^2 ] = \operatorname{Var}(X_1) = p(1-p)$$


the $X_i$ being iid:
$$E[ (\tilde p - p)^2 ] = E[ (\sum \frac{T(X_i) - p}{n})^2 ] = \operatorname{Var}(\sum \frac{X_i - p}{n}) = \sum_i \frac{Var(X_i)}{n^2} = \frac{Var(X_1)}{n} = \frac{p(1-p)}{n}$$

$\hat p$ has better variance than $\tilde p$, because it uses the information from all the $X_i$.
\end{enumerate}


\Q{3}

\begin{itemize}

\item
$Var(E[X|Y]) = E[ (E[X|Y] - E[ E[X|Y]])^2] = E[ (E[X|Y] - E[X])^2]$

\begin{align*}
E[ Var(X|Y) ] &= E[ E[(X - E[X|Y])^2 |Y]] 
\\&= E[ E[(X - E[X] + E[X] - E[X|Y])^2 ] 
\\&= E[ E[(X - E[X])^2] ] - E[(E[X] - E[X|Y])^2 ] &\text{By Pythagor, because $E[X] - E[X|Y] \perp X - E[X]$}
\end{align*}

By summing:
$$E[ Var(X|Y) ] + Var(E[X|Y]) = E[ E[(X - E[X])^2] ] = Var(X)$$

\item Let $g(X)$ be an estimator of $\theta$ and $T(X)$ a sufficient statistics. The bias of $g(X)$ and $g(X)|T(X)$ are the same because of the law of iterated expectation. We can assume that the bias is 0 without loss of generality by substracting it from both variables.

Then

$E[ (E[g(X)|T(X)] - \theta)^2] = E[ \left(E[g(X) - \theta|T(X)]\right)^2] = E[Var(g(X)|T(X))] \leq Var(g(X))$
ie:

$$E[ (E[g(X)|T(X)] - \theta)^2] \leq E[ (g(X) - \theta)^2 ]$$

\end{itemize}

\Q{4}


Let's prove that $\phi(\{c_j\}^l$ is non-increasing at each step.

By assigning each $x_i$ to the nearest ${c'}_j^{l+1}$, each quantity $||x_i - c_j^l||^2$ in the sum above is replaced by a smaller (or equal) quantity $||x_i - {c'}_j^{l+1}||^2$

For every $j = 1..K$, $\sum_{x_i \in C_j^l} ||x_i - {c'}_j^{l+1}||^2 \leq \sum_{x_i \in C_j^l} ||x_i - c_j^{l+1}||^2$
because the mean of the point $x_i \in C_j^l$, minimizes the quantity $\mu \rightarrow \sum_{x_i \in C_j^l} ||x_i - \mu||^2$. ( By taking the derivative, the function being quadratic )

So: $\phi(\{c_j\}^l = \sum_j \sum_{x_i \in C_j^l} ||x_i - c_j^l||^2 \leq\sum_i \sum_{x_i \in C_j^l} ||x_i - c_j^{l+1}||^2$

Therefore $\phi(\{c_j\}^l$ is non negative non-increasing, and the limit exsits.



\Q{5}

\begin{enumerate}
\item 

\begin{align*}
Cov( (T, a)^T ) &= \mathbb{E} (T, a) (T, a)^T -  (\mathbb{E} (T, a))(\mathbb{E} (T, a))^T\\
\\ &= \mathbb{E} 
\begin{bmatrix}TT^T & T^T a \\ aT^T & aa^T \\\end{bmatrix} 
- \begin{bmatrix}\Es T \Es T^T & \Es T^T \Es a \\ \Es a \Es T^T & \Es a \Es a^T \\\end{bmatrix} 
\\ &= 
\begin{bmatrix}Cov(T) & Cov(T,a) \\ Cov(a,T) & Cov(a) \\\end{bmatrix} 
\\ &= 
\begin{bmatrix}Cov(T) & \nabla_\theta g(\theta) \\ \nabla_\theta g(\theta)^T & I(\theta) \\\end{bmatrix} 
\end{align*}

Because:
\begin{itemize}
\item $\Es{a} = \int \nabla_\theta \log f_\theta(x) f_\theta(x) \rm{d}x 
= \int \frac{\nabla_\theta f_\theta(x)}{f_\theta(x)}f_\theta(x) \rm{d}x = \nabla_\theta 1 = 0$
\item $Cov(a) = \Es{aa^T} = I(\theta)$

\item
\begin{align*}
Cov(T, a) &= \Es{T^T a} \\
&= \int T(x) \nabla_\theta \log f_{\theta}(x)  f_{\theta}(x)\rm{d}x \\
&= \int T(x) \frac{\nabla_\theta f_{\theta}(x)}{ f_{\theta}(x)} f_{\theta}(x)\rm{d}x \\
&= \nabla_\theta \int T(x) f_{\theta}(x)\rm{d}x & \text{(By regularity condition)} \\
&= \nabla_\theta g(\theta)
\end{align*}
\end{itemize}

\item
\[ B = \left(\begin{array}{cc}-I_p&,\nabla_\theta g(\theta)^T I(\theta)^{-1}\end{array}\right)^T \]

\begin{align}B^T Cov(T, a)^T B &= Cov(T) - \nabla_\theta g(\theta)^T I(\theta)^{-1}\nabla_\theta g(\theta) - \nabla_\theta g(\theta)^T I(\theta)^{-1}\nabla_\theta g(\theta) \\&+ (\nabla_\theta g(\theta)^T I(\theta)^{-1}) I(\theta) (I(\theta)^{-1} \nabla_\theta g(\theta) )\\
&= Cov(T) - \nabla_\theta g(\theta)^T I(\theta)^{-1} \nabla_\theta g(\theta)
\end{align}

\item $Cov(T) - \nabla_\theta g(\theta)I(\theta) \nabla_\theta g(\theta)  = B^T Cov(T, a)^T B = Cov( B(T, a)^T ) \geq 0$
\end{enumerate}

\Q{6}

In the following we write $f$ instead of $f_\theta(x)$ or $f_\theta(X)$.

$$\nabla_\theta^2 \log f = \nabla_\theta (\frac{\nabla_\theta f}{f}) = \frac{\nabla_\theta^2 f}{f} - \frac{\nabla_\theta f \nabla_\theta f^T}{f^2} = \frac{\nabla_\theta^2 f}{f} - \nabla_\theta \log f \nabla_\theta \log f^T$$


But $\Es {\frac{\nabla_\theta^2 f}{f}} = \int \frac{\nabla_\theta^2 f}{f} f \rm d x 
= \nabla_\theta ^2 \int f \rm d x = 0$, so
$$
I(\theta) = \Es{\nabla_\theta f_{\theta}(x) \nabla_\theta f_{\theta}(x)^T}  = - \Es{ \nabla_{\theta}^2 f}
$$

\Q{7}
\begin{enumerate}

\item
By the series expansion of exponential:
$$|\frac{e^{az} - 1}{z}| = |\sum_{k=1}^{\infty} \frac{a^kz^{k-1}}{k!}|
\leq \sum_{k=1}^{\infty} \frac{|a|^k|z|^{k-1}}{k!}
\leq \sum_{k=1}^{\infty} \frac{1}{\delta} \frac{|a|^k|\delta|^{k}}{k!}
=\frac{e^{a\delta}}{\delta} 
$$
\item
Let $\alpha_n \Rightarrow \alpha$

\begin{align}
\frac d {d\alpha} E g = \lim_{h\rightarrow0} \int g(x) h(x) \frac{l(\alpha+h) e^{(\alpha+h) T(x)} - l(\alpha) e^{\alpha T(x)}}{h} dx
\end{align}


$$\frac{g(x)f_{\alpha_n}(x) - g(x) f_\alpha(x)}{\alpha_n - \alpha} 
= g(x)h(x)
 $$ 

\end{enumerate}

\Q{8}
\begin{enumerate}


\item 

$$\hat \beta = (X^TX)^{-1} X^T y  = (X^TX)^{-1} X^T X \beta + (X^TX)^{-1} X^T \eta = \beta + (X^TX)^{-1} X^T \eta$$

$\Es {\hat \beta} = \beta + (X^TX)^{-1} X^T \Es{\eta} = \beta$. So $\hat \beta$ is unbiased.

best var
\item $R_2(\hat \beta) = Var(\hat \beta) = Var((X^TX)^{-1} X^T \eta) = (X^TX)^{-1} X^T Var(\eta) ((X^TX)^{-1} X^T)^T = \sigma^2 (X^TX)^{-1} X^T X (X^TX)^{-1} = \sigma^2 (X^TX)^{-1}$

If $X^TX = I_p$, $R_2(\hat \beta) = \sigma^2 I_p$
\end{enumerate}



\Q{9}
\begin{enumerate}
\item 

\begin{align*}
\frac{\rm d}{\rm d c} \Es {|X-c|} &= \frac{\rm d}{\rm d c} \int^c (c-x)f(x) \rm{dx} + \int_c (x-c)f(x) \rm{dx} 
\\&= \frac{\rm d}{\rm d c} c (F(c) - (1-F(c)) + \int^c -xf(x) \rm{dx} + \int_c x f(x) \rm{dx}
\\&= \frac{\rm d}{\rm d c} c (2F(c) - 1) - 2 \int^c xf(x) \rm{dx} + \int_{\mathbb{R}} x f(x) \rm{dx}
\\&= 2F(c)-1 + 2cf(c) - 2 cf(c) 
\\&= 2F(c)-1
\end{align*}

The derivative is increasing so the function is strictly convexe, and therefore it attains its minimum when the derivative is 0, or $F(c) = \frac12$, or $c = \text{median}(P_X)$ 

\item 


By Fubini-Tonelli
$$\bar {\mathcal L}_{\mathcal R} (\hat \theta) = E_X[ E_{\theta|X}[|\hat \theta(X) - \theta|] ]$$

By the previous question, taking  $\hat \theta(X) = \operatorname{median}F_{\theta|X} = F_{\theta|X}^{-1}(\frac12)$ minimizes the quantity inside the expectation pointwise, and therefore in average.



\item 
For $Y$ a discrete variable, the $c$ that minimizes $E[1_{Y \neq c}]$ is $c = \arg \max_y P(Y = y)$

$$\bar {\mathcal L}_{\mathcal R} (\hat \theta) = E_X[ E_{\theta|X}[1_{\hat \theta(X) \neq \theta}] ]$$
$$\hat \theta = \arg \max_{k=1...K} P(\theta = k | X)$$

\item The $c$ that minimizes $E[||Y - c||_2^2]$ is $c = E[Y]$
$$\hat \theta = E[\theta | X]$$
\end{enumerate}

\end{document}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

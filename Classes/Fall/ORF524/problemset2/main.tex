\documentclass[12pt]{article}

% packages
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}

% custom commands
\newcommand{\Q}[1]{\subsubsection*{Question #1}}


% parameters
\geometry{hmargin=1cm,vmargin=1cm}
\title{ORF524 - Problem Set 2}
\author{Bachir EL KHADIR }

\begin{document}

\maketitle

\Q{1}
\begin{align}
\mathbb{P}^{\theta}(X = a) 
&= \mathbb{P}^{\theta}(X = a , T(X) = T(a)) \\
&= \mathbb{P}^{\theta}(X = a | T(X) = T(a))\mathbb{P}^{\theta}(T(X) = T(a)) \\
&= \mathbb{P}(X = a | T(X) = T(a))\mathbb{P}^{\theta}(T(X) = T(a)) & \text{By definition of sufficiency}\\
&= \mathbb{P}(X' = a | T(X) = T(a))\mathbb{P}^{\theta}(T(X) = T(a)) \\
&= \mathbb{P}^{\theta}(X' = a)
\end{align}


\Q{2}
Let's first note that:

$$l(\theta) = \frac{1}{\int_{\mathbb{R}^d} h(x) e^{\alpha(\theta)^T T(x)} \rm{d}x} = l(\alpha(\theta))$$

As a result, $f^{\theta}$ is determined entirely by $\alpha(\theta)$, we can then denote it $f_{\alpha(\theta)}$
As a result 
$$P = \{ f_{\alpha} | \alpha \in \alpha(\Theta) \}$$

\Q{3}
Since $T$ a sufficient statistics for $P_{\theta}$, it is also sufficient for $P'_{\theta}$, because by definition, for all $P^{\theta} \in \cal P'^{\theta} \subseteq \cal P^{\theta}$ , conditioning on the value of $T$ makes the distribution independent of $\theta$.

If there exist a sufficient statistics $T'$ for $P^{\theta}$, $T$ is also sufficient for $P'^{\theta}$. Since $T$ is minimal for $P'_{\theta}$, there exist a function $\phi$ such that $T = \phi(T')$, and thus $T$ is minimal for $P_{\theta}$.

\Q{4}
\begin{align}
\mathcal{N}^n_{\mu, \mu}(x) 
&= \frac{1}{(\sqrt{2\pi\mu})^n} e^{-\sum_i \frac{(x_i-\mu)^2}{2\mu}} \\
&= \frac{1}{(\sqrt{2\pi\mu})^n} e^{ - \frac{\sum_i x_i^2}{2\mu} - \sum x_i - n\frac{\mu}{2}} \\
&= \frac{1}{(\sqrt{2\pi\mu})^n} e^{ - \frac{\sum_i x_i^2}{2\mu} - n\frac{\mu}{2}} e^{-\sum x_i} \\
&= g_{\mu}(\sum x_i^2) f(x)
\end{align}
$$T(x) = \sum x_i^2$$

In the case $n=1$, $x^2 = f(x) $, so $x^2$ is sufficient. But it is not minimal because $x$ cannot be written as a function of $x^2$


\Q{5}

Let $A$ be the constant of normalization.
\begin{align*}
f^n_{\theta}(X) &= 
f^n_{(\Sigma, \mu)}(X1, .., X_n)\\
&=A  \exp\{- \sum_i \frac12 (X_i-\mu)^T\Sigma^{-1}(X_i-\mu)\}  \\
&=A  \exp\{- \frac12 \sum_i (X_i-\hat \mu + \hat \mu - \mu)^T\Sigma^{-1}(X_i-\hat \mu + \hat \mu - \mu)\}  \\
&=A  \exp\{- \frac12 \sum_i   \left(
(X_i-\hat \mu)^T\Sigma^{-1}(X_i-\hat \mu)
+ 2 (X_i-\hat \mu)^T\Sigma^{-1}(\hat \mu- \mu)
+ (\hat \mu - \mu)^T\Sigma^{-1} (\hat \mu - \mu)
\right)\\
&=A  \exp\{- \frac12 \sum_i  
\text{Tr} 
\left((X_i-\hat \mu)^T\Sigma^{-1}(X_i-\hat \mu)
+ (\hat \mu - \mu)^T\Sigma^{-1} (\hat \mu - \mu)
\right)\}\\
&=A  \exp\{- \frac12  \sum_i 
\text{Tr} 
\left(\Sigma^{-1}(X_i-\hat \mu)(X_i-\hat \mu)^T
+ \Sigma^{-1} (\hat \mu - \mu)(\hat \mu - \mu)^T
\right)
\}\\
&=A \exp\{-  \frac n 2 
\text{Tr} 
\Sigma^{-1}
\left(
\hat \Sigma
+ (\hat \mu - \mu)(\hat \mu - \mu)^T
\right)
\}\\
\end{align*}
By the factorisation theorem $(\hat \Sigma, \hat \mu)$ is sufficient.


For $X$ and $X'$ two observations, let's note $\hat u = \hat \Sigma+ (\hat \mu - \mu)(\hat \mu - \mu)^T$, $\hat u' = \hat \Sigma'+ (\hat \mu' - \mu)(\hat \mu' - \mu)^T$

$$\frac{f^n_{\theta}(X)}{ f^n_{\theta}(X')} = 
\exp\{-  \frac n 2 
\text{Tr} 
\Sigma^{-1}
\left(
\hat u - \hat u'
\right)
\}
$$


Let's suppose that this quantity doesn't depend on $\theta$. so
$$\text{Tr} 
\Sigma^{-1}_1 
\left(
\hat u - \hat u'
\right) =
\text{Tr} 
2 \Sigma^{-1}_1 
\left(
\hat u - \hat u'
\right) = 0$$


But $\hat u - \hat u'$ is symmetric non negative, so there exist $P$ a invertible matrix and $D = \text{diag}(a_1, ... a_n)$ such that it equals $PDP^{-1}$.


Let $\Sigma = P \text{diag}( b_1 ,...,b_n ) P^{-1}$ where $b_i = \frac{1}{a_n}$ if $a_n \neq 0$, $1$ otherwise.

then $\text{Tr}(\Sigma^{-1}PDP^{-1}) = \sum a_i^2 = 0$, which means $\hat u = \hat u'$. eg:


$$(\forall \mu)\, \hat \Sigma+ (\hat \mu - \mu)(\hat \mu - \mu)^T = \hat \Sigma'+ (\hat \mu' - \mu)(\hat \mu' - \mu)^T$$

Letting $\mu = \hat \mu$, we have $\hat \Sigma = \hat \Sigma' + (\hat \mu' - \hat \mu)(\hat \mu' - \hat \mu)^T$


Letting $\mu = \hat \mu'$, we have $\hat \Sigma = \hat \Sigma' - (\hat \mu' - \hat \mu)(\hat \mu' - \hat \mu)^T$


We conclude that $\hat \Sigma = \hat \Sigma'$ and $|| \hat \mu - \hat \mu'||^2 = \text{Tr}(\hat \mu' - \hat \mu)(\hat \mu' - \hat \mu)^T = 0$, eg $\hat \mu = \hat \mu'$. 
And therfore $(\hat \Sigma, \hat \mu)$ is minimal.




\Q{6}
The log-likelihood function:

\begin{align}
\mathcal{L}(\theta; x) &= \log(\Pi_i f(x_i | \theta)) & \text{because iid}\\
&= \log\frac{1}{(\sigma \sqrt{2\pi})^n} e^{-\sum_i \frac{(x_i-\mu)^2}{2\sigma^2}} \\
&= -n\log(\sqrt{2\pi}) - n \log \sigma -\sum_i \frac{(x_i-\mu)^2}{2\sigma^2} \\
\end{align}


We maximize first in $\mu$. 

$$\mu = \arg\max -\sum_i (x_i-\mu)^2$$

Since the function is concave in $\mu$, we find the optimum by setting the first derivative to 0, ie $\mu = \bar x$

We now maximize in $\sigma$ by setting the first derivative to $0$ and verifying thatthe second derivative is negative

$$\frac{d \mathcal{L}}{d \sigma}(\mu = \bar x, \sigma) = -\frac{n}{\sigma} + \frac{\sum_i (x_i-\bar x)^2}{\sigma^3} = 0 \Rightarrow \sigma^2 = \frac{\sum_i (x_i - \bar x)^2} n$$
$$\frac{d^2 \mathcal{L}}{d \sigma^2}(\mu = \bar x, \sigma^2 = S_n^2) = \frac{n}{\sigma^2} - 3 \frac{\sum_i (x_i-\bar x)^2}{\sigma^4} = \frac n {S_n^2} - \frac{3n} {S_n^2} < 0 $$

MLE

$$\theta = (\bar x, \frac1n \sum_i (x_i - \bar x)^2) = (\bar x, S_n^2)$$



\Q{7}

Let's denote
$$\theta := (p_l, \mu_l, \Sigma_l)_l$$

\subsubsection*{Estimation}



\begin{align}
&\mathcal L^n (X, L; \theta) \\
&= \prod_i \mathcal L(X_i, L_i; \theta) 
\\&= \prod_i \sum_l \mathbb P (L_i = l) f(X_i, \mu_l, \Sigma_l) 1_{\{L = l\}}
\\&= \prod_i \sum_l \mathbb p_l 1_{\{L = l\}} \frac{e^{-\frac12 (X_i - \mu_j)^T \Sigma_j^{-1}(X_i - \mu_j)}}{(2\pi)^{\frac{n}2} \sqrt{\det \Sigma_j}}
\\&= \prod_i \exp{\sum_l 1_{L = l} ( -\frac12 (X_i - \mu_j)^T \Sigma_j^{-1}(X_i - \mu_j) -\frac{n\log(2\pi)}2  - \frac{\log\det \Sigma_j}2 + \log p_l) }
\\&= \exp{\sum_{i, l} 1_{L = l} ( -\frac12 (X_i - \mu_j)^T \Sigma_j^{-1}(X_i - \mu_j) -\frac{n\log(2\pi)}2  - \frac{\log\det \Sigma_j}2 + \log p_l) }
\end{align}


\begin{align}
Q(\theta, \theta') &= \mathbb{E}^{\theta'}[ \log\mathcal{L}^n(X, L | \theta) | X] \\
&= \sum_{i,l} \mathbb{E}^{\theta'}[1_{L_i = l} ( -\frac12 (X_i - \mu_j)^T \Sigma_j^{-1}(X_i - \mu_j) -\frac{n\log(2\pi)}2  - \frac{\log\det \Sigma_j}2 + \log p_l | X] \\
&= \sum_{i,l} \mathbb{P}^{\theta'}(L_i = l | X) ( -\frac12 (X_i - \mu_j)^T \Sigma_j^{-1}(X_i - \mu_j) -\frac{n\log(2\pi)}2  - \frac{\log\det \Sigma_j}2 + \log p_l)\\
&= \sum_{i,l} T^{\theta'}_{i,l} ( -\frac12 (X_i - \mu_j)^T \Sigma_j^{-1}(X_i - \mu_j) -\frac{n\log(2\pi)}2  - \frac{\log\det \Sigma_j}2 + \log p_l)\\
\end{align}


Where:
\begin{align}
T^{\theta'}_{i,l} &:= \mathbb{P}^{\theta'}(L_i = l | X_i) 
\\&= \frac{f(L_i = l , X=X_i| \theta')}{f(X=X_i; \theta')} 
\\&=  \frac{ \mathbb{P}(L = l|\theta')  f(X=X_i| L = l; \theta')}{\sum_k \mathbb{P}(L = k|\theta') f(X=X_i| L = k; \theta')}
\end{align}

\subsubsection*{Maximization}

We can optimize first in $p$, $\mu$ and then $\Sigma$

\begin{enumerate}

\item
\begin{align}
p^* &:= \arg\max_{p, \sum_l p_l = 1} Q(\theta, \theta')
\\ &=  \arg\max_{p} \sum_{i,l} T^{\theta'}_{i,l}  \log p_l
\end{align}

Using lagrange multiplier:
\begin{align}
p^* &:= \arg\max_{p, \lambda \geq 0} \sum_{i,l} T^{\theta'}_{i,l}  \log p_l - \lambda (1 - \sum_l p_l)
\\&= g(p, \lambda)
\end{align}
For $l = 1.. n$:
$$ 0 = \frac{\partial g}{\partial p_l} = \frac{\sum_i T^{\theta'}_{i,l}}{p_l} + \lambda \Rightarrow p_l = - \frac{\sum_i T^{\theta'}_{i,l}}{\lambda}$$
and since $\sum_l p_l = 1$, $\lambda = -\sum_{i,l} T^{\theta'}_{i,l} = -n$ and therefore:
$$p_l^{*} = \frac1 n \sum_i T^{\theta'}_{i,l}$$

\item

The optimization in $\mu$ and $\Sigma$ looks like the optimization in question $6$.
\begin{align}
\mu_l^*, \Sigma_l^* &= \arg\max_{\mu_l, \Sigma_l} Q(\theta, \theta')
\\&=\arg\max_{\mu, \Sigma} \sum_{i} T^{\theta'}_{i,l} ( -\frac12 (X_i - \mu)^T \Sigma^{-1}(X_i - \mu) - \frac{\log|\det \Sigma|}2)
\end{align}


If we write $\Sigma = \text{diag}(\sigma_1, ... \sigma_n) > 0$  and $\mu = (\mu_k)_k$ we have:

\begin{align}
\mu_l^*, \Sigma_l^* 
&=\arg\max_{\mu, \Sigma} \sum_{i} T^{\theta'}_{i,l} \left( -\frac12 \sum_k \frac{1}{\sigma_k}((X_i)_k - \mu_k)^2  - \frac12 \sum_k log|\sigma_k|\right)
\\&=\arg\min_{\mu, \Sigma} \sum_{k, i} T^{\theta'}_{i,l} \frac{1}{\sigma_k}((X_i)_k - \mu_k)^2  +  \sum_k (\sum_i T^{\theta'}_{i,l}) log|\sigma_k|
\end{align}

The program is quadratic and concave in $\mu$, so by setting the first derivative to 0:

$$\mu_l^* = \frac{\sum_i T^{\theta'}_{i,l} X_i}{\sum_i T^{\theta'}_{i,l}}$$


\item
We can optimize on each $\sigma_k$ independenly by linearity, $\sigma_k^*$ is the solution to:
$$\frac{\partial}{\partial \sigma_k} \sum_{i} T^{\theta'}_{i,l} \frac{1}{\sigma_k}((X_i)_k - \mu_k^*)^2  -  (\sum_i T^{\theta'}_{i,l}) log(\sigma_k) =
 - (\sum_{i} T^{\theta'}_{i,l} ((X_i)_k - \mu_k^*)^2) \frac{1}{\sigma_k^2}  +  (\sum_i T^{\theta'}_{i,l}) \frac{1}{\sigma_k}
 = 0$$

so:

$$\sigma_k^* = \frac{\sum_{i} T^{\theta'}_{i,l} ((X_i)_k - \mu_k^*)^2}{\sum_i T^{\theta'}_{i,l}}$$

(we can calculate the second derivative to prove that this is indeed a minimum like we did in question 6).
\end{enumerate}

\Q{8}
\begin{itemize}

\item
\begin{align}
  \mathbb{P}(\hat \theta \leq x) &= \mathbb{P}\{ \text{max}_i \, x_i \leq x \}\\
  &= \mathbb{P}(\cap_i \{ x_i \leq x \})\\
  &= \Pi_i \mathbb{P}(x_i \leq x) &\text{ By independence}\\ 
  &= \text{min} \, (1, \left(\frac{x}{\theta}\right)^n)\\
  &= \int_{\mathbb{R}} n \frac{y^{n-1}}{\theta^n} 1_{0 \leq y \leq \theta} 1_{y \leq x} \rm{dy} \\
  &= \int^x f(y) \rm{dy} & \text{where $f(y) = n \frac{y^{n-1}}{\theta^n} 1_{0 \leq y \leq \theta}$}
\end{align}
 $f$ is the density of $\hat \theta$ w.r.t to lebesgue measure.


\item
\begin{align}
\mathbb{E}[ \hat \theta] 
&= \int_0^{\theta} y n \frac{y^{n-1}}{\theta^n} \rm{dy} \\
&= \frac{n}{n+1} \theta 
\end{align}
So $\hat \theta$ is biased for $\theta \neq 0$.

\Q{9}
\begin{enumerate}

\item

By definition of conditional probability:
$$\mathcal{L}(\theta; x) = \mathcal{L}(\theta; x_1 | x_2,...) \mathcal{L}(\theta; x_2 | x_3,...) ... \mathcal{L}(\theta; x_n)$$
$$\mathbb{E} \log \mathcal{L}(x; \theta) = \sum_i \mathbb{E} \log\mathcal{L}(x_i; \theta | x_{i+1} ... x_n) = - \sum_i H(x_i|x_{i+1}...x_n)$$
If the $x_i$ are iid:
$$\mathbb{E} \log \mathcal{L}(x; \theta) = - \sum_i H(x_i)$$

\item
\begin{align}
H(X) - H(X|Y) &= 
\mathbb{E} log(f(Y)/f(X,Y)) \\
&\leq \log \mathbb{E} \frac{f(Y)}{f(X,Y)} &\text{By concavity of $\log$}\\
&= \log \int \frac{f(Y)}{f(X,Y)} f(X,Y)\\
&= \log 1 = 0
\end{align}
\end{enumerate}
\Q{10}


$$g(\beta) = \sum (y_i - x_i^T \beta)^2$$
$$f(\beta) = \sum (y_i - x_i^T \beta)^2) + \lambda ||\beta||^2 + g(\beta) = \lambda ||\beta||^2$$

\begin{align}
\nabla_{\beta} f &= \sum_i -2 (y_i - x_i^T \beta) x_i + 2 \lambda \beta \\
&= 2 ( \lambda \beta - \sum_i(y_i - x_i^T \beta) x_i)\\
&= 2 ( (\lambda I_n + \sum_i x_i x_i^T) \beta + \sum_i y_i x_i)
\end{align}

The hessian of $f$ is $F := 2(\lambda I_n + \sum_i x_i x_i^T)$. $F$ is symetric and its eigen values are those of $\sum_i x_i x_i^T$ offset by $\lambda$. For $\lambda$ large enough ($\lambda > ||\sum_i x_i x_i^T||_{\infty}$), the eigen values of $F$ are all positive, and therefore $f$ is strictly convexe and admit at most one global minimum.

In addition, there is a solution iff $\nabla f = 0$ has a solution, and the solution happens to be the minimum. Which is the case for 

$$\beta = \frac12 F^{-1} \sum y_i x_i = (\lambda I_n + \sum_i x_i x_i^T)^{-1} \sum_i y_i x_i$$


\Q{11}

Let's consider that $\mathcal L(\beta) \sim \mathcal N (0, \frac{\sigma^2}{\lambda})$, and $\mathcal L (Y | \beta) = \mathcal N(X^T \beta, \Sigma^2) $ where 
$\Sigma^2 = \text{diag}(\sigma^2_i)_i$


then
$$f(\beta) = cte \, e^{-\frac{\lambda}{2\sigma^2} \beta^T \beta}$$
$$f(Y | \beta) = cte \, e^{-\frac{1}{2\sigma^2} \sum_i (y_i - x_i^T \beta)^2}$$


$$\sum (y_i - x_i^T \beta)^2 + \lambda ||\beta||^2 = cte  -\log( e^{-\sum_i \frac1 {\sigma^2}(y_i - x_i^T \beta)^2} e^{-\frac{\lambda}{\sigma^2} \beta^T \beta}) = cte -log( f(Y | \beta) g(\beta) )$$




Minimizing for ridge regression is the same as maximizing the posterior distribution of $\beta$: $\arg\max_{\beta} h(\beta|Y) = \arg\max_{\beta} f(Y | \beta) g(\beta) =  \arg\max_{\beta} \sum (y_i - x_i^T \beta)^2 + \lambda ||\beta||^2$

\Q{12}

Let $\hat X = (X^l)_{l \in \mathbb{N}^p: |l| \leq k}$
$$Y = poly(X) + \epsilon = \beta \hat X + \epsilon$$
$\beta = (\sum_i \hat x_i \hat x_i^T)^{-1} \sum_i y_i \hat x_i$
by using the precedent questions.
\end{itemize}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

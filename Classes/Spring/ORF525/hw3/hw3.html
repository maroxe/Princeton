<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2016-03-08 Tue 21:21 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title></title>
<meta name="generator" content="Org-mode" />
<meta name="author" content="bachir el khadir" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../../css/special-block.css" />
<link href="http://thomasf.github.io/solarized-css/solarized-dark.min.css" rel="stylesheet"></link>
<script type="text/javascript" src="http://code.jquery.com/jquery-latest.min.js"></script>
<script src="http://127.0.0.1:60000/autoreload.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<div class="HTML">
<p>
<script>
AutoReload.Watch('localhost:60000');
</script>
</p>

</div>


<div id="outline-container-orgheadline1" class="outline-2">
<h2 id="orgheadline1"><span class="section-number-2">1</span> Money Ball: Who You Will Buy?</h2>
<div class="outline-text-2" id="text-1">
<p>
a)
</p>

<div class="org-src-container">

<pre class="src src-R" id="orgsrcblock1">data <span style="color: #8bc34a;">&lt;-</span> tbl_df(read.csv(<span style="color: #9ccc65;">'MLB2008.csv'</span>))
data.train <span style="color: #8bc34a;">&lt;-</span> data[1:154, ]
data.test <span style="color: #8bc34a;">&lt;-</span> data[155:dim(data)[1], ]
0
</pre>
</div>




<div class="org-src-container">

<pre class="src src-R" id="orgsrcblock2">feature.names <span style="color: #8bc34a;">&lt;-</span> colnames(data)[6:length(data)]
formula <span style="color: #8bc34a;">&lt;-</span> as.formula(paste(<span style="color: #9ccc65;">'SALARY ~'</span>, paste(feature.names, collapse=<span style="color: #9ccc65;">'+'</span>)))
rpart.model <span style="color: #8bc34a;">&lt;-</span> rpart(formula, data=data.train, method=<span style="color: #9ccc65;">'anova'</span>)
prp(rpart.model)
</pre>
</div>

<div class="org">

<div class="figure">
<p><img src="img/tree.png" alt="tree.png" class="center" />
</p>
<p><span class="figure-number">Figure 1:</span> Tree generated by rpart</p>
</div>

</div>



<p>
Now we try the pruning of the tree:
</p>

<div class="org-src-container">

<pre class="src src-R" id="orgsrcblock3">mse <span style="color: #8bc34a;">&lt;-</span> data.frame(
    prune=0,
    train=norm(predict(rpart.model) - data.train$SALARY, type=<span style="color: #9ccc65;">"2"</span>), 
    test=norm(predict(rpart.model, data.test) - data.test$SALARY, type=<span style="color: #9ccc65;">"2"</span>)
)
<span style="color: #fff59d;">for</span>(prune <span style="color: #fff59d;">in</span> (1:10 / 10.)) {
    rpart.model.prune <span style="color: #8bc34a;">&lt;-</span> prune(rpart.model, cp=prune)
    mse <span style="color: #8bc34a;">&lt;-</span> rbind(mse,
                 c(  
                     prune=prune,
                     train=norm(predict(rpart.model.prune) - data.train$SALARY, type=<span style="color: #9ccc65;">"2"</span>), 
                     test=norm(predict(rpart.model.prune, data.test) - data.test$SALARY, type=<span style="color: #9ccc65;">"2"</span>)
                 ))
}
mse
</pre>
</div>

<div class="org">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides" class="center">
<caption class="t-above"><span class="table-number">Table 1:</span> MSE</caption>

<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">Prune Parameter</td>
<td class="org-right">train</td>
<td class="org-right">test</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">36198240.8617805</td>
<td class="org-right">72784943.1305674</td>
</tr>

<tr>
<td class="org-right">0.1</td>
<td class="org-right">44735392.6910594</td>
<td class="org-right">74445712.3863091</td>
</tr>

<tr>
<td class="org-right">0.2</td>
<td class="org-right">44735392.6910594</td>
<td class="org-right">74445712.3863091</td>
</tr>

<tr>
<td class="org-right">0.3</td>
<td class="org-right">52769875.8870473</td>
<td class="org-right">73588157.3153948</td>
</tr>

<tr>
<td class="org-right">0.4</td>
<td class="org-right">52769875.8870473</td>
<td class="org-right">73588157.3153948</td>
</tr>

<tr>
<td class="org-right">0.5</td>
<td class="org-right">52769875.8870473</td>
<td class="org-right">73588157.3153948</td>
</tr>

<tr>
<td class="org-right">0.6</td>
<td class="org-right">52769875.8870473</td>
<td class="org-right">73588157.3153948</td>
</tr>

<tr>
<td class="org-right">0.7</td>
<td class="org-right">52769875.8870473</td>
<td class="org-right">73588157.3153948</td>
</tr>

<tr>
<td class="org-right">0.8</td>
<td class="org-right">52769875.8870473</td>
<td class="org-right">73588157.3153948</td>
</tr>

<tr>
<td class="org-right">0.9</td>
<td class="org-right">52769875.8870473</td>
<td class="org-right">73588157.3153948</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">52769875.8870473</td>
<td class="org-right">73588157.3153948</td>
</tr>
</tbody>
</table>

</div>



<p>
The following plots the result:
</p>
<div class="org-src-container">

<pre class="src src-R" id="orgsrcblock4">ggplot(mse) + 
geom_point(aes(x = prune, y = train, color=<span style="color: #9ccc65;">'train'</span>)) + geom_line(aes(x = prune, y = train, color=<span style="color: #9ccc65;">'train'</span>)) + 
geom_point(aes(x = prune, y = test, color=<span style="color: #9ccc65;">'test'</span>)) +
xlab(<span style="color: #9ccc65;">"B cp (Pruning Parameter)"</span>) + ylab(<span style="color: #9ccc65;">"MSE"</span>)
</pre>
</div>

<div class="org">

<div class="figure">
<p><img src="img/mse.png" alt="mse.png" class="center" />
</p>
<p><span class="figure-number">Figure 2:</span> MSE for traingin / test for different pruning parameters</p>
</div>

</div>




<p>
b) 
</p>

<p>
This section we try random forests.
</p>

<div class="org-src-container">

<pre class="src src-R" id="orgsrcblock5">mse.forest <span style="color: #8bc34a;">&lt;-</span>  data.frame(B=integer(), train=numeric(), test=numeric()) 
<span style="color: #fff59d;">for</span>(B <span style="color: #fff59d;">in</span> seq(10, 100,by=10)) {
    randomForest.model <span style="color: #8bc34a;">&lt;-</span> randomForest(formula, data=data.train, ntree=B)
    mse.forest <span style="color: #8bc34a;">&lt;-</span> rbind(mse.forest,
                        data.frame(  
                            B=B,
                            train=norm(predict(randomForest.model, data.train) - data.train$SALARY, type=<span style="color: #9ccc65;">"2"</span>), 
                            test=norm(predict(randomForest.model, data.test) - data.test$SALARY, type=<span style="color: #9ccc65;">"2"</span>)
                        ))
}
0
</pre>
</div>




<div class="org-src-container">

<pre class="src src-R" id="orgsrcblock6">ggplot(mse.forest) + 
geom_point(aes(x = B, y = train, color=<span style="color: #9ccc65;">'train'</span>)) + geom_line(aes(x = B, y = train, color=<span style="color: #9ccc65;">'train'</span>)) + 
geom_point(aes(x = B, y = test, color=<span style="color: #9ccc65;">'test'</span>)) +
xlab(<span style="color: #9ccc65;">"B"</span>) + ylab(<span style="color: #9ccc65;">"MSE"</span>)
</pre>
</div>

<div class="org">

<div class="figure">
<p><img src="img/rfmse.png" alt="rfmse.png" class="center" />
</p>
<p><span class="figure-number">Figure 3:</span> MSE for random Forest</p>
</div>

</div>



<p>
c) 
   Random forest tends to perform better than pruning in both training and testing settings. The best bagging parameter in hindsight is \(B = 20\). We use that for our prediction.
</p>


<div class="org-src-container">

<pre class="src src-R">Bmax = 20
randomForest.model <span style="color: #8bc34a;">&lt;-</span> randomForest(formula, data=data.train, ntree=Bmax)
pred <span style="color: #8bc34a;">&lt;-</span> predict(randomForest.model, data.test)
most.undervalued.player <span style="color: #8bc34a;">&lt;-</span> which.max(pred - data.test$SALARY)
as.character(most.undervalued.player)
name <span style="color: #8bc34a;">&lt;-</span> as.character(data.test$PLAYER[most.undervalued.player])
real.salary <span style="color: #8bc34a;">&lt;-</span> data.test$SALARY[most.undervalued.player]
predicted.salary <span style="color: #8bc34a;">&lt;-</span> pred[most.undervalued.player]
most.undervalued.player
data.frame(name=name, real.salary=real.salary, predicted.salary=round(predicted.salary))
</pre>
</div>

<div class="org">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides" class="center">
<caption class="t-above"><span class="table-number">Table 2:</span> The most undervalued player</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-left">name</td>
<td class="org-right">real salary</td>
<td class="org-right">predicted salary</td>
</tr>

<tr>
<td class="org-left">Kevin Youkilis</td>
<td class="org-right">3000000</td>
<td class="org-right">10788613</td>
</tr>
</tbody>
</table>

</div>


<p>
Looking at the evolution of this player at the prospectus:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides" class="center">
<caption class="t-above"><span class="table-number">Table 3:</span> Salary Evolution</caption>

<colgroup>
<col  class="org-right" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">Year</th>
<th scope="col" class="org-left">Team</th>
<th scope="col" class="org-left">Salary</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">2008</td>
<td class="org-left">BAL</td>
<td class="org-left">$455,000</td>
</tr>

<tr>
<td class="org-right">2009</td>
<td class="org-left">BAL</td>
<td class="org-left">$3,350,000</td>
</tr>

<tr>
<td class="org-right">2010</td>
<td class="org-left">BAL</td>
<td class="org-left">$7,100,000</td>
</tr>

<tr>
<td class="org-right">2011</td>
<td class="org-left">BAL</td>
<td class="org-left">$10,600,000</td>
</tr>

<tr>
<td class="org-right">2012</td>
<td class="org-left">BAL</td>
<td class="org-left">$12,350,000</td>
</tr>

<tr>
<td class="org-right">2013</td>
<td class="org-left">BAL</td>
<td class="org-left">$15,350,000</td>
</tr>

<tr>
<td class="org-right">2014</td>
<td class="org-left">BAL</td>
<td class="org-left">$15,350,000</td>
</tr>

<tr>
<td class="org-right">2015</td>
<td class="org-left">ATL</td>
<td class="org-left">$11,000,000</td>
</tr>

<tr>
<td class="org-right">2016</td>
<td class="org-left">ATL</td>
<td class="org-left">$11,000,000</td>
</tr>

<tr>
<td class="org-right">2017</td>
<td class="org-left">ATL</td>
<td class="org-left">$11,000,000</td>
</tr>

<tr>
<td class="org-right">2018</td>
<td class="org-left">ATL</td>
<td class="org-left">$11,000,000</td>
</tr>
</tbody>
</table>

<p>
We see that he was indeed very undervalued, the reason might be that he was just starting his career at 2008.
</p>
</div>
</div>


<div id="outline-container-orgheadline2" class="outline-2">
<h2 id="orgheadline2"><span class="section-number-2">2</span> Tame Categorical Variables in Tree Regression</h2>
<div class="outline-text-2" id="text-2">
<p>
2.1.
</p>

\begin{align*}
\sum_i (Y_i - f(X_i))^2 &= \sum_i Y_i^2 + \sum_s  \sum_{X_i = s}  f(s)^2 - 2  Y_i f(s)
\\&= \sum_i Y_i^2 +  \sum_s |\{X_i = s\}| [f(s)^2 - 2 \bar Y_s f(s)]
\\&= \sum_i Y_i^2 +  \sum_k \sum_{s \in L_k} |\{X_i = s\}| [\alpha_k^2 - 2 \bar Y_s \alpha_k]
\\&= \sum_i Y_i^2 +  \sum_k  |L_k| (\alpha_k^2 - 2 avg(L_k) \alpha_k)
\end{align*}
<p>
It is clear that an optimal choice for \(\alpha_k\) would satisfy \(\alpha_k = avg(L_k)\) by minimizing a quadratic form.
Using the assumption \(\bar Y_1 < \ldots <\bar Y_M\), and the fact that \(L_k \ne \emptyset\), if \(k \ne k'\) then \(avg(L_k) \ne avg(L_k')\)
</p>

<p>
Let's assume that \(u, w \in L_k\) and that \(v \in L_k'\).
</p>

<p>
By minimality of \(f\) we have that:
</p>
<ul class="org-ul">
<li>\(f(u)^2 - 2\bar Y_u f(u)^2 \le {\alpha_k'}^2 - 2\bar Y_u {\alpha_k'}\), otherwise we take out \(u\) from \(L_k\) and put it in \(L_k'\) (\(L_k\) would still be non empty) which would contradict the minimality of \(f\). Using the fact that \(f(u) = \alpha_k\), \(\alpha_k^2 - 2\bar Y_u \alpha_k^2 \le {\alpha_k'}^2 - 2\bar Y_u {\alpha_k'}^2\)</li>
<li>By the same argument, \(f(v)^2 - 2\bar Y_v f(v)^2 \le {\alpha_k}^2 - 2\bar Y_v {\alpha_k}^2\), or  \(-{\alpha_k}^2 + 2\bar Y_v \alpha_k \le -{\alpha_k'}^2 + 2\bar Y_v {\alpha_k'}\).</li>
<li>\(\alpha_k^2 - 2\bar Y_w \alpha_k \le {\alpha_k'}^2 - 2\bar Y_w {\alpha_k'}\)</li>
</ul>
<p>
Adding the second identity to the other two we get that:
\(2 \alpha_k \underbrace{(\bar Y_v - \bar Y_u)}_{\ge 0} \le 2 \alpha_k' (\bar Y_v - \bar Y_u)\)
\(2 \alpha_k \underbrace{(\bar Y_w - \bar Y_u)}_{\le 0} \le 2 \alpha_k'(\bar Y_w - \bar Y_u)\)
Which proves that \(\alpha_k = \alpha_k'\), and so \(k = k'\)
</p>

<p>
2.2
\(K = 2\)
</p>

<p>
The set of partitions \((L_1, L_2)\) admits a one to one mapping to the set of functions \(\{0, 1\}^{\{1, \ldots M\}}\), so:
\(\mathcal N_1 = 2^{M}\)
</p>

<p>
In this case, we know that \(L_1\) is of the form \(\{1, \ldots j\}\), so
\(\mathcal N_2 = M\)
</p>

<p>
\(\frac{\mathcal N_2}{\mathcal N_1} = \frac{M}{2^M}\)
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-orgheadline3" class="outline-2">
<h2 id="orgheadline3"><span class="section-number-2">3</span> <span class="todo TODO">TODO</span> Baggin and Random Forest</h2>
<div class="outline-text-2" id="text-3">
<p>
3.1 WLOS, take \(i = 1\).
</p>

<p>
For \(B \in \mathbb{N}^*\), and \(j \le B\), note
</p>
<ul class="org-ul">
<li>\(V_j = (Z^*_1, \ldots Z^*_n)\) the \(B\) bootstraps samples from \(\{Z_2, \ldots Z_n\}\) used in \(\hat f_{CV}^{(1)} = \frac1n \sum_{j=1}^B \mathcal A(U_j )\), Where \(\mathcal A(U)\) is the algorithm that returns the tree corresponding the observation \(U\).</li>
<li>\(U_j = (Z^*_1, \ldots Z^*_n)\) the \(B\) bootstraps samples from \(\{Z_1, \ldots Z_n\}\) used in \(\hat f_{OOB}^{(1)} =\frac1{|\{j, Z_1 \not \in V_j\}|} \sum_{j=1, Z_1 \not \in V_j} \mathcal A(V_j )\)</li>
</ul>

<p>
By the law of large numbers:
</p>
<ul class="org-ul">
<li>\(f^{(1)}_{CV}(X_1) \rightarrow E[\mathcal A(V)(X_1) | X_1]\) where \(V \in \mathbb R^d\) is drawn uniformly from \(\{Z_2, \ldots, Z_n\}^B\)</li>
<li><p>
\(f^{(1)}_{CV}(OOB) \rightarrow E[\mathcal A(U)(X_1) | X_1, Z_1 \not \in U]\) where \(U \in \mathbb R^d\) is drawn uniformly from \(\{Z_1, \ldots, Z_n\}^B\) 
</p>

<p>
It is easy to see that \(Law(U| Z_1 \not \in U) \overset{d}{=} Law(V)\) because by symmetry of the \(Z_2, \ldots Z_n\}\), all bootstrapping samples are equally likely for \(U\) when \(Z_1 \not \in U\)
</p>

<p>
We have just proven that  conditional on the \(Z_i\), \(\hat f_{OOB}^{(1)}(X_1) - \hat f_{CV}^{(1)}(X_1)\) converges a.s to 0 i. The result follow because the convergence in  is preserved with respect to taking sums and products, and the fact that convergence almost sure \(\implies\) convergence in probability.
</p>

<p>
3.2
</p></li>
</ul>


<p>
WLOS of generality we can assume that \(\mu = 0, \sigma=1\) by considering the rv \(\frac{X - \mu}{\sigma}\) and the fact that the correlation is insensitive to adding / multiplying.
</p>

<ul class="org-ul">
<li>Bootstrap 1 defines an application \(\pi: \{1, \ldots n\} \rightarrow \{1, \ldots n\}\), where \(\bar X_1^* = \frac1n \sum_{i=1}^n X_{\pi(i)}\), and \(\pi\) is uniformly chosen among such applications.</li>
<li><p>
Same way, Bootstrap 2 defines an application \(\sigma: \{1, \ldots n\} \rightarrow \{1, \ldots n\}\), independent from \(\pi\).
</p>

<p>
Then:
\(Cor(\bar X_1^*, \bar X_2^*) = E[\frac1n \sum_{i=1}^n X_{\pi(i)} \frac1n \sum_{i=1}^n X_{\pi(i)}]\)
</p></li>
</ul>
<p>
\(Var(\frac1B \sum_1^B \bar X_b^*) = \frac1{B^2} \sum Var(\bar X_b^*) + 2 \frac1{B^2} \sum_{a, b} Cov(\bar X_a^*, \bar X_b^*)\)
By symetry of the \(\barX_b^*\):
\(Var(\frac1B \sum_1^B \bar X_b^*) = \frac1{B}  Var(\bar X_1^*) + 2 \frac{B^2 - B}{B^2}  Cov(\bar X_1^*, \bar X_2^*) = \frac1{B}  Var(\bar X_1^*) + 2 \frac{B - 1}{B}  Cov(\bar X_1^*, \bar X_2^*)\)
</p>
</div>
</div>

<div id="outline-container-orgheadline4" class="outline-2">
<h2 id="orgheadline4"><span class="section-number-2">4</span> Explore the Boundary of RIP Conditions</h2>
<div class="outline-text-2" id="text-4">
<p>
4.1
\(|\inner{Ax}{Ay}| \le \delta_{s+t} \norm{x}_2 \norm{y}_2 \iff |\inner{A\frac{x}{\norm{x}_2}}{A\frac{y}{\norm{y}_2}}| \le \delta_{s+t}\)
Without loss of generality we assume that \(x\) and \(y\) have unit norm.
</p>

<p>
Since \(x\) and \(y\) have distinct support, \(\norm{x - y}_2^2 =  \norm{x+y}_2^2 = \norm{x}_2^2 + \norm{y}_2^2\) and \(\norm{x + y}_0 = \norm{x-y}_0 = \norm{x}_0 + \norm{y}_0 \le s + t\)
 then \(\norm{x \pm y}_2^2 = 2\) and:
\(2(1 - \delta_{s+t}) \le \norm{Ax \pm Ay}^2 \le 2(1 + \delta_{s+t})\), so:
</p>

\begin{align*}
 |\inner{Ax}{Ay}|
&= \frac14 |\norm{Ax + Ay}^2 - \norm{Ax - Ay}^2)|
\\&\le \frac14 | 2(1+\delta_{s+t}) - 2(1-\delta_{s+t})|
\\&\le \delta_{s+t}
\end{align*}
<p>
Which ends the proof
</p>


<p>
4.2
</p>
<ul class="org-ul">
<li><p>
<b>step 1:</b>
Let \(\chi\) be set that satisfies those conditions with maximal size so that \(U = \cup_{x \in \chi} B(x, \sqrt \frac s2)\) where:
</p>

<p>
\[B(x, \sqrt \frac s2) = \{ y \in U : \norm{x-y}_2 \le  \sqrt \frac s2 \} \subset  \{ y \in U : \norm{x-y}_0 \le \frac s2 \}\].
So \(|U| \le \sum_{x \in \chi} |B(x, \sqrt \frac s2)| \le |\chi| |B(0, \sqrt \frac s2)|\)
</p></li>
</ul>

<p>
But:
  \(|B(0, \sqrt \frac s2)| \le \#\{ z \in \{0, 1, -1\}^d \norm{x-z}_0 \le \frac s2\} \le {d \choose \frac s2} 3^{\frac s2}\),  and: \(|U| = {d \choose s} 2^s\)
</p>

<p>
So
</p>
\begin{align*}
.|\chi| &\ge (\frac43)^{\frac s2} \frac{{d \choose s}}{{d \choose \frac s2}}
\\& \ge (\frac43)^{\frac s2} \frac{(s/2)! (d-s/2)!}{s! (d-s)!}
\\& \ge (\frac43)^{\frac s2} \prod_{i=1}^{s/2} \frac{d-s+i}{s/2+1+i}
\\& \ge (\frac43)^{\frac s2} (\frac{d-s/2}{s+1})^{\frac s2}
\\& \ge (\frac43 \frac{d-s/2}{s+1})^{\frac s2}
\end{align*}

<p>
\(\frac43 \frac{d-s/2}{s+1} \ge \frac ds \iff s(4d- 2s) \ge  3 d(s+1) \iff 4ds - 2s^2 \ge 3ds + 3d \iff   2s^2 -ds + 3 d \ge 0 \iff  2s^2 \ge (s - 3)d\)
</p>
<ul class="org-ul">
<li><b>step 2</b>:</li>
</ul>
<p>
\(\norm{x-z}_0 \le 2s\)
\[\norm{Ax - Az}_2^2 \ge (1- \delta_{2s}) \norm{x - z}_2^2 \ge (1 - \delta_{2s}) \frac{s}2 \ge \frac{s}4\]
Which proves that the center of two balls are distant by more than twice their radiuses.
</p>
<ul class="org-ul">
<li><b>step 3:</b>
For \(x \in U\), \(\norm{Ax}_2 \le (1 + \delta_s) \norm{x}_2^2 \le \frac 32 s\)
So the balls of the centered at \(Ax\) where \(x \in \chi\) with raidus \(\sqrt{\frac{s}{16}}\) are contained in the ball centered at 0 with radius \((\sqrt{\frac 32} + \frac 14) \sqrt s\).
Since such balls are disjoint, their total volume  \(|\chi| Vol(\sqrt{\frac s{16}})\), where \(Vol(r)\) is the volume of the the ball of radius \(r\). We know that \(Vol(r) = r^n Vol(1)\), so:
\[|\chi| (\frac{s}{16})^{n/2} Vol(1) \le Vol(1) (\sqrt{\frac 32} + \frac 14)^{n/2}  s^{n/2}\]
Taking the \(\log\) and using step 1:
\[\frac{s}2 \log(\frac ds)  \le  \frac n2 \log(16 (\sqrt{\frac 32} + \frac 14)) \]  
So:
\[Cs \log(\frac ds)  \le  n  \]</li>
</ul>

<p>
4.3.
</p>

<p>
For computation reasons, we restrict the calculation to the case where \(d = 100\) instead of \(d = 1024\). 
</p>

<div class="org-src-container">

<pre class="src src-matlab">number_monte_carlo = 3;
epsilon = 0.001;
d = 100;
L = 20;
probabilities = zeros(L<span style="color: #84ffff;">-</span>1, L<span style="color: #84ffff;">-</span>1);
<span style="color: #fff59d;">for</span> <span style="color: #ffcc80;">i</span>=<span style="color: #8bc34a;">1:number_monte_carlo</span>
    A = randn(d,d);

    <span style="color: #fff59d;">for</span> <span style="color: #ffcc80;">n</span>=<span style="color: #8bc34a;">1:(L-1)</span>
    t = floor((d<span style="color: #84ffff;">/</span>(L)) <span style="color: #84ffff;">*</span> (n<span style="color: #84ffff;">-</span>1));

        X = 1<span style="color: #84ffff;">/</span>sqrt(t) <span style="color: #84ffff;">*</span> A(1<span style="color: #84ffff;">:</span>t,1<span style="color: #84ffff;">:</span>d);
        beta = randn(d, 1);
        <span style="color: #fff59d;">for</span> <span style="color: #ffcc80;">s</span>=<span style="color: #8bc34a;">1:(L-1)</span>
        r = floor((d<span style="color: #84ffff;">/</span>(L)) <span style="color: #84ffff;">*</span> (s<span style="color: #84ffff;">-</span>1));

            beta(1<span style="color: #84ffff;">:</span>r, 1) = 0;
            Y = X <span style="color: #84ffff;">*</span> beta;
            probabilities(n, s) = probabilities(n, s) <span style="color: #84ffff;">+</span> (norm(l1eq_pd(0<span style="color: #84ffff;">*</span>beta, X, 0<span style="color: #84ffff;">*</span>X, Y) <span style="color: #84ffff;">-</span> beta) <span style="color: #84ffff;">&lt;</span> epsilon);
            <span style="color: #b0bec5;">%probabilities(s, n) = probabilities(s, n) + norm(l1eq_pd(0*beta, X, 0*X, Y) - beta) ;</span>
        <span style="color: #fff59d;">end</span>
    <span style="color: #fff59d;">end</span>
<span style="color: #fff59d;">end</span>
dlmwrite(<span style="color: #9ccc65;">'mat.txt'</span>, probabilities <span style="color: #84ffff;">/</span> number_monte_carlo)
<span style="color: #8bc34a;">ans</span> = 0;
</pre>
</div>




<div class="figure">
<p><img src="img/heatmap.png" alt="heatmap.png" />
</p>
</div>

<div class="figure">
<p><img src="img/heatmap.png" alt="heatmap.png" />
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: bachir el khadir</p>
<p class="date">Created: 2016-03-08 Tue 21:21</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>

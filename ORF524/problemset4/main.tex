\documentclass[12pt]{article}

% packages
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}


\newcommand{\Q}[1]{\subsubsection*{Problem #1}}
\newcommand{\optimize}[4]
{
\begin{align*}
& \underset{#1}{\text{#4}}
& & #2 \\
& \text{subject to}
& & #3
\end{align*}
}



\newcommand{\union}[1]{\underset{#1}{\cup} }
\newcommand{\bigunion}[1]{\underset{#1}{\bigcup} \, }
\newcommand{\inter}[1]{\underset{#1}{\cap} }
\newcommand{\biginter}[1]{\underset{#1}{\bigcap} }

\newcommand{\minimize}[3]{\optimize{#1}{#2}{#3}{min}}
\newcommand{\maximize}[3]{\optimize{#1}{#2}{#3}{max}}
\newcommand{\icol}[1]{% inline column vector
  \left(\begin{smallmatrix}#1\end{smallmatrix}\right)%
}

\newcommand{\irow}[1]{% inline row vector
  \begin{smallmatrix}(#1)\end{smallmatrix}%
}



% parameters
\geometry{hmargin=1cm,vmargin=1cm}
\title{ORF524 - Problem Set 5}
\author{Bachir EL KHADIR }

\begin{document}

\maketitle

\Q{1}
By the triangle inequality
$||X|| - ||X_n||_p < ||X - X_n||_p$, and $||X_n||_p - ||X||_p < ||X - X_n||_p$, so $|||X_n|| - ||X||_p| < ||X - X_n||_p \rightarrow 0$

The converse is not true. Take $X_n = (-1)^n$, and $X = 1$.
$||X_n||_p = 1 = ||X||_p$, but
\[||X_n - X||_p = \left\{ \begin{array}{cc}2 & \text{if $n$ even}\\ 0 &\text{otherwise} \end{array} \right.\]
doesn't converge to 0.

\Q{2}

\begin{enumerate}
\item
  \begin{itemize}
  \item[(a.s)] Let's suppose $X_n \rightarrow X$ (resp.
    $Y_n \rightarrow Y$ as), and $\Omega_x$ (resp. $\Omega_y$) the set
    where it holds.  Then $P(\Omega_x \cap \Omega_y) = 1$ and for
    $\omega \in \Omega_x \cap \Omega_y$,
    $X_n(\omega) + Y_n(\omega) \rightarrow X(\omega) + Y(\omega)$

    Let's now suppose that $X_n \rightarrow X$ and $Y_n \rightarrow Y$
    in probabolity. Let $\epsilon > 0$, since
    $|X_n + Y_n - X - Y| \le |X_n - X| + |Y_n - Y|$, then:
    \begin{align*}
      P(|X_n + Y_n - X - Y| > \epsilon)
      &\le P(|X_n - X|+ | Y_n - Y| > \epsilon)
      \\&\le P(|X_n - X| > \epsilon) + P(| Y_n - Y| > \epsilon)
      \\& \rightarrow 0
    \end{align*}
  \item [(p)] Let's suppose $X_n \rightarrow X$ (resp.
    $Y_n \rightarrow Y$ as), and $\Omega_x$ (resp. $\Omega_y$) the set
    where it holds.  Then $P(\Omega_x \cap \Omega_y) = 1$, and for
    $\omega \in \Omega_x \cap \Omega_y$,
    $X_n(\omega)^T Y_n(\omega) \rightarrow X(\omega)^TY(\omega)$
  
    Let's now suppose that $X_n \rightarrow X$ and $Y_n \rightarrow Y$
    in probabolity. Let $\epsilon > 0$, since
  \item[($L_p$)] By the triangular inequality: $||X_n + Y_n - (X + Y)||_p \le ||X_n - X||_p + ||Y_n - Y||_p \rightarrow_n 0$
  \end{itemize}

  \item 
    \begin{itemize}
    \item[(a.s.)] For $\omega \in \Omega_x \cap \Omega_y$, $X_n(\omega) \rightarrow X(\omega)$ and $Y_n(\omega) \rightarrow Y(\omega)$ so $X_nY_n(\omega) \rightarrow XY(\omega)$
    \item[(p)]
    \begin{align*}|X_n^TY_n - X^TY| &\le |X_n^TY_n - X_n^TY + X_n^TY -
      X^TY| \\&\le |X_n^T(Y_n - Y)| + |(X_n-X)^TY| \\&\le
      |(X_n-X)^T(Y_n - Y)| + |X^T(Y_n-Y)| + |(X_n-X)^TY| \\&\le
      |X_n-X| |Y_n - Y| + |X| |Y_n-Y| + |X_n-X| |Y| &\text{ Cauchy
        Shwartz}
    \end{align*}
    , then:
    \begin{align*}
      P(|X_n^TY_n - X^TY| > \epsilon)
      &\le P(|X_n-X| |Y_n - Y|  > \epsilon) + P(|X| |Y_n-Y| > \epsilon) + P(|X_n-X| |Y| > \epsilon)
    \end{align*}
    Let's show that each term in the RHS converges to 0 when $n$ goes
    to infinity. Indeed:
    \begin{itemize}
    \item
      $P(|X_n-X| |Y_n - Y| > \epsilon)\le P(|X_n - X| >
      \sqrt{\epsilon}) + P(|Y_n - Y| > \sqrt{\epsilon}) \rightarrow 0$
    \item For $A > 0$,
      $(|X| \le A \text{ and } |Y_n-Y| \le \frac{\epsilon}{A}) \Rightarrow |X|
      |Y_n - Y| < \epsilon$,
      so
      $\{|X| |Y_n - Y| > \epsilon \} \subset \{|X|>A\} \cup \{|Y_n-Y|
      \} > \frac{\epsilon}{A+1}\}$, so (since $|X| < \infty$ a.s.):
    $$P(|X| |Y_n-Y| > \epsilon) \le P(|X| > A) + P(|Y_n - Y| > \frac{\epsilon}{A} \rightarrow_n P(|X| > A) \rightarrow_A 0$$
  \item Same for $P(|Y| |X_n-X| > \epsilon)$
  \end{itemize}
  
\item
  \begin{itemize}
  \item[(a.s)] If $Y_n \rightarrow Y$ on $\Omega_y$ of size one, then for
    $w \in \Omega_y \cap \{Y \ne 0\}$ (which is also of size 1) we have
    that for $n$ large enough $|Y_n - Y|(\omega) < \frac{Y(\omega)}2$,
    and therefore $Y_n(\omega) \ne 0$ and
    $\frac1{Y_n}(\omega) \rightarrow \frac1 Y(\omega)$. eg
    $\frac1 {Y_n} \rightarrow \frac1 Y$ a.s. and we can use the last
    question to prove that
  $$\frac{X_n}{Y_n} \rightarrow \frac{X}{Y} \text{ a.s.} $$

\item[(p)]
  We suppose $Y_n$ is bounded from below in probability (this is a necessary condition since: $\frac1 {Y_n} \text{converges} \Rightarrow \frac 1 {Y_n} \text{is bounded in probability} \Rightarrow Y_n \text{ bounded from below in probability}$

  Let $\alpha > 0$, $\epsilon > 0$, and $n$ large enough so that $ P(|Y - Y_n| > \epsilon) < \alpha$.

  Since $Y_nY$ is bounded from below in probability, $\exists \delta > 0 \forall n P(|Y_nY| < \delta) < \alpha $

  We have that $|\frac1 {Y_n} - \frac1 {Y}| = \frac{|Y - Y_n|}{|Y_nY|}$, so:
  \begin{align*}
    P(|\frac1 {Y_n} - \frac1 {Y}| > \epsilon)
    &\le P(|Y - Y_n| > \epsilon |Y_nY|)
    \\&= P(|Y - Y_n| > \epsilon |Y_nY|, |Y_nY| \le \delta)  + P(|Y - Y_n| > \epsilon |Y_nY|, |Y_nY| > \delta)
    \\&\le \alpha + P(|Y - Y_n| > \epsilon)
        \le 2\alpha
  \end{align*}
  And therefore we have convergence in probability.
\end{itemize}
\end{itemize}
\item Take $\forall n X_n = Y_n = X = -Y \sim \mathcal N(0,1)$, then:
  \begin{itemize}
  \item For $a)$, $V(X_n+Y_n) = 2$, but $Var(X+Y) = 0$.
  \item For $b)$, $E[X_nY_n] = E[\mathcal N(0, 1)] = 1$, but
    $E[XY] = -1$
  \item For $c)$, $E[\frac{X_n}{Y_n}] = 1, E[\frac{X}{Y}] = -1$
  \end{itemize}

\end{enumerate}
\Q{3}
\begin{enumerate}
\item $\forall \epsilon > 0 \, P(|X_n - X| > \epsilon) =  P(|X_n - X|^p > \epsilon^p)\le \frac{E[ |X_n - X|^p]}{\epsilon^p} \rightarrow 0$
\item
  \begin{itemize}
  \item[$\Rightarrow$] Assume for  every subsequence $X_{n_k} \rightarrow X$. $(X_n)$ is trivially a subsequence of $(X_n)$, so $X_n\rightarrow X$ 
  \item[$\Leftarrow$] Assume for  $X_n$ converges to $X$, and let $(X_{n_k})_k$ be a subsequence.
    \begin{itemize}
    \item a.s.:
      Let $\Omega$ the set of measure one in which the convergence holds.
      Let $\omega \in \Omega$ and $\epsilon > 0$, There exist $N \in \mathbb{N}$  such that for all $n > N$, $|X_n(\omega) - X(\omega)| < \epsilon$, in particular, since $n_k > k$, when
      $k > N$,  $|X_{n_k}(\omega) - X(\omega)| < \epsilon$,
      and thus $X_{n_k}(\omega) \rightarrow X(\omega)$
    \item in probability:
      $\epsilon > 0$, and $\delta > 0$. There exist $N \in \mathbb{N}$  such that for all $n > N$, $P(|X_n - X| > \epsilon) < \delta$, in particular when $k > N$, $P(|X_{n_k} - X| > \epsilon) < \delta$, and thus $\forall \epsilon P(|X_{n_k} - X| > \epsilon) \rightarrow 0$
    \item In distribution:
      for $\epsilon > 0 \exists N > 0 \forall n > N ||X_n - X||_{L_p} < \epsilon$

      Since $n_k > k$, if $k < N$ then $||X_{n_k} - X||_{L_p} < \epsilon$
    \end{itemize}

    
  \end{itemize}
\item
  if $x \ne c$, \[ F_n(x) \rightarrow_n \left\{ \begin{array}{cc}1&\text{if $x > c$}\\0&\text{if $x < c$}\end{array} \right.\]
  
  $\forall \epsilon > 0 \, P(|X_n - c| > \epsilon) = E[1_{|X_n - c| > \epsilon}] = E[1_{X_n - c > \epsilon} + 1_{X_n - c < -\epsilon}] \le (1-F_n(\epsilon+c)) + F_n(c - \epsilon) \rightarrow 0$
\end{enumerate}

\Q{4}
\begin{itemize}

\item
  $F$ is invertible so it is increasing and:
  $$P(X^* \le x) = P(F^{-1}(u) \le x) = P(u \le F(x)) = F^(x) = P(X \le x)$$
\item

  \begin{align*}
  X^* \le x &\Rightarrow F^{-1}(U) \le x
    \\& \Rightarrow \forall \epsilon > 0, \, U \le F(x+\epsilon)
    \\& \Rightarrow U \le F(x) &\text{(by right continuty)}
  \end{align*}

  So $P(X^* \le x) \le P(U \le F(x)) = F(x)$

  By definition of the $\sup$, there exist a sequence $x_n \rightarrow F^{-1}(F(x))$ so that $F(x_n) < F(x)$. Since $F$ is increasing $x_n < x$, and therefore by going to the limit $F^{-1}(F(x)) \le x$
  Since $F^{-1}$ is increasing (because it is taking the sup over larger sets):
  \begin{align*}
    U < F(x) & \Rightarrow F^{-1}(U) \le F^{-1}(F(x)) \le x
  \end{align*}

  So $F(x) = P(U < F(x)) \le X^* \le x)$

  c/c: $F$ is the cdf of $X^*$, so $X^*$ has the same distribution of $X$.
  
\end{itemize}

\Q{5}Let $u \in R^d$,
\begin{itemize}
\item
  Lemma: If for all $i \le d$ $X_n^{(i)}$(the $i$-th component of $X$) converges  to $X^{(i)}$ in probability, then $X_n$ converges to $X$ in probability.
  
  Proof: $P(|X_n - X| > \epsilon) \le P(\sum_i |X_n^{(i)} - X^{(i)}| > \epsilon) \rightarrow 0$
  
  \begin{align*}
    X_n \overset{a.s.}{\rightarrow} X
    & \Rightarrow (\forall u \in R^d )\, u^TX_n \overset{a.s.}{\rightarrow} u^TX & \text{By Ex2, b)}\\
    &\Rightarrow  (\forall u \in R^d )\,u^TX_n \overset{p}{\rightarrow} u^TX & \Rightarrow X_n \overset{p}{\rightarrow} X 
    \\&\Rightarrow  (\forall u \in R^d )\,u^TX_n \overset{D}{\rightarrow} u^TX & \Rightarrow X_n\overset{D}{\rightarrow} X\\
  \end{align*}
  
\item Let $X_n \rightarrow X$ in probability, and $X_{n_k}$ be a subsequence.
  Let prove by induction on the dimension $d$ of $X_n$, that there exist a subsequence $X_{n_{k_j}}$ such that $X_{n_{k_j}} \rightarrow X$ a.s.

  The case $d = 1$ was proven in class.

  For $d > 1$, let's suppose the induction property true for $d-1$, and let's denote $X_n = \icol{X_n^{(1)}\\X_n^{(-1)}}$, where $X_n^{(1)}$ has dimension $1$, and $X_n^{(-1)}$ has dimension $n-1$.

  Since $X_n^{(-1)} \overset{p}{\rightarrow} X^{(-1)}$, there exist a subsequence $X_{n_{k_j}}^{(-1)} \overset{a.s}{\rightarrow_j} X^{(-1)}$.

  And since $ X_{n_{k_j}}^{(1)} \overset{p}{\rightarrow_j} X^{(1)}$ we have also that there exist a subsequence $X^{(1)}_{n_{k_{j_r}}} \overset{a.s}{\rightarrow_j} X^{(1)}$
  
  c/c: $$X_{n_{k_{j_r}}} \overset{a.s}{\rightarrow_j} X$$
And thus the induction proof is complete.
\end{itemize}

\Q{6}
\begin{itemize}
\item
  $P(|X| > a)  \rightarrow_a 1$, Let
  Let $\epsilon > 0$ and $a$ large enough so that $P(|X| > a) < \epsilon$.
  
  $X_n \overset{D}{\rightarrow} X$, so by the continuous mapping theorem $||X_n|| \overset{D}{\rightarrow} ||X||$.

  Let $N \in N$, st for
  $n > N$, and $a > 0$ such that $a$ and $-a$ are continuous point of $F$, (such points are dense in $R$),
  $|F_n(a) - F(a)| < \epsilon$ and $|F_n(-a) - F(-a)| < \epsilon$
  
  Therefore $|E[1_{|X_n| > a}] - E[1_{|X| > a}]|
  \le |F_n(a) - F(a)| + |F_n(-a) - F(-a)| \le 2 \epsilon$.
  
  $|E[1_{|X_n| > a}]| \le \epsilon + |E[1_{|X_n| > a}] - E[1_{|X| > a}]|
   \le 3\epsilon$.
  
  As a result $X_n = O_p(1)$

\item 
  $||Y_n^TX_n|| \le ||Y_n|| ||X_n||$, without loss of generality we assume that $X_n$ and $Y_n$ are 1-dimensional.
  Like in question 2:
  $P(|X_nY_n| > \epsilon) \le P(|X_n| > A) + P(|Y_n - Y| > \frac{epsilon}{A})$
  Let $A$ be large enough so that $\forall n \, P(|X_n| > A) < \delta$, and let $n$ be large enough so that $P(|Y_n - Y| > \frac{epsilon}{A}) < \delta$.
  c/c: $P(|X_nY_n| > \epsilon) \le 2 \delta$ and $X_nY_n = o_p(1)$

\end{itemize}
\Q{7}
Since convergence a.s and in probability imply convergence in distribution, we only need to show that:
$$X_n \overset{D}{\rightarrow} X \Rightarrow E[g(X_n)] \rightarrow E[g(X)]$$

By Skorokhod representation theorem, there exist $Y_n \overset{D}{\sim} X_n$, $Y \overset{D}{\sim} X$ such that $Y_n \rightarrow Y$ a.s.
Let $\Omega$ be the set where $g$ is continuous, we know that $P_Y(\Omega) = P_X(\Omega) = 1$. 
Let $\omega \in \Omega$, $t_o = Y(\omega)$ and $\epsilon > 0$.
\begin{itemize}
\item 
$g$ is continuous on $t_0$, so there exist a $\delta > 0$, so that for $|t - t_0| < \delta$, $|g(t) - g(t_0)| < \epsilon$.
\item 
Let $N \in \mathbb{N}$ so that for $n > N$
$|Y_n(\omega) - Y(\omega)| < \delta$, and therefore $g(Y_n(\omega)) - g(Y(\omega))| < \epsilon$.
\end{itemize}
As a conclusion $g(Y_n) \rightarrow g(Y)$ a.s.
Since $g$ is bounded, by dominated convergence theorem we have that: $E[g(Y_n)] \rightarrow E[g(Y)]$, and since $E[g(Y_n)] = E[g(X_n)]$ and $E[g(Y)] = E[g(X)]$, we have the result.

\Q{8}
\begin{itemize}
\item Let \[y_i = \left( \begin{array}{c} x_i\\x_i^2\end{array} \right)\]


  $(y_i)_i$ are iid $L_2$ integrable r.v,
  $E[y_i] = (E[X], E[X^2])$
  \[ Cov(y_i) = \left( \begin{array}{cc} Var(X) & Cov(X, X^2)\\Cov(X,X^2) & Var(X^2)  \end{array}\right) \]
  by TCL $$\sqrt n (\bar y_n - E[y]) \overset{D}{\rightarrow} \mathcal N(0, Cov(Y))$$
  \begin{itemize}
  \item 
    $S_{n}^2 = \frac1n \sum x_i^2 - {\bar x}^2 = g(\bar y)$, where $g: (u, v) \rightarrow v - u^2$, $\nabla g = \icol{-2u\\1}$
  \item 
  $g(E[y]) = \sigma^2$
\end{itemize}  
\begin{itemize}
\item By the continuous mapping theorem:
  $ \sqrt n (S_n^2 - \sigma^2) \rightarrow \mathcal N (0, \nabla
  g_{E[y]}^T Cov(Y) \nabla g_{E[y]})$

\item Since $\frac n{n-1} \rightarrow 1$, by Slutsky:

  $ \sqrt n (S_{n-1}^2 - \frac n{n-1}\sigma^2) \rightarrow \mathcal N
  (0, \nabla g_{E[y]}^T Cov(Y) \nabla g_{E[y]})$

\item Since $\sqrt n (\sigma - \frac n{n-1}\sigma ) \rightarrow 0$, by Slutsky:

  $ \sqrt n (S_{n-1}^2 - \sigma^2) = \sqrt n (S_{n-1}^2 - \frac
  n{n-1}\sigma^2) + \sqrt n (\sigma - \frac n{n-1}\sigma ) \rightarrow
  \mathcal N (0, \nabla g_{E[y]}^T Cov(Y) \nabla g_{E[y]}) = \mathcal N
  (0, E[(X_1 - EX_1)^4] - \sigma^4)$
\end{itemize}

\item
  $z_i = \icol { x_i\\ y_i\\ x_iy_i}$ iid $L_2$, so $\sqrt{n}(\bar z_i - \icol { E[X]\\ E[Y] \\ E[XY]} \rightarrow \mathcal N(0, Cov(Z))$
  
  $\hat C_n = f(\bar z)$ where $f(u, v, t) = t - uv$ is differentiable and:
  $\nabla f = \icol{ -v\\ -u \\1 }$ is continuous

  $\sqrt{n}(\hat C_n - Cov(x_1,y_1)) \rightarrow \mathcal N(0, \nabla f^T cov(Z) \nabla f) $

  \[ cov(Z) = \left(
      \begin{array}{ccc}
        Var(X) & Cov(X, Y)& Cov(X, XY)\\
        Cov(X,Y) & Var(Y) & Cov(Y, XY\\
        Cov(X, XY) &  Cov(Y, XY)& Var(XY) \end{array}
    \right)
  \]
\end{itemize}

$$\nabla f = \icol{ -EY\\ -EX \\1 }$$
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:


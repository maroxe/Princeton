\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[ margin=1in]{geometry}

\newcommand{\hbeta}{\hat \beta }
\newcommand{\union}[1]{\underset{#1}{\cup} }
\newcommand{\bigunion}[1]{\underset{#1}{\bigcup} \, }
\newcommand{\inter}[1]{\underset{#1}{\cap} }
\newcommand{\biginter}[1]{\underset{#1}{\bigcap} }
\newcommand{\minimize}[3]{\optimize{#1}{#2}{#3}{min}}
\newcommand{\maximize}[3]{\optimize{#1}{#2}{#3}{max}}
\newcommand{\esp}{{\mathbb E}}
\newcommand{\pr}{{\mathbb P}}
\newcommand{\norm}[1]{\Vert #1 \Vert}
\newcommand{\fnorm}[1]{\Vert #1 \Vert_F}
\newcommand{\nucnorm}[1]{\Vert #1 \Vert_*}
\newcommand{\opnorm}[1]{\Vert #1 \Vert_{op}}
\newcommand{\inner}[2]{\langle #1 \, , \, #2 \rangle}
\newcommand{\row}[2]{\begin{pmatrix}#1 & #2\end{pmatrix}}


\begin{document}

\section*{P2}
\begin{itemize}
\item[2.1]
  Let's assume  $X\beta_1 \ne X\beta_2$.
  
  Let $f^*$ be the optimal value, $\alpha = \frac12$, $\beta_{\alpha} = \alpha \beta_1 + (1-\alpha) \beta_2$.
  Then, by the convexity of $\norm{.}_2^2, \norm{.}_1$:
  \begin{align*}
    f^* &\le ||Y - X \beta_{\alpha}||_2^2 + \lambda ||\beta_{\alpha}||_1
    \\&= \norm{ \alpha (Y - X \beta_1) + (1-\alpha)(Y - X\beta_2)}_2^2
        + \lambda \norm{\alpha \beta_1 + (1-\alpha)\beta_2}_1
    \\&<
        \alpha \left(||Y - X \beta_1||_2^2 + \lambda ||\beta_1||_1\right)
        + (1-\alpha)\left(||Y - X \beta_2||_2^2 + \lambda ||\beta_2||_1\right)p
        &\text{(By strict convexity of $\norm{.}_2^2$)}
    \\&\le f^*
  \end{align*}
  Contradicition.
  
\item[2.2]
  ${\cal L}(\beta^*, \lambda) = \frac12 \norm{Y-X\beta}_2^2 + \lambda \norm{\beta}_1$

  $\partial \norm{\beta}_1 = \{ \alpha \in [-1, 1]^n, \alpha_j = sign(\hat \beta_j) \text{ when } \hat \beta_j \ne 0\}$
  
  Let $(\beta^*, \lambda^*)$ be an optimal solution, then $0 \in \partial_{\lambda} L (\beta^*, \lambda^*)$

  We know that $\partial_{\lambda^*} L (\beta, \lambda^*) =  -X^T(Y-X\beta) +  \lambda^* \partial \norm{\beta}_1$.

  Coordinate wise, this gives for all $j$:
  
  $ X_j^T (Y - X\beta) = \lambda sign(\beta_j)$ if $\beta_j \ne 0$
  
  $ - X (Y - X\beta) = \lambda \alpha_i$ if $\beta_j = 0$ with $\alpha_i \in [0, 1]$
  
  e.g
  
  $ \lambda^* = -sign(\beta_j^*) X_j^T(Y - X\beta^*)$ if $\beta_j^* \ne 0$
  
  $ \lambda^* \ge | X_j^T(Y - X\beta^*)|$ if $\beta_j^* = 0$
  

  
\item[2.3]
  Let $\hat \beta$ be an optimal solution. Let $\chi = \{ j, \hat \beta_j \ne 0 \}$, and let's suppose it is non empty.   Let $j \in \chi$.

  \begin{itemize}
  \item If $\hat \beta_j > 0$, then by 2.2,   $\lambda = X_j^T (Y - X \hat \beta)$, but since  $\lambda > \norm{X^TY}_{\infty} \ge X_j^TY$, we have that  $X_j^TX\hat \beta > 0$.
  \item   Similarly, if there for $j$ such that $\hat \beta_j < 0$,  $X_j^TX\hat \beta < 0$.
  \item  As a conclusion $\beta_j \ne 0 \implies \beta_j X_j^TX\hat \beta > 0$
  \end{itemize}
  
  \begin{align*}
    \frac12 \norm{Y - X\beta}_2^2 + \lambda \norm{\beta}_1
    &= \frac12 \norm{Y}_2^2 - \hat \beta^TX^TY + \frac12 \beta^T X^TX \hat \beta +  \lambda \sum_{i \in \chi} |\hat \beta_i|
    \\&=\frac12 \norm{Y}_2^2 +
        \sum_{i \in \chi} |\hat \beta_i| (\lambda - sign(\beta_i) X_i^TY)
        + \frac12  \sum_{i \in \chi} \hat\beta_i X_i^TX\hat \beta
    \\&\ge \frac12 \norm{Y}_2^2 +
        \sum_{i \in \chi} |\hat \beta_i| \underbrace{(\lambda - |X_i^TY|)}_{\ge 0}
        + \frac12  \underbrace{\sum_{i \in \chi} \hat\beta_i X_i^TX\hat \beta}_{> 0}
    \\&> \frac12 \norm{Y}_2^2
    \\&= \frac12 \norm{Y - X0}_2^2 + \lambda\norm{0}_1
  \end{align*}
  Contradiction, so $\hat \beta = 0$.
\item[2.4]
  $$\lambda \in [\lambda_0, \lambda_1]$$
  Let $\chi(\lambda) = \{ j, \hat \beta_j(\lambda) \ne 0 \} := \chi$, $r = |\chi|$ (doesn't depend on $\lambda$ by assumption)

  We have proved in 2.2 that there exist  $\alpha(\lambda) \in \partial \norm{\hat \beta(\lambda)}_1$ such that:
  $$X^T(Y - X\hat \beta(\lambda)) = \lambda \alpha(\lambda) $$


  It is easy to see that this KKT conditions is actualy necessary and sufficient (because we are minimizing a convexe function), since we are assuming uniqueness, $\hat \beta(\lambda)$ is the unique solution to :
  $$(\exists \alpha(\lambda) \in \partial \norm{\hat \beta(\lambda)}_1) \; X^T(Y - X\hat \beta(\lambda)) = \lambda \alpha(\lambda) $$
  
  Note that by uniqueness of $X\beta$ and $\hat \beta(\lambda)$, $\alpha(\lambda)$ is unique when $\lambda > 0$.
  
  Note also, that since we assumed that the signs and support are unchanged, $\partial \norm{\hat \beta(\lambda)}_1 = \partial \norm{\hat \beta(\lambda_0)}_1$.
  
  The last condition becomes:
  $$X^T(Y - X\hat \beta(\lambda)) \in \lambda \partial \norm{\hat \beta(\lambda_0)}_1$$


  \textbf{Notation: } $ \alpha(\lambda_0) =  X^T \underbrace{\frac{(Y - X\hat \beta(\lambda_0))}{\lambda_0}}_v = X^Tv$,  $\gamma_0 = X^{\dagger} v$, $\delta = \hat \beta(\lambda_0) - (\lambda - \lambda_0) \gamma_0$. 

  Note that:
  $$X^TX\gamma_0 = X^T XX^{\dagger} v = (V \Lambda U^T) (U\Lambda V^T)  (V \Lambda^{-1} U^T) v = V \Lambda U^T v = X^T v = \alpha(\lambda_0)$$
  
  
  \begin{align*}
    X^T(Y - X\delta)
    &= \underbrace{X^T(Y - X\hat \beta(\lambda_0))}_{\lambda_0 \alpha(\lambda_0)} + (\lambda - \lambda_0) \underbrace{X^T X \alpha_0}_{\alpha(\lambda_0)}
    \\&= \lambda \alpha(\lambda_0) \in \lambda \partial \norm{\hat \beta(\lambda_0)}_1
  \end{align*}
  Which proves that $\hat \beta(\lambda) = \delta = \hat \beta(\lambda_0) - (\lambda - \lambda_0) \alpha(\lambda_0)$

  
\item
  \textbf{Notation}: For a vector $v$, let $v^+ = \max(v, 0), v^- = -\min(-v, 0)$, $sign(v), supp(v)$ the sign and support of $v$, $\phi(v) = (supp(v^+), supp(v^-))$
  
  The number of values $\phi(v)$ can take is finite and at most $n^2$ because $\phi(v) \in \mathcal P(\{1 \ldots n\})^2$.

  Notice that in the last part, we have proven a stronger result:
  if for $\lambda_1, \lambda_2$, $\phi(\beta(\lambda_1)) = \phi(\beta(\lambda_2))$, then $\beta(\lambda_2) = \beta(\lambda_1) - (\lambda_2 - \lambda_1) \gamma_0$, where $\gamma_0$ depend only on $\lambda_1$. This proves a segment of the path $C$ is fully caracterized by $\phi(v(C))$ where $v(C)$ is one of the element of $C$ chosen arbitrarly.
  

  Let $\mathcal A$ denote the set of segments that form the lasso path, and consider the following application:
  
  ${\cal A} \rightarrow {\cal B}; C \rightarrow \phi(v(C)) $  

  We have proven that this application is injective, so $|{\cal A}| \le n^2 < \infty $. Which proves that the number of segments in the lasso path is finite.
  Let $\lambda_0$ be small enough so that $(0, \lambda_0]$ corresponds to last segment, and let $0 < \lambda < \lambda_0$
  then $\hbeta(\lambda) = \hbeta(\lambda_0) - (\lambda - \lambda_0)\gamma_0$.

  This proves that the $\hbeta(\lambda)$ has $\hbeta(\lambda_0) + \lambda_0 \gamma_0$ as limit at $0^+$. Let's call $\hbeta$ that limit.
  
  Recall the definition of $\gamma_0$: $X^{\dagger} \frac{(Y - X\hat \beta(\lambda_0))}{\lambda_0}$, and sine $X$ is full rank, $XX^{\dagger} = I_n$. So $X\hbeta = \hbeta(\lambda_0) +  XX^{\dagger}(Y - X\hbeta(\lambda_0)) = Y$

  Suppose by contradiction that $\hbeta \ne \hbeta^{CS}$, e.g  $\norm{\hbeta^{CS}}_1 < \norm{\hbeta}_1$.

  By continuity of norms in finite dimensional space, $\norm{\hbeta(\lambda)}_1 \rightarrow \norm{\hbeta}_1$. Let $\lambda$ be small enought so that
  $$ \norm{\hbeta^{CS}}_1 < \norm{\hbeta(\lambda)} $$
  Which would imply that   $ \lambda \norm{\hbeta^{CS}}_1 < \lambda \norm{\hbeta(\lambda)} + \norm{Y - X\hbeta(\lambda)}_2^2$, which contradicts the minimality of $\hbeta(\lambda)$.
  
  Conclusion:  $\hbeta = \hbeta^{CS}$
  
\end{itemize}

\section*{P3}
\begin{itemize}
\item[3.1]

  Let's consider the unconstrained optimization problem:
  $$\min ||Y - X \beta||^2$$
  $\beta$ is optimal iff $X^TY = X^TX\beta$.
  
  We check easily that $(X^TX)^{\dagger}X^TY$ is a solution to the last equation, therefore it minimizes the $L_2$ risk.
  
  If $t > \norm{(X^TX)^{\dagger}X^TY}_{L_1}$, then it is also solution to the following problem:   $\min_{\norm{\beta}_{L_1} \le t} ||Y - X \beta||^2$.
  
\item
  We adopt the following  notations:
\begin{itemize}
  
\item $$\gamma = (-1, \beta)^T, |\gamma|_{1} = |\beta| + 1, Z_i = (Y_i, X_i^T)^T$$
  
\item $$R(\beta) = \gamma^T \Sigma \gamma$$
  
\item $$\hat R(\beta) = \gamma^T \hat \Sigma \gamma$$
  
\item $$\hat R^{(V_k)}(\beta) := \frac{1}{|V_K|}\sum_{i \in V_k} (Y_i - X_i^T\beta)^2 = \gamma^T \hat \Sigma^{V_k} \gamma$$
  
\item $$\hat R^{(-V_k)}(\beta) := \sum_{j \ne k} \frac{1}{|V_j|} \sum_{i \in V_j} (Y_i - X_i^T\beta)^2 = \gamma^T \underbrace{(\hat \Sigma - \hat \Sigma^{V_k})}_{\hat \Sigma^{(-V_k)}} \gamma$$
  Note that $|Z_i| \le 2 b, |Z_iZ_i^T| \le 4b^2$ , by Hoeffding inequality:
  \begin{itemize}
  \item $$\pr(||\Sigma - \hat \Sigma||_{\infty} \ge \varepsilon) \le 2 \exp(\frac{-n\varepsilon^2}{8b^4})$$

  \item $$\pr(||\Sigma - \hat \Sigma^{(V_k)}||_{\infty} \ge \varepsilon) \le 2 \exp(\frac{-|V_k|\varepsilon^2}{8b^4})$$
  \item $$\pr(||\Sigma - \hat \Sigma^{(-V_k)}||_{\infty} \ge \varepsilon) \le 2 \exp(\frac{-(n-|V_k|)\varepsilon^2}{8b^4}) \le 2 \exp(\frac{-|V_k|\varepsilon^2}{8b^4})$$
  
  \item $$\pr(||\hat \Sigma - \hat \Sigma^{(V_k)}||_{\infty} \ge \varepsilon) \le \pr(||\Sigma - \hat \Sigma^{(V_k)}||_{\infty} \ge \frac{\varepsilon}2) + \pr(||\Sigma - \hat \Sigma||_{\infty} \ge \frac{\varepsilon}2) \le 4 \exp(\frac{-|V_k| \varepsilon^2}{32b^4})$$

  
  \item $$\pr(||\hat \Sigma^{(-V_k)} - \hat \Sigma^{(V_k)}||_{\infty} \ge \varepsilon) \le \pr(||\Sigma - \hat \Sigma^{(V_k)}||_{\infty} \ge \frac{\varepsilon}2) + \pr(||\Sigma - \hat \Sigma^{(-V_k)}||_{\infty} \ge \frac{\varepsilon}2) \le 4 \exp(\frac{-|V_k| \varepsilon^2}{32b^4})$$
  \end{itemize}
  \end{itemize}
  1.)

    $$R(\hbeta_{\hat t}) - \hat R_{CV}(\hat t)
    =  R(\hbeta_{\hat t}) - \hat R(\hbeta_{\hat t}) + \hat R(\hbeta_{\hat t}) - \hat R_{CV}(\hat t)$$


  $$R(\hbeta_{\hat t}) - \hat R(\hbeta_{\hat t}) = \hbeta_{\hat t} (\Sigma - \hat \Sigma) \hbeta_{\hat t} \le ||\Sigma - \hat \Sigma||_{\infty} |\gamma|_1 \le ||\Sigma - \hat \Sigma||_{\infty} (1+t_n)$$

  \begin{align*}
    \hat R(\hbeta_{\hat t}) - \hat R_{CV}(\hat t)
    &= \frac 1K \sum_k (\hat R(\hbeta_{\hat t}) - \hat R^{V_k}(\hbeta_{\hat t}^{(V_k)})
    \\&\le \frac 1K \sum_k \hat R(\hbeta^{V_k}_{\hat t}) - \hat R^{V_k}(\hbeta_{\hat t}^{(V_k)}
    \\&= \frac 1K \sum \hat (\gamma^{V_k}_{\hat t})^T (\hat \Sigma - \hat \Sigma^{V_k})\hat \gamma^{V_k}_{\hat t}
    \\&\le \frac1K |\gamma^{V_k}_{\hat t}|_{L_1}^2 \sum ||\hat \Sigma - \hat \Sigma^{V_k}||_{\infty}
    \\&\le \frac1K (1+t_n)^2 \sum ||\hat \Sigma - \hat \Sigma^{V_k}||_{\infty}
  \end{align*}

  Using the property $\pr(\frac1K \sum_k Y_i \ge \varepsilon) \le \sum_k \pr(Y_i \ge \varepsilon)$:
  
  \begin{align*}
    \mathbb P ( (i) \ge \varepsilon)
    &\le \mathbb P( ||\Sigma - \hat \Sigma||_{\infty} \ge \frac{\varepsilon}{(1+t_n)^2}) + \sum_{k} \pr( \frac1K ||\hat \Sigma - \hat \Sigma^{(V_k)}||_{\infty} \ge \frac{\varepsilon}{(1+t_n)^2K})
    \\& \le 2 \exp(\frac{-n \varepsilon^2}{16 b^4 (1+t_n^2)^2}) + 2K \exp(\frac{-|V_1| \varepsilon^2}{16 b^4 (1+t_n^2)^2})
    \\&\le 2(1+K) \exp(\frac{-|V_1| \varepsilon^2}{16 b^4 (1+t_n^2)^2})
    \\&\le C_1 K \exp(\exp(\frac{-|V_1| \varepsilon^2}{C_2 b^4 (1+t_n^2)^2})
  \end{align*}
  Where $C_1 \ge 4, C_2 \ge 32$
  
  2.)
  By definition of $\hat t$:
  $$\hat R_{CV}(\hat t) - \hat R_{CV}(t_{\max}) \le 0$$

  3.)
  \begin{align*}&\hat R_{CV}(t_{\max} ) - \hat R(\hbeta_{t_{\max}})
    \\&= \frac1K \sum_{k} \hat R^{V_k}(\hat \beta^{V_k}_{t_{\max}}) - \hat R(\hat \beta_{t_{\max}})
    \\&= \frac1K \sum_{k} \hat R^{V_k}(\hat \beta^{V_k}_{t_{\max}}) + R^{-V_k}(\hat \beta^{V_k}_{t_{\max}}) + \underbrace{R^{-V_k}(\hat \beta^{V_k}_{t_{\max}}) -  \hat R^{-V_k}(\hat \beta_{t_{\max}}) }_{\le 0} - \hat R^{V_k}(\hat \beta_{t_{\max}})
    \\&\le \frac1K \sum_{k} \hat R^{V_k}(\hat \beta^{V_k}_{t_{\max}}) - R^{-V_k}(\hat \beta^{V_k}_{t_{\max}})
    \\&= \frac1K \sum_{k}  (\hat \gamma^{V_k}_{t_{\max}})^T(\hat \Sigma^{V_k} - \hat \Sigma^{-V_k})\hat \gamma^{V_k}_{t_{\max}}
    \\&\le \frac{(1+t_n)^2}K \sum_k ||\hat \Sigma^{V_k} - \hat \Sigma^{-V_k}||_{\infty}
  \end{align*}

  \begin{align*}\pr( iii \ge \varepsilon)
    &\le K \pr(||\hat \Sigma^{V_1} - \hat \Sigma^{-V_1}||_{\infty} \ge \frac{\varepsilon}{(1+t_n)^2})
    \\&\le 4K exp(\frac{-|V_1|\varepsilon^2}{32(1+t_n)^2b^4})
    \\&\le C_1K exp(\frac{-|V_1|\varepsilon^2}{C_2(1+t_n)^2b^4})
    \end{align*}

  4.)
  
  $$t_{\max} \le t_n \implies \hat R(\hbeta_{t_{\max}}) \le \hat R(\hbeta_{t_n})$$
  
  5. and 6.) 
  \begin{align*}
    \hat R(\hbeta_{t_n}) - R(\beta_{t_n})
    \\&\le\hat R(\beta_{t_n}) - R(\beta_{t_n})
    \\&\le  \gamma_{t_n}(\hat \Sigma  - \Sigma) \gamma_{t_n}
    \\&\le (1+t_n)^2|\hat \Sigma  - \Sigma|_{\infty}
  \end{align*}
  
  \begin{align*}
    \pr(v + vi \ge \varepsilon) &\le \mathbb P( ||\Sigma - \hat \Sigma||_{\infty} \ge \frac{\varepsilon}{(1+t_n)^2})
    \\&\le 2 \exp(-\frac{n \varepsilon^2}{8b^4(1+t_n)^2})
    \\&\le C_1 K \exp(-\frac{|V_1| \varepsilon^2}{C_2b^4(1+t_n)^2})
  \end{align*}
  
  \begin{align*}
    \pr( i + ii + iii + iv + v + vi \ge \varepsilon)
    &= \pr( i \ge \varepsilon/ 3) + \pr( i \ge \varepsilon / 3) + \pr(v + vi \ge \varepsilon /3)
    \\&\le 3 C_1 K \exp(-\frac{|V_1| \varepsilon^2}{9 C_2 b^4(1+t_n)^2})
    \\&\le C K \exp(-\frac{n\varepsilon^2}{CK (1+t_n)^2})
  \end{align*}
  Where $C \ge \max(3C_1, 9C_2b^4)$

  Which gives the result by setting $\delta$ to $C K \exp(-\frac{n\varepsilon^2}{CK (1+t_n)^2})$, or equivalently setting $\varepsilon$ to $C(1+t_n^2) \sqrt{\log (\frac{CK}{\delta}) K/ n}$
  
\end{itemize}



\end{document}



















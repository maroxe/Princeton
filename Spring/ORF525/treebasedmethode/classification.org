#+HTML_HEAD:    <link rel="stylesheet" type="text/css" href="./org-style.css" />
#+HTML_HEAD:    <link rel="stylesheet" type="text/css" href="./special-block.css" />
#+latex_header: \usepackage{amsthm}
#+latex_header: \newtheorem{theorem}{Theorem}
#+latex_header: \newtheorem{definition}{Definition}

#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t broken-links:nil c:nil creator:nil
#+OPTIONS: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+OPTIONS: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+OPTIONS: timestamp:t title:t toc:t todo:t |:t
#+TITLE: ORF525 notes
#+DATE: <2016-03-10 Thu>
#+AUTHOR: Bachir El Khadir
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Bachir El Khadir



* CLassification

#+begin_definition
Classification: Regression with categorical response $Y \in \{ C_1, \ldots, C_k \}$
#+end_definition


\begin{math}
h^*(x) = \left\{
\begin{array}{cc}
1 & \text{if } P(Y = 1 | X = x) > \frac12\\
-1 & \text{o.w.}
\end{array}
\right.
\end{math}

** Modeling $P(Y = 1 | X = x)$
- Logisitic Model (the best)
  $$P(Y = y | X = x) = \frac1{1 + e^{-yf(x)}}$$

- $f(x) = \beta^Tx$



** SVM
   
   #+begin_definition
   $$\hat \beta^{\lambda} = \arg\min \sum_{i=1}^n (1 - Y_i \beta^T X_i)^+ + \lambda ||\beta||_2$$
   #+end_definition
*Remark*: This is just Ridge Logistic Regerssion but replace the loss by the "Hinge" loss.
In general SVM > LR > LDA.

** Perceptron
   #+begin_definition
   $$\hat \beta = \arg\min \sum_{i=1}^n ( - Y_i \beta^T X_i)^+ $$
   #+end_definition

ALl these methods are discrimnative classifiers: $P(Y = 1 | X = x)$ directly model the discrimanant functions

** ROC Curve: For visualizing and Evaluating classifiers
Motivation: Unbalanced class / Nayman Pearson Paradigm
Corcia

$P(Y = 1 | X = x) > t \implies +1$
$P(Y = 1 | X = x) \le t \implies -1$


#+begin_definition
Confusion matrix:

|        | predict  | +1 | -1 |   |
| actual |          |    |    |   |
|     +1 |          | TP | FN |   |
|     -1 |          | FP | TN |   |
#+end_definition

#+begin_definition
Sensitivity: $\frac{TP}{TP + FN}$
Specificity: $\frac{TN}{TN + FP}$
ROC curve: $Sensitivity = f(\underbrace{FPR}_{1 - Sepcificity} )$ for different values of $t$.
#+end_definition

- ROC curve is invariant to class skewness.
- The diagonal: random guess

* Generative Model
  Recall Logistic Model: $P(Y = 1  |X = x) = \frac{1}{1+\exp(-Yf(X))}$
  Another way to do this model $P(Y | X = x)$ si by Bayes formula:
  $P(Y = 1| X = x) = \frac{P(X = x | Y = 1) }{p(x | Y = 1) + \frac{P(Y = -1)}{P(Y = 1)} p(x | Y = -1)}$
  Then We model $X | Y = 1$, $X | Y = -1$
  Examples:
  - QDA: $X|Y=1 \sim \mathcal N(\mu^+, \Sigma^+)$,  $X|Y=-1 \sim \mathcal N(\mu^-, \Sigma^-)$
  - LDA:  $X|Y=1 \sim \mathcal N(\mu^+, \Sigma)$. $X|Y=-1 \sim \mathcal N(\mu^-, \Sigma)$
  - DLAD: same as LDA, bu $\Sigma = diag(\sigma_1^2, \ldots, \sigma_p^2)$

** Decision Boundary of QDA
$Y \sim Ber(\eta)$
$\log P(Y = 1 | X = x) = \log P(Y = -1 | X = x)$
$\frac12 r_-^2(x) - \frac12 r_+^2(x) + \frac12 \log \frac{|\Sigma_-}{|\Sigma_+} + \log \frac {\eta}{1 - \eta} = 0$
  
$r_-^2(x) = (x - \mu_-)'\Sigma_-^{-1}(x - \mu_-)$

* Generative vs Dsicriminative Model
  - Discriminative: $p(y, x) = \underbrace{p(y | x)}_{\text{only model this part}} \underbrace{p(x)}_{\text{ignored}}$: Directly relevant to classification.
  - Generative: $p(y, x) = \underbrace{p(x | y)p(y)}_{\text{Model both}}$ 
** Generative model in High Dimension
   - Answer1: LDA <-> L2-regression (hw1) and add regularization
   - Answer2: Naive bayes: class conditionla independence regularization
     $\log(p(x | Y = i) = \prod_{j=1}^d p(x_j | Y = i)$
     $$\log \frac{p(x | Y = 1)}{p(x | Y = -1)} = \sum_{j=1}^d \log \frac{p(x_j | Y = 1)}{p(x_j | Y = -1)} + \log \frac{\eta}{1 - \eta} = \sum_{j=1}^d f_j(x_j) + \beta_0$$
     
* GLM: Generazed Linear Model

  #+BEGIN_DEFINITION
  Exponential dispersion family (univariate)
  $$p_{\theta, \tau}(y) = h_{\tau}(y) e^{\frac{\theta y - A(\theta)}{\tau}}$$
 #+END_DEFINITION
  

Components:
- Stochastic $Y_i \sim P_i$, $\mu_i = E[Y_i]$
- Systematic: $\eta_i = \beta^T X_i$
- Link: $\eta_i = g(\mu_i)$

#+BEGIN_DEFINITION 
Canonical Link Principle:
We want to choose a link $g$ s.t $\underbrace{\eta_i}_{\beta^T X_i} = \theta_i$
#+END_DEFINITION
Queston: How do we actually characterise the canonical link?
Answer: $g(\mu) = (A')^{-1}(\mu)$



* Gaussian Graphical Model
** Problem Setup
   Let $X_1, \ldots, X_n$ be random samples from a multivariate $X \sim \mathcal N(\mu, \Sigma)$ We aim to estimate the precision matrix: $\Theta = \Sigma^{-1}$.
   We construct an undericted graph $G = (V, E)$ accoriding to $j, k \in E \iff \Theta_{jk} \ne 0$
   Question: Given $X_1 \ldots X_n$ How shall we estimate $G$ ( and $G$)
   Answer: MLE (MPLE, P for $L_1$ -regularization)
   The joint likelihood:
   $$L_n(\Theta) = \log [(2\pi)^{\frac{-nd}2}|\Theta|^{\frac n2} e^{-\sum_{i=1}^n (X_i - \bar X)'\Theta(X_i - \bar X)}]$$

   $\Theta_{jk} = 0 \iff X_j \perp X_k | X \setminus \{j, k\}$
   
   #+name: Graph separation
   #+BEGIN_DEFINITION 
   Given $G = (V, E)$, let $A, B, C \subset V$. We say $C$ separates $A$ and $B$ if all paths between nodes in $A$ and $B$ pass through at least on node in $C$ ($A \perp B | C$)
   #+END_DEFINITION


   #+name: Pairwise Markov Property (PMP)
   #+BEGIN_DEFINITION 
   $$\underbrace{X_j \perp X_k | X \setminus \{j, j\}}_{\text{probability}} \iff \underbrace{(j, k) \not \in E}_{\text{Graph}}$$
   #+END_DEFINITION
   Gaussian graphical model satisfies the pairwise Markov Property. 

   #+name: Global Markov Property (GMP)
   #+BEGIN_DEFINITION 
   $\forall A, B, C \subset V, X_A \perp X_B | X_C \iff A \perp B | C$
   #+END_DEFINITION


   #+BEGIN_THEOREM 
   If $\forall x, p(x) > 0$, then PMP $\implies$ GMP
   #+END_THEOREM

   Corrolary: Guassian Graphical Model is GMP

   By GMP, $p(x_1 | X \setminus 1) = p(x_1 | \underbrace{X_{Nb(1)}}_{\text{All the neighbors of node } 1})$
   Question: Finding the neighbourhood of $X_1$

   $X = \begin{pmatrix}X_1 \in \mathbb R \\ X_2 \in \mathbb R^{d-1}\end{pmatrix} \sim \mathcal N( \begin{pmatrix}\mu_1\\mu_2\end{pmatrix}, \begin{pmatrix}\Sigma_{11}&\Sigma_{12}\\\Sigma_{21}&\Sigma_{22}\end{pmatrix})$
   $X_1 | X_2 \sim \beta_0 + \beta^T X_2 + \varepsilon_1$, $\varepsilon_1 \sim \mathcal N(0, \Theta_11^{-1})$
   $supp(\beta) = nb(1)$
   Use Lasso to estimate $supp(\beta)$ to get $nb(1), \ldots, nb(d)$, then erge them using the AND rule.
   
** Ising Graphical Model
   $X_1, \ldots, X_n \in \{+1, -2\}$, Estimate the graph?
   Natural Idea: regress $X_j$ on $X \setminus j$ using sparse LR to get nb(j).
   Merge using AND rule.
   
   Justification: works under the Ising model

   #+name: Ising model
   #+BEGIN_DEFINITION 
   $p(x) \propto e^{\sum \beta_j X_j + \sum_{k < l} \beta_{kl} X_k X_l}$
   #+END_DEFINITION

   #+BEGIN_THEOREM
   $\beta_{kl} = 0 \iff X_k \perp X_l | X \setminus \{k, l\}$
   #+END_THEOREM

Generalized Linear graphical Model


** EM Algorithm

   Steps:
   #+BEGIN_ALGORITHM 
   1. Init randomly $\psi_0$
   2. Come up with $F_{\psi_0}$ locally equal to $l$ around $\psi_0$, and smaller than $l$ (By Jensen for model mixture $\sum\log\sum \ge \log\sum\sum$)
   3. Optimize $\psi_1 = \arg\max F_{\psi_0}$
   4. Loop
   #+END_ALGORITHM
   $$l(\psi^{t}) = F_{\psi^t}(\psi^t) \le F_{\psi^t}(\psi^{t+1}) \le l(\psi^{t+1})$$

   Corollary: If $l$ is bounded, EM algorithm converges

   \begin{align*}
   l(\psi) &= \sum_{i=1}^n \log p_\psi(X_i)
   \\&= \sum_{i=1}^n \log \sum_j p_{\psi_t}(z_j | X_i) \frac{p_\psi(X_i,z_j)}{p_{\psi_t}(z_j | X_i)}
   \\&\ge \sum_{i=1}^n \sum_j p_{\psi_t}(z_j | X_i) \log \frac{p_\psi(X_i,z_j)}{p_{\psi_t}(z_j | X_i)}& \text{(Jensen)}
   \\&= \sum_{i=1}^n \sum_j r_i(z_j) \log \frac{p_\psi(X_i,z_j)}{r_i(z_j)}
   \\&= F_{\psi_t}(\psi)
   \end{align*}

   $F_{\psi_t}(\psi_t) = \sum_{i=1}^n \sum_j r_i(z_j) \log p_{\psi^t}(X_i)$


** Example: (EM on mixture model)
$Z \sim Multi(1, \eta_0, \ldots, \eta_{k-1})$
$X | Z=j \sim P_{\theta_j}(x)$
$\Psi = (\eta, \theta)$
$$p_{\psi}(x) = \sum_{j=0}^{k-1} \eta_j p_{\theta}(x)$$

- E-Step: $$r_{ij}^(t) = P_{\psi^t}(Z_i = j | X_i) = \frac{p_{\theta^t}(X_i) \eta_j^t}{\sum_{l=0}^{K-1}p_{\theta_l^t}(X_i) \eta_l^t}$$
- M-Step:
  -- $$\theta_j^{(t+1)} \leftarrow \arg\max_{\theta_j} \sum_{i=0}^n \sum_{j=0}^{k-1}r_{ij}^{(t+1)} \log p_{\theta_j}(X_i)$$
  -- $$\eta_j^{(t+1)} \leftarrow \arg\max_{\sum \eta_j = 1} \sum_{i=0}^n \sum_{j=0}^{k-1}r_{ij}^{(t+1)} \log \eta_j \implies \eta_j \propto \sum_i r_{ij}^{(t+1)}$$
  
** Examaple: (Mixture of Gaussians)
   $Z \sim Multi(1, \eta_0, \ldots eta_{k-1})$
   $X | Z = j \sim \mathcal N(\mu_j, \Sigma_j)$

   - E-step:   $r_{ij}^{(t+1)} \propto p_{\mu_j^t, \Sigma_j^t}(X_i)$(and sum to 1)
   - M-step:
     -- Same as before $$\eta_j^{(t+1)} \leftarrow \frac{\sum_{i=1}^n r_{ij}^{t+1}}n$$
     -- $$\mu_j^{(t+1)} \leftarrow \frac{\sum_{i=1}^n r_{ij}^{(t+1)X_i}}{\sum_{i=1}^n r_{ij}^{(t+1)}}$$
     -- $$\Sigma _j^{t+1} \leftarrow \frac{ \sum_{i=1}^n r_{ij}^{t+1}(X_i - \mu_j^{t+1})(X_i - \mu_j^{t+1})'}{\sum_{i=1}^n r_{ij}^{t+1}}$$
** Subtle properties of Latent Variable Models   
1. Infinite Likelihood $X_1 \ldots X_n \sim \sum_{j=0}^{K-1} \eta_j \Phi(X, \mu_j, \sigma_j^2)$
   $\max_{\psi} L_{\psi}(X_1, \ldots, X_n) = \infty$ if $X_1 is the only point in cluster 1
2. Not identifiable
3. Irregular

* K-mean
  #+BEGIN_Definition 
  K-means algorithms is defined as the limitng procedure by applying EM on a sequence of degenerate mixture of $K$ -Gaussian
  #+END_Definition

  $\mathcal P = \{ P(x) = \sum \eta_j p_{\mu_j, \Sigma_j}(x)\}$
  $\mathcal P_{\sigma} =\{ p \in \mathcal P, \Sigma_0=\ldots=\Sigma_{k-1} = \sigma^2 I\}$, and let $\sigma^2 \rightarrow 0$

  ** An empirical risk minimization of K-means
  $$\hat R(\mu_0, \ldots, \mu_{k-1}) = \frac1n \sum_{i=1}^n \min_{j \in [k]}||X_i - \mu_j||^2$$

  Equivalent formulation:
  
  $$\min_{A_i \in [K]^n, \mu_0, \ldots, \mu_n} frac1n \sum_{i}^n ||X_i - \mu_{A_i}||_2^2$$
  
  Block coordinate descent:
  
  Iterate:
  - *step 1:* Given $\mu_0, \ldots \mu_{k-1}$ update $A_i, i=1\ldotsn$
  - *step 2:* Given $A_i$, update $\mu_0, \ldots, \mu_{k-1}$


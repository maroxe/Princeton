#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../../css/special-block.css" />
#+HTML_HEAD: <link href="http://thomasf.github.io/solarized-css/solarized-dark.min.css" rel="stylesheet"></link>
#+HTML_HEAD: <script type="text/javascript" src="http://code.jquery.com/jquery-latest.min.js"></script>
#+HTML_HEAD: <script src="http://127.0.0.1:60000/autoreload.js"></script>
#+LATEX_HEADER: \usepackage{pdfpages}
#+LATEX_HEADER: \newcommand{\inner}[2]{\langle #1 \, , \, #2 \rangle}
#+LATEX_HEADER: \newcommand{\norm}[1]{\Vert #1 \Vert}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+OPTIONS: toc:nil  

#+TITLE: Problem set 3, ORF525
#+DATE: <2016-03-09 Wed>
#+AUTHOR: Bachir El khadir

#+name: Watch changes
#+BEGIN_HTML 
@@html:<script>@@
@@html:AutoReload.Watch('localhost:60000');@@
@@html:</script>@@
#+END_HTML


* Codes                                                            :noexport:

#+BEGIN_SRC emacs-lisp :exports none
(defun add-caption-header-and-center (caption header )
  (concat (format "org\n#+ATTR_LATEX: :float nil\n#+attr_html: :class center\n#+caption: %s\n%s" caption header)))

  (defun add-caption-and-center (caption)
    (concat (format "org\n#+attr_html: :class center\n#+caption: %s" caption)))

#+END_SRC

#+RESULTS:
: add-caption-and-center





#+BEGIN_SRC R :session :exports none :cache yes
library(dplyr)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(pander)
library(randomForest)

set.seed(525)
#+END_SRC

#+RESULTS[19a76ddbaedbb14e8db38dfd22594da7e3c85b8d]:



* Money Ball: Who You Will Buy?

a)

#+name: loaddata
#+BEGIN_SRC R  :session  :cache yes
data <- tbl_df(read.csv('MLB2008.csv'))
data.train <- data[1:154, ]
data.test <- data[155:dim(data)[1], ]
0
#+END_SRC

#+RESULTS[b7f7da921aecb8dfaa0cdafa7c8241817280ad64]: loaddata
: 0




#+name: treemodel
#+BEGIN_SRC R :session :cache yes :results graphics :file img/tree.png :exports both :wrap (add-caption-and-center "Tree generated by rpart")
feature.names <- colnames(data)[6:length(data)]
formula <- as.formula(paste('SALARY ~', paste(feature.names, collapse='+')))
rpart.model <- rpart(formula, data=data.train, method='anova')
prp(rpart.model)
#+END_SRC

#+RESULTS[208d6a861a6f5c2037a5091f5185c7b2748cdcc6]: treemodel
#+BEGIN_org
#+attr_html: :class center
#+caption: Tree generated by rpart
[[file:img/tree.png]]
#+END_org


Now we try the pruning of the tree:

#+name: Prune and MSE
#+BEGIN_SRC R :session  :exports both :cache yes :wrap (add-caption-header-and-center "MSE" "|Prune Parameter|train|test|")
mse <- data.frame(
    prune=0,
    train=norm(predict(rpart.model) - data.train$SALARY, type="2"), 
    test=norm(predict(rpart.model, data.test) - data.test$SALARY, type="2")
)
for(prune in (1:10 / 10.)) {
    rpart.model.prune <- prune(rpart.model, cp=prune)
    mse <- rbind(mse,
                 c(  
                     prune=prune,
                     train=norm(predict(rpart.model.prune) - data.train$SALARY, type="2"), 
                     test=norm(predict(rpart.model.prune, data.test) - data.test$SALARY, type="2")
                 ))
}
mse
#+END_SRC

#+RESULTS[e296f029af5a1832b0392501f02e5b1c16acd023]: Prune and MSE
#+BEGIN_org
#+ATTR_LATEX: :float nil
#+attr_html: :class center
#+caption: MSE
|Prune Parameter|train|test|
|   0 | 36198240.8617805 | 72784943.1305674 |
| 0.1 | 44735392.6910594 | 74445712.3863091 |
| 0.2 | 44735392.6910594 | 74445712.3863091 |
| 0.3 | 52769875.8870473 | 73588157.3153948 |
| 0.4 | 52769875.8870473 | 73588157.3153948 |
| 0.5 | 52769875.8870473 | 73588157.3153948 |
| 0.6 | 52769875.8870473 | 73588157.3153948 |
| 0.7 | 52769875.8870473 | 73588157.3153948 |
| 0.8 | 52769875.8870473 | 73588157.3153948 |
| 0.9 | 52769875.8870473 | 73588157.3153948 |
|   1 | 52769875.8870473 | 73588157.3153948 |
#+END_org



The following plots the result:
#+name: plotmse
#+BEGIN_SRC R :session :cache yes :results graphics :file img/mse.png :exports both :wrap (add-caption-and-center "MSE for traingin / test for different pruning parameters")
ggplot(mse) + 
geom_point(aes(x = prune, y = train, color='train')) + geom_line(aes(x = prune, y = train, color='train')) + 
geom_point(aes(x = prune, y = test, color='test')) +
xlab("B cp (Pruning Parameter)") + ylab("MSE")
#+END_SRC

#+RESULTS[08dea209f63ff2dc5d0da28568219e3b73e27a44]: plotmse
#+BEGIN_org
#+attr_html: :class center
#+caption: MSE for traingin / test for different pruning parameters
[[file:img/mse.png]]
#+END_org



b) 

   This section we try random forests.
   
#+name: forestmodel
#+BEGIN_SRC R :session :cache yes 
mse.forest <-  data.frame(B=integer(), train=numeric(), test=numeric()) 
for(B in seq(10, 100,by=10)) {
    randomForest.model <- randomForest(formula, data=data.train, ntree=B)
    mse.forest <- rbind(mse.forest,
                        data.frame(  
                            B=B,
                            train=norm(predict(randomForest.model, data.train) - data.train$SALARY, type="2"), 
                            test=norm(predict(randomForest.model, data.test) - data.test$SALARY, type="2")
                        ))
}
0
#+END_SRC

#+RESULTS[11b8f676c476c354c84a61625784f838b997ff55]: forestmodel
: 0




#+name: plotrfmse
#+BEGIN_SRC R :session :cache yes :results graphics :file img/rfmse.png :exports both :wrap (add-caption-and-center "MSE for random Forest" ) 
ggplot(mse.forest) + 
geom_point(aes(x = B, y = train, color='train')) + geom_line(aes(x = B, y = train, color='train')) + 
geom_point(aes(x = B, y = test, color='test')) +
xlab("B") + ylab("MSE")
#+END_SRC

#+RESULTS[c10bbc40327c1b0b4c515e6920065d65562718d1]: plotrfmse
#+BEGIN_org
#+attr_html: :class center
#+caption: MSE for random Forest
[[file:img/rfmse.png]]
#+END_org


c) 
   Random forest tends to perform better than pruning in both training and testing settings. The best bagging parameter in hindsight is $B = 20$. We use that for our prediction.

   
   #+BEGIN_SRC R :session :cache yes 
     Bmax = 20
     randomForest.model <- randomForest(formula, data=data.train, ntree=Bmax)
     pred <- predict(randomForest.model, data.test)
     most.undervalued.player <- which.max(pred - data.test$SALARY)
     as.character(most.undervalued.player)
     name <- as.character(data.test$PLAYER[most.undervalued.player])
     real.salary <- data.test$SALARY[most.undervalued.player]
     predicted.salary <- pred[most.undervalued.player]
     most.undervalued.player
     data.frame(name=name, real.salary=real.salary, predicted.salary=round(predicted.salary))
   #+END_SRC
   
   #+RESULTS[47afc24df85d7a5f768d20c4b9116ea947045164]:
   | Ryan Ludwick | 411000 | 9136108 |


   #+attr_html: :class center
   #+caption: The most undervalued player
   | name         | real salary | predicted salary |
   | Ryan Ludwick |      411000 |          9136108 |



   Looking at the evolution of this player at the prospectus:
   #+attr_html: :class center
   #+caption: Salary Evolution
   | Year         | Team | Salary              |
   |--------------+------+---------------------|
   | 2008	 | BAL  | 	$455,000    |
   | 2009	 | BAL  | 	$3,350,000  |
   | 2010	 | BAL  | 	$7,100,000  |
   | 2011	 | BAL  | 	$10,600,000 |
   | 2012	 | BAL  | 	$12,350,000 |
   | 2013	 | BAL  | 	$15,350,000 |
   | 2014	 | BAL  | 	$15,350,000 |
   | 2015	 | ATL  | 	$11,000,000 |
   | 2016	 | ATL  | 	$11,000,000 |
   | 2017	 | ATL  | 	$11,000,000 |
   | 2018	 | ATL  | 	$11,000,000 |

   We see that he was indeed very undervalued, the reason might be that he was just starting his career at 2008.


* Tame Categorical Variables in Tree Regression
2.1.

\begin{align*}
\sum_i (Y_i - f(X_i))^2 &= \sum_i Y_i^2 + \sum_s  \sum_{X_i = s}  f(s)^2 - 2  Y_i f(s)
\\&= \sum_i Y_i^2 +  \sum_s |\{X_i = s\}| [f(s)^2 - 2 \bar Y_s f(s)]
\\&= \sum_i Y_i^2 +  \sum_k \sum_{s \in L_k} |\{X_i = s\}| [\alpha_k^2 - 2 \bar Y_s \alpha_k]
\\&= \sum_i Y_i^2 +  \sum_k  |L_k| (\alpha_k^2 - 2 avg(L_k) \alpha_k)
\end{align*}
It is clear that an optimal choice for $\alpha_k$ would satisfy $\alpha_k = avg(L_k)$ by minimizing a quadratic form.
Using the assumption $\bar Y_1 < \ldots <\bar Y_M$, and the fact that $L_k \ne \emptyset$, if $k \ne k'$ then $avg(L_k) \ne avg(L_k')$

Let's assume that $u, w \in L_k$ and that $v \in L_k'$.

By minimality of $f$ we have that:
- $f(u)^2 - 2\bar Y_u f(u)^2 \le {\alpha_k'}^2 - 2\bar Y_u {\alpha_k'}$, otherwise we take out $u$ from $L_k$ and put it in $L_k'$ ($L_k$ would still be non empty) which would contradict the minimality of $f$. Using the fact that $f(u) = \alpha_k$, $\alpha_k^2 - 2\bar Y_u \alpha_k^2 \le {\alpha_k'}^2 - 2\bar Y_u {\alpha_k'}^2$
- By the same argument, $f(v)^2 - 2\bar Y_v f(v)^2 \le {\alpha_k}^2 - 2\bar Y_v {\alpha_k}^2$, or  $-{\alpha_k}^2 + 2\bar Y_v \alpha_k \le -{\alpha_k'}^2 + 2\bar Y_v {\alpha_k'}$.
-   $\alpha_k^2 - 2\bar Y_w \alpha_k \le {\alpha_k'}^2 - 2\bar Y_w {\alpha_k'}$
Adding the second identity to the other two we get that:

- $2 \alpha_k \underbrace{(\bar Y_v - \bar Y_u)}_{\ge 0} \le 2 \alpha_k' (\bar Y_v - \bar Y_u)$
- $2 \alpha_k \underbrace{(\bar Y_w - \bar Y_u)}_{\le 0} \le 2 \alpha_k'(\bar Y_w - \bar Y_u)$
  
Which proves that $\alpha_k = \alpha_k'$, and so $k = k'$

2.2
Now $K = 2$

The set of partitions $(L_1, L_2)$ admits a one to one mapping to the set of functions $\{0, 1\}^{\{1, \ldots M\}}$, so:
$\mathcal N_1 = 2^{M}$

In this case, we know that $L_1$ is of the form $\{1, \ldots j\}$, so
$\mathcal N_2 = M$

$$\frac{\mathcal N_2}{\mathcal N_1} = \frac{M}{2^M}$$


* Baggin and Random Forest
  
3.1 WLOS, take $i = 1$.

For $B \in \mathbb{N}^*$, and $j \le B$, note
- $V_j = (Z^*_1, \ldots Z^*_n)$ the $B$ bootstraps samples from $\{Z_2, \ldots Z_n\}$ used in $\hat f_{CV}^{(1)} = \frac1n \sum_{j=1}^B \mathcal A(U_j )$, Where $\mathcal A(U)$ is the algorithm that returns the tree corresponding the observation $U$.
- $U_j = (Z^*_1, \ldots Z^*_n)$ the $B$ bootstraps samples from $\{Z_1, \ldots Z_n\}$ used in $\hat f_{OOB}^{(1)} =\frac1{|\{j, Z_1 \not \in V_j\}|} \sum_{j=1, Z_1 \not \in V_j} \mathcal A(V_j )$

By the law of large numbers:
- $f^{(1)}_{CV}(X_1) \rightarrow E[\mathcal A(V)(X_1) | X_1]$ where $V \in \mathbb R^d$ is drawn uniformly from $\{Z_2, \ldots, Z_n\}^B$
- $f^{(1)}_{OOB}(X_1) \rightarrow E[\mathcal A(U)(X_1) | X_1, Z_1 \not \in U]$ where $U \in \mathbb R^d$ is drawn uniformly from $\{Z_1, \ldots, Z_n\}^B$ 

  It is easy to see that $Law(U| Z_1 \not \in U) \overset{d}{=} Law(V)$ because by symmetry of the $Z_2, \ldots Z_n\}$, all bootstrapping samples are equally likely for $U$ when $Z_1 \not \in U$

  We have just proven that  conditional on the $Z_i$, $\hat f_{OOB}^{(1)}(X_1) - \hat f_{CV}^{(1)}(X_1)$ converges a.s to 0. The result follow because the convergence in  is preserved with respect to taking sums and products, and the fact that convergence almost sure $\implies$ convergence in probability.

  3.2

For $i = 1,\ldots, B$, let $X_{i1}^*, \ldots X_{in}^*$ the bootstrapped sample used to construct mean $\bar X_i^*$, such that $\mathcal L \text{aw} (X_{ij}^*|X_1, \ldots X_n) \overset{iid}{\sim} \mathcal U(\{X_1 \ldots X_n\})$.

Let $\bar X = \frac1n \sum_{i=1}^n X_i$ be the empirical mean, and $\Sigma = \frac1{n-1} \sum_{i=1}^n (X_i - \bar X)^2$ the empirical variance.

- $$E[\bar X_i^*] =E[ E[\bar X_i^* | X_1, \ldots X_n]] = E[\frac 1n \sum_{j=1}^n E[X^*_{ij} |  X_1, \ldots X_n ]] = E[\bar X] = \mu$$
- $$Var(\bar X_i^* | X_1, \ldots, X_n) =  \frac1{n^2} Var(X_{11}^*| X_1, \ldots, X_n) = \frac1n \sum_{j=1^n} (X_j - \bar X) = \frac{n-1}{n^2} \Sigma$$
- $$E[Var(\bar X_i^* | X_1 \ldots X_n)] = \frac{n-1}{n^2} E[\Sigma] = \frac{n-1}{n^2} \sigma^2$$
- $$Var(E[\bar X_i^*]) = Var( \bar X) = \frac{\sigma^2}n$$
- $$Var(\bar X_i^*) = E[Var(\bar X_i^* | X_1 \ldots X_n)] + Var(E[\bar X_i^* | X_1 \ldots X_n]) = \frac{2n-1}{n^2}\sigma^2$$
- $$Cov(\bar X_1^*, \bar X_2^*) = E[\bar X_1^* \bar X_2^* ] - E[\bar X_1^*] E[\bar X_2^* ] = \frac1{n^2} \sum_{i,j} E[ X_{i1}^* X_{j2}^* ] - \mu^2$$
- $$E[X_{i1}^* \bar X_{j2}^*] = E[ E[X_{i1}^* \bar X_{j2}^* | X_1 \ldots X_n] ] E[ E[X_{i1}^* | X_1 \ldots X_n]E[X_{j2}^* | X_1 \ldots X_n] ] = E[\bar X^2]$$
- $$Cov(\bar X_1^*, \bar X_2^*) = E[\bar X^2] - \mu^2 = Var(\bar X) = \frac{\sigma^2}{n}$$
- $$Cor(\bar X_1^*, \bar X_2^*) = \frac{Cov(\bar X_1^*, \bar X_2^*)}{Var(\bar X_1^*)Var(\bar X_2^*)} = \frac{2n-1}n$$
- $$Var(\frac1B \sum_1^B \bar X_b^*) = \frac1{B^2} \sum Var(\bar X_b^*) + 2 \frac1{B^2} \sum_{a, b} Cov(\bar X_a^*, \bar X_b^*)$$
-

  \begin{align*}
  Var(\frac1B \sum_1^B \bar X_b^*)
  &= \frac1{B}  Var(\bar X_1^*) + 2 \frac{B^2 - B}{B^2}  Cov(\bar X_1^*, \bar X_2^*)
  \\&= \frac1{B}  Var(\bar X_1^*) + 2 \frac{B - 1}{B}  Cov(\bar X_1^*, \bar X_2^*)
  \\&= (\frac{2n-1}{n^2}+ 2 \frac{B - 1}{n})\frac{\sigma^2}{B}
  \\&= (\frac2n -\frac{1}{Bn^2}) \sigma^2
  \end{align*}


* Explore the Boundary of RIP Conditions
4.1
$|\inner{Ax}{Ay}| \le \delta_{s+t} \norm{x}_2 \norm{y}_2 \iff |\inner{A\frac{x}{\norm{x}_2}}{A\frac{y}{\norm{y}_2}}| \le \delta_{s+t}$
Without loss of generality we assume that $x$ and $y$ have unit norm.

Since $x$ and $y$ have distinct support, $\norm{x - y}_2^2 =  \norm{x+y}_2^2 = \norm{x}_2^2 + \norm{y}_2^2$ and $\norm{x + y}_0 = \norm{x-y}_0 = \norm{x}_0 + \norm{y}_0 \le s + t$
 then $\norm{x \pm y}_2^2 = 2$ and:
$2(1 - \delta_{s+t}) \le \norm{Ax \pm Ay}^2 \le 2(1 + \delta_{s+t})$, so:

\begin{align*}
 |\inner{Ax}{Ay}|
&= \frac14 |\norm{Ax + Ay}^2 - \norm{Ax - Ay}^2)|
\\&\le \frac14 | 2(1+\delta_{s+t}) - 2(1-\delta_{s+t})|
\\&\le \delta_{s+t}
\end{align*}
Which ends the proof


4.2
- *step 1:*
  Let $\chi$ be set that satisfies those conditions with maximal size so that $U = \cup_{x \in \chi} B(x, \sqrt \frac s2)$ where:
  
  $$B(x, \sqrt \frac s2) = \{ y \in U : \norm{x-y}_2 \le  \sqrt \frac s2 \} \subset  \{ y \in U : \norm{x-y}_0 \le \frac s2 \}$$.
  So $|U| \le \sum_{x \in \chi} |B(x, \sqrt \frac s2)| \le |\chi| |B(0, \sqrt \frac s2)|$
  
But:
  $|B(0, \sqrt \frac s2)| \le \#\{ z \in \{0, 1, -1\}^d \norm{x-z}_0 \le \frac s2\} \le {d \choose \frac s2} 3^{\frac s2}$,  and: $|U| = {d \choose s} 2^s$

  So
  \begin{align*}
  .|\chi| &\ge (\frac43)^{\frac s2} \frac{{d \choose s}}{{d \choose \frac s2}}
  \\& \ge (\frac43)^{\frac s2} \frac{(s/2)! (d-s/2)!}{s! (d-s)!}
  \\& \ge (\frac43)^{\frac s2} \prod_{i=1}^{s/2} \frac{d-s+i}{s/2+i}
  \\& \ge (\frac43)^{\frac s2} (\frac{d-s/2}{s})^{\frac s2}
  \\& \ge (\frac43 \frac{d-s/2}{s})^{\frac s2}
  \end{align*}

  $\frac43 \frac{d-s/2}{s} \ge \frac ds \iff 4d - 2s \ge 3d \iff d \ge 2s$, so the proof is complete.

  
- *step 2*:
$\norm{x-z}_0 \le 2s$
$$\norm{Ax - Az}_2^2 \ge (1- \delta_{2s}) \norm{x - z}_2^2 \ge (1 - \delta_{2s}) \frac{s}2 \ge \frac{s}4$$
Which proves that the center of two balls are distant by more than twice their radiuses, so they mssust be disjoint.

- *step 3:*
  For $x \in U$, $\norm{Ax}_2 \le (1 + \delta_s) \norm{x}_2^2 \le \frac 32 s$.

  So the balls of the centered at $Ax$ where $x \in \chi$ with raidus $\sqrt{\frac{s}{16}}$ are contained in the ball centered at 0 with radius $(\sqrt{\frac 32} + \frac 14) \sqrt s$.

  Since such balls are disjoint, their total volume is  $|\chi| Vol(\sqrt{\frac s{16}})$, where $Vol(r)$ is the volume of the the ball of radius $r$. We know that $Vol(r) = r^n Vol(1)$, so:
  $$|\chi| (\frac{s}{16})^{n/2} Vol(1) \le Vol(1) (\sqrt{\frac 32} + \frac 14)^{n/2}  s^{n/2}$$
  
  Taking the $\log$ and using step 1:
  $$\frac{s}2 \log(\frac ds)  \le  \frac n2 \log(16 (\sqrt{\frac 32} + \frac 14)) $$  
  So:
  $$Cs \log(\frac ds)  \le  n  $$  

4.3.

For computation reasons, we restrict the calculation to the case where $d = 100$ instead of $d = 1024$. 

#+BEGIN_SRC matlab :session *MATLAB* :cache yes
  number_monte_carlo = 3;
  epsilon = 0.001;
  d = 100;
  L = 20;
  probabilities1 = zeros(L-1, L-1);
  probabilities2   = zeros(L-1, L-1);
  probabilities3  = zeros(L-1, L-1);
  for i=1:number_monte_carlo
      for n=1:(L-1)
      t = floor((d/(L)) * (n-1));
          %X = 1/sqrt(t) * A(1:t,1:d);
          X1 = 1/sqrt(t) * randn(t, d);
          X2   = SubDCT_Phi(t, d);
          X3  = SubToep_Phi(t, d);
          beta = randn(d, 1);
          for s=1:(L-1)
              r = floor((d/(L)) * (s-1));
              beta(1:r, 1) = 0;
              probabilities1(n, s) = probabilities1(n, s)...
              + (norm(l1eq_pd(0*beta,X1, 0*X1, X1*beta) - beta) < epsilon);
              probabilities2(n, s) = probabilities2(n, s)...
              + (norm(l1eq_pd(0*beta,X2, 0*X2, X2*beta) - beta) < epsilon);
              probabilities3(n, s) = probabilities3(n, s)...
              + (norm(l1eq_pd(0*beta,X3, 0*X3, X3*beta) - beta) < epsilon);
          end
      end
  end
  %dlmwrite('mat1.txt', probabilities1 / number_monte_carlo)
  %dlmwrite('mat2.txt', probabilities2 / number_monte_carlo)
  %dlmwrite('mat3.txt', probabilities3 / number_monte_carlo)
  ans = 0;
#+END_SRC

#+RESULTS[c887d0d9c8fb96ab44e1bf106dc1b995f4d6d5f4]:
: 0





#+BEGIN_SRC python :session  :exports none
  import numpy as np
  for i in range(1, 4):
      m = np.array(map(lambda L: map(float, L.split(',')), open('mat%d.txt' % i).readlines()[:-1]))[::-1, ::-1]
      import matplotlib
      matplotlib.use('Agg')
      import matplotlib.pyplot as plt
      fig=plt.figure()
      plt.imshow(m, interpolation="nearest")
      plt.title('Probability of recovery')
      plt.xlabel('n')
      plt.ylabel('s')
      fig.tight_layout()
      plt.savefig('img/heatmap%d.png' % i)


#+END_SRC

#+RESULTS:
: Text(83.1321,0.5,u's')


#+ATTR_LATEX: :float nil
#+ATTR_LATEX: :width 0.75\textwidth
#+caption: X Gaussian 
[[file:img/heatmap1.png]]
#+ATTR_LATEX: :float nil
#+ATTR_LATEX: :width 0.75\textwidth
#+caption: X SubDCT
file:img/heatmap2.png
#+ATTR_LATEX: :float nil
#+ATTR_LATEX: :width 0.75\textwidth
#+caption: X SubToepPhi 
file:img/heatmap3.png







































